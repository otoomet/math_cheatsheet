% Encoding: UTF-8

@Article{calzolari+fiorentini1993,
  author    = {Giorgio Calzolari and Gabriele Fiorentini},
  title     = {Alternative covariance estimators of the standard {T}obit model},
  journal   = EL,
  year      = {1993},
  volume    = {42},
  number    = {1},
  pages     = {5-13},
  annote    = {26/IV 2007},
  owner     = {siim},
  timestamp = {2019.08.05},
}

@Book{chiang1984,
  title     = {Fundamental Methods of Mathematical Economics},
  publisher = {McGraw-Hill},
  year      = {1984},
  author    = {Alpha C. Chiang},
  address   = {Singapore},
  edition   = {Third},
  owner     = {siim},
  timestamp = {2019.08.05},
}

@Article{lin1991,
  author    = {Jianhua Lin},
  title     = {Divergence measures based on the Shannon entropy},
  journal   = {Information Theory, IEEE Transactions on},
  year      = {1991},
  volume    = {37},
  number    = {1},
  pages     = {145-151},
  month     = {Jan},
  issn      = {0018-9448},
  abstract  = {A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness},
  doi       = {10.1109/18.61115},
  keywords  = {entropy;information theory;Shannon entropy;boundedness;divergence measures;finiteness;information theory;nonnegativity;probability of misclassification error;semiboundedness;variational distance;Computer science;Entropy;Genetics;Pattern analysis;Pattern recognition;Probability distribution;Signal analysis;Signal processing;Taxonomy;Upper bound},
  owner     = {siim},
  review    = {Construct a measure for distance between probability distributions: K(p1, p2) = \sum_x p1(x) log (p1(x)/(1/2 p1(x) + 1/2 p2(x))) log is base-2 log. It allows p2(x)=0 while p1(x) is not 0, and has other nice properties.},
  timestamp = {2015.08.10},
}

@Book{miller1979,
  title     = {Dynamic Optimization and Economic Applications},
  publisher = {McGraw-Hill},
  year      = {1979},
  author    = {Ronald Eugene Miller},
  address   = {New York},
  owner     = {siim},
  timestamp = {2019.08.05},
}

@Book{murphy2012,
  title     = {Machine Learning: A Probabilistic Perspective},
  publisher = {MIT Press},
  year      = {2012},
  author    = {Kevin P. Murphy},
  address   = {Cambridge, MA},
  owner     = {otoomet},
  timestamp = {2018.12.10},
}

@Article{newey1991Econometrica,
  author    = {Whitney K. Newey},
  title     = {Uniform Convergence in Probability and Stochastic Equicontinuity},
  journal   = {Econometrica},
  year      = {1991},
  volume    = {59},
  number    = {4},
  pages     = {1161-1167},
  issn      = {00129682, 14680262},
  owner     = {otoomet},
  publisher = {[Wiley, Econometric Society]},
  review    = {Explains how the concept of stochastic equicontinuity helps to re-define the uniform continuity assumptions for different statistical models.

2017-12-14 UW},
  timestamp = {2017.12.14},
  url       = {http://www.jstor.org/stable/2938179},
}

@Article{tuenter2000SN,
  author    = {Tuenter, H. J. H.},
  title     = {On the generalized Poisson distribution},
  journal   = {Statistica Neerlandica},
  year      = {2000},
  volume    = {54},
  number    = {3},
  pages     = {374-376},
  abstract  = {We use Euler's difference lemma to prove that, for θ > 0 and 0 ≤λ < 1, the function Pn defined on the non-negative integers by P n (θ, λ) = [θ(θ + nλ)n−1/n!]e−nλ−θ defines a probability distribution, known as the Generalized Poisson Distribution.},
  doi       = {10.1111/1467-9574.00147},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9574.00147},
  keywords  = {Probability Theory, Euler's difference lemma},
  owner     = {otoomet},
  review    = {Show that generalized Poisson sum coverges to 1, i.e. it is a valid probability distribution.},
  timestamp = {2020.08.19},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9574.00147},
}

@Article{joe+zhu2005BJ,
  author    = {Joe, Harry and Zhu, Rong},
  title     = {Generalized Poisson Distribution: the Property of Mixture of Poisson and Comparison with Negative Binomial Distribution},
  journal   = {Biometrical Journal},
  year      = {2005},
  volume    = {47},
  number    = {2},
  pages     = {219-229},
  abstract  = {Abstract We prove that the generalized Poisson distribution GP(θ, η) (η ≥ 0) is a mixture of Poisson distributions; this is a new property for a distribution which is the topic of the book by Consul (1989). Because we find that the fits to count data of the generalized Poisson and negative binomial distributions are often similar, to understand their differences, we compare the probability mass functions and skewnesses of the generalized Poisson and negative binomial distributions with the first two moments fixed. They have slight differences in many situations, but their zero-inflated distributions, with masses at zero, means and variances fixed, can differ more. These probabilistic comparisons are helpful in selecting a better fitting distribution for modelling count data with long right tails. Through a real example of count data with large zero fraction, we illustrate how the generalized Poisson and negative binomial distributions as well as their zero-inflated distributions can be discriminated. (© 2005 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim)},
  doi       = {10.1002/bimj.200410102},
  eprint    = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.200410102},
  keywords  = {Poisson mixture, Overdispersion, Skewness, Zero-inflated distribution},
  owner     = {otoomet},
  review    = {Show that generalized Poisson (GP) is a Poisson mixture, and compare it with negative binomial (NB)

Demonstrate that 
* GP has less mass on 0, given mean and overdispersion ratio, and has thicker tail.  Hence prefer NB if more mass at 0, prefer GP if long right tail
* GP has more skewness, hence its mixture distribution is more skewed too.
* Discuss zero-inflated versions of these models.  The best model may be a different one if selecting a zero-inflated one.

Illustrate the effects with cancer data.},
  timestamp = {2020.08.19},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.200410102},
}

@Comment{jabref-meta: databaseType:bibtex;}
