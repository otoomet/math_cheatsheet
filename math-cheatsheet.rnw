\documentclass[a4paper]{article}
\usepackage{amsthm}
\usepackage{asymptote}
\usepackage{booktabs}
\usepackage[font={small,sl}]{caption}
\usepackage[T1]{fontenc}
\usepackage[estonian,english]{babel}
\usepackage[colorlinks, linkcolor=blue, urlcolor=MidnightBlue, citecolor=TealBlue]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{mathrsfs}
\usepackage{xspace}
\usepackage{amsmath,amssymb}
\usepackage{makeidx}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{pxfonts}
\usepackage[dvipsnames]{xcolor}
\input{isomath}
\allowdisplaybreaks

\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
% fullref: https://tex.stackexchange.com/questions/121865/nameref-how-to-display-section-name-and-its-number
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1}
    \nameref*{#1}}, page~\pageref{#1}} % One single link
\numberwithin{equation}{subsection}
\makeindex

<<setup, include=FALSE>>=
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(fig.height=100/25.4, fig.path=".fig/",
                      echo=FALSE,
                      cache=TRUE, cache.path=".cache/",
                      message=FALSE, warning=FALSE, error=FALSE)
@ 

\title{Mathematical Formulas and Other Valuable Knowledge\\[1.2ex]
\large that I have found Useful for Myself\\
and Decided to Write Down}
\author{Ott Toomet}
\date{Seattle, \today}

<<init, echo=FALSE, include=FALSE, error=FALSE, message=FALSE>>=
library(ggplot2)
library(gridExtra)
library(dplyr)
options(tibble.width=50, tibble.print_max=6, tibble.print_min=4)
h1.3 <- 75/25.4  # 1x3 figure panel
@ 

\begin{document}
\maketitle

This is a collection of formulas, definitions, and other math stuff I
have had to look up, find out, struggle with, and decided to write
down for the future refence.  The pdf version is available at \href{https://otoomet.github.io/math_cheatsheet/math_cheatsheet.pdf}{Github
Pages} and the (mostly latex) source
\href{https://github.com/otoomet/math_cheatsheet}{on Github}. 

Feel free to use and copy the formulas here.  But be careful -- there
is probably a number of errors and misunderstandings!

The cheatsheet started around 1999 when I studied economics at Tartu
University and wrote a sheet (yes, a single sheet ;-) of formulas to help
my coursemates with math.  There are still pieces of text from that
time, in particlar those related to limits and Taylor's
approximation.  The pages filled with Tobit and duration models'
Hessians are leftovers from my Master and PhD thesis.  I wrote it in Estonian back then, and I don't intend
to translate it just for translation's sake.  Only if I am rewriting
an old piece of text I'll do it in English.

\medskip

Have fun!

\bigskip

Ott

\medskip

Seattle, \today

\tableofcontents

\newpage
\section{Geometry}

\subsection{Lines and planes}
\label{sec:lines-planes}

Line
\index{line|textbf}
on plane is expressed by
\begin{equation}
  \label{eq:line-on-plane}
  ax + by + c = 0
\end{equation}
Its direction vector
\index{line!direction|textbf}
\begin{equation}
  \label{eq:line-plane-direction-vector}
  (b, -a)
\end{equation}
and normal vector is
\index{line!normal|textbf}
\begin{equation}
  \label{eq:line-plane-normal-vector}
  (a, b)
\end{equation}
(not normalized).

\subsection{Supporting Hyperplane}
\label{sec:supporting-hyperplane}

\emph{Supporting Hyperplane} \index{supporting hyperplane|textbf} of a
set $S$ in Euclidean space $\Real^{n}$ is a hyperplane that satisfies:
\begin{itemize}
\item $S$ is entirely contained in one of the two closed half-spaces
  bounded by the hyperplane;
\item $S$ has at least one boundary-point on the hyperplane.
\end{itemize}
Supporting hyperplane may not exist if $S$ is not convex.


\selectlanguage{estonian}
\subsection{Koordinaatteisendused}
\subsubsection{Pinnaelement koordinaatteisendusel}
Üleminekul koordinaatsüsteemilt $(x,y)$ uutele koordinaatidele
$(u=u(x,y), v=v(x,y))$ teisenevad väikesed koordinaatide sihilised
lõigud nii:
\begin{equation}
\begin{pmatrix}
\dif u\\
\dif v
\end{pmatrix} =
  \begin{pmatrix}
  u_x   & u_y \\
  v_x   & v_y
  \end{pmatrix}
  \begin{pmatrix}
  \dif x\\
  \dif y
  \end{pmatrix},
\end{equation}
kus paremal poolel esimene on vastavate osatuletiste maatriks
(jakobjaan $\mathcal{J}$).
Pinnaelement avaldub
\begin{equation}
\dif S = \dif x \dif y = 
 \frac{\dif u \dif v}{|\det \mathcal{J}|},
\end{equation}
kus
\begin{math}
|\det \mathcal{J}| = \dif u \dif v \sin( \widehat{u,v})
\end{math}
on kahe lühikese lõigu $\dif u$ ja $\dif v$ poolt defineeritud
elementaarpindala.  Analoogiline seos kehtib ka kõrgemate mõõtmete
puhul.


\subsection{Kolmnurk}
\subsubsection{Seos kolmnurga külgede ja nurkade vahel}

\begin{equation}
a^2 + b^2 - 2ab \cos \gamma = c^2.
\end{equation}
Täisnurksel kolmnurgal, kui $\gamma = 90\kraad$, kehtib
\emph{Pythagorase teoreem}:
\begin{equation}
a^2 + b^2 = c^2.
\end{equation}

\subsubsection{Kolmnurga pindala}
Kui kolmnurk on tasandil antud kolme punktiga $(0,0)$, $(X_A, Y_A)$
ning $(X_B, Y_B)$ siis kolmnurga pindala on
\begin{equation}
S = \frac{1}{2} \left| X_A \cdot Y_B - Y_A \cdot X_B \right| =
\frac{1}{2} \abs
  \begin{vmatrix}
  X_A   & Y_A \\
  X_B   & X_B
  \end{vmatrix}.
\end{equation}
Tõestus: joonista kolmnurk välja ja vaata, millised pindalad kirjeldab
determinant.


\selectlanguage{english}

\subsection{Trigonometrics}
\label{sec:trigonometrics}

\paragraph{Trigonometric Identities}

\begin{align*}
  \sin(2\alpha) &= 2\cos \alpha\,\sin \alpha
  \\
  \cos(2\alpha) &= \cos^{2}\alpha - \sin^{2}\alpha
  \\
  \sin(\alpha + \beta) &= \cos \alpha\,\sin \beta+\sin \alpha\,\cos \beta
  \\
  \cos(\alpha + \beta) &= \cos\alpha\, \cos\beta - \sin\alpha\, \sin\beta
\end{align*}


\subsection{Sphere}
\label{sec:sphere}
\index{sphere|textbf}

Surface: $4\pi R^{2}$
\index{sphere!surface|textbf}

$n$-dimensional hypersphere
\index{sphere!hypersphere|textbf}
is a set of points in $\Real^{n+1}$ so
that they are equidistant from the center:
\begin{equation}
  \label{eq:n-sphere}
  S^{n} = \{x \in \Real^{n+1}: \norm{x - c} = R\}.
\end{equation}
So 0-sphere is two points in $\Real^{1}$, 1-sphere is a circle in
$\Real^{2}$, etc.

$n$-ball
\index{n-ball@$n$-ball|textbf}
is a set of points inside of $n-1$-sphere (that may or may
not include the sphere itself).  So $n$-ball is a subset of
$\Real^{n}$. 

$n-1$ dimensional surface area
\begin{equation}
  \label{eq:n-sphere-surface}
  S_{n-1}(R) = \frac{2 \pi^{n/2}}{\Gamma(n/2)} R^{n-1}
\end{equation}
$n$-dimensional volume
\begin{equation}
  \label{eq:n-sphere-volume}
  V_{n}(R) =
  \frac{\pi^{n/2}}{\Gamma(1 + \frac{n}{s})} R^{n}
\end{equation}
where $\Gamma(\cdot)$ is \hyperref[sec:gamma-function]{gamma
  function}. 

\selectlanguage{estonian}
\subsection{Joone kõverus}

Parameetriliselt antud joone kõverus:
\begin{equation}
  \label{eq:param_joone_kqverus}
  k = \frac{x'y'' - y'x''}{({x'}^2 + {y'}^2)^{3/2}}
\end{equation}
\selectlanguage{english}



\newpage

\section{Functions}
\label{sec:functions}

\subsection{General}
\label{sec:functions-general}

\paragraph{Bijective function}
\index{bijective function|textbf}

(also bijection) is an one-to-one relationship between elements of two sets.

\paragraph{Measurable function}

\index{functions!measurable|textbf}
Let $E$ be a (Lebesgue-) measurable set.
Function $f: E \to \Real$ is measurable if for any interval $I
\subseteq \Real$:
\begin{equation}
  \label{eq:measurable-function}
  f^{-1}(I) =
  \{ x \in \Real : f(x) \in I \}
\end{equation}
is measurable.

Briefly: the inverse image of intervals is measurable. 


\subsection{Elementary functions}
\label{sec:elementary-functions}

\paragraph{Trigonometric functions}
\label{sec:trigonometric-functions}

\begin{equation}
  \label{eq:sin-complex}
  \sin x =
  \frac{\mi}{2}\left( \me^{-\mi x} - \me^{\mi x} \right)
\end{equation}


\paragraph{Hyperbolic functions}
\begin{align}
  \label{eq:hyperbolic}
  \sinh x &= \frac{\me^{x} - \me^{-x}}{2}\\
  \arcsinh x &= \log \left( x + \sqrt{x^{2} + 1 }\right)\\
  \tanh x &= \frac{\sinh x}{\cosh x} =
            \frac{\me^{2x} - 1}{\me^{2x} + 1}
\end{align}
\index{sinh|textbf} \index{asinh|textbf} \index{tanh|textbf}

<<sinhPlot>>=
par(mar=c(3,3,0,0)+0.1,
    mgp=c(2,1,0))
curve(sinh, -4, 4, n=201, col="red",
      ylab="")
legend("topleft", legend=c("sinh"), col=c("red"),
       lty=1, bty="n")
@ 

<<arcsinhPlot>>=
par(mar=c(3,3,0,0)+0.1,
    mgp=c(2,1,0))
curve(asinh, -9, 9, n=201, col="red")
curve(tanh, col="skyblue", add=TRUE, ylab="")
legend("topleft", legend=c("asinh", "tanh"), col=c("red", "skyblue"),
       lty=1, bty="n")
@ 

A closely related to $\tanh$ is \emph{hard tanh}: \index{hard tanh|textbf}
\begin{equation*}
  \text{hard tanh}(x) =
  \begin{cases}
    -1 & \text{if}\quad x < -1\\
    x & \text{if}\quad -1 \le x \le 1\\
    1 & \text{if}\quad x > 1\\
  \end{cases}
\end{equation*}

\subparagraph{Derivatives}

\begin{align}
  \label{eq:hyperbolic-derivatives}
  \pderiv{x}\tanh(x) &=
                         4 \frac{\me^{2x}}{\left(
                         \me^{2x} + 1\right)^{2}} =
                       1 - \tanh^{2}(x)
\end{align}

\subsection{Algebraic Functions}

\paragraph{Inverse Mills Ratio}
\label{sec:inverse-mills-ratio}
\index{inverse mills ratio|textbf}

\begin{equation}
  \label{eq:inverse-mills-ratio}
  \lambda(x) =
  \frac{\phi(x)}{\Phi(x)}
\end{equation}
where $\phi(\cdot)$ and $\Phi(\cdot)$ are standard normal \pdf and
\cdf respectively (Figure~\ref{fig:inverse-mills-ratio}).

Properties:
\begin{equation}
  \label{eq:inverse-mills-ratio-inequality}
  \lambda(x) > -x
\end{equation}
Proof: if $X \sim N(0,1)$ then $\E[ X| X < \theta] = -\lambda(\theta)$
(see \fullref{sec:normal-cond-expect})
and $\E[X|x < \theta] < \theta$.  Follows from these two conditions.

\begin{figure}[ht]
  \centering
<<>>=
curve(dnorm(x)/pnorm(x), -3, 3,
      ylim=c(-1, 3.1),
      col="red", xlab="x", ylab="")
abline(a=0, b=-1)
@
\caption{Inverse Mills Ratio (red) versus $y=-x$ (black).}
\label{fig:inverse-mills-ratio}
\end{figure}


\paragraph{Gamma Function $\Gamma(\cdot)$}
\label{sec:gamma-function}
\index{functions!gamma|textbf}

\begin{equation}
  \Gamma(s) = \int_{0}^{\infty} t^{s-1} \me^{-t}\,\dif t
\end{equation}
See Figure~\ref{fig:gamma-function}.

\begin{figure}[ht]
<<gammaPlot>>=
par(mar=c(3,3,0,0)+0.1,
    mgp=c(2,1,0))
curve(gamma, -1, 5, n=201, ylim=c(-10,10), col="red")
abline(h=0)
abline(v=0)
@ 
\caption{Gamma function}  
\label{fig:gamma-function}
\end{figure}


Properties:
\begin{align}
  \Gamma(1) = \Gamma(2) &= 1
  \\
  \Gamma(n) &= (n-1)!
  \\
  \Gamma(n+1) &= n \Gamma(n)
\end{align}
\begin{proof}
  Compute 
  \begin{math}
    \Gamma(s+1) =
    \int_{0}^{\infty} t^{s} \me^{-t}\,\dif t.
  \end{math}
  Integrate by parts by choosing $u = t^{s}$ and $\dif v =
  \me^{-t}\,\dif t$, hence $\dif u = st^{s-1} \,\dif t$ and $v =
  -\me^{-t}$.  We have
  \begin{equation}
    \label{eq:gamma-integrate-by-parts}
    \Gamma(s+1) =
    \int_{0}^{\infty} t^{s} \me^{-t}\,\dif t =
    \left.
      -t^{s} \me^{-t}
    \right|_{0}^{\infty}
    +
    \left.
      \int_{0}^{\infty} s t^{s-1} \me^{-t} \,\dif t
    \right|_{0}^{\infty}
    =
    s \Gamma(s+1)
  \end{equation}
\end{proof}


\paragraph{Digamma function}
\label{sec:digamma}
\index{functions!gamma!digamma|textbf}

\begin{equation}
  \label{eq:digamma}
  \psi(x) = \pderiv{x} \log\Gamma(x)
  =
  \frac{1}{\Gamma(x)}
  \int_0^\infty t^{x-1} e^{-t} \log t \,\dif t.
\end{equation}
Properties:
\begin{align}
  \psi(\alpha+1) &= \frac{1}{\alpha} + \psi(\alpha)
                   \\
  \psi(1) &= -0.5772 & \psi(2) &= 0.4228
\end{align}

\paragraph{Multivariate gamma function}
\label{sec:multivariate-gamma}
\index{functions!gamma!multivariate|textbf}

Multivariate generalization of
\hyperref[sec:gamma]{gamma function}.
\index{functions!gamma}
\begin{equation}
  \label{eq:mv-gamma}
  \Gamma_{K}(x) =
  \mpi^{\frac{1}{4}K(K-1)}
  \prod_{i=1}^{K} \Gamma\left(x + \frac{1-i}{2} \right)
\end{equation}
where $x\in\Complex$ and $\realpart(x) > (K - 1)/2$.
Note that $\Gamma_{1}(x) = \Gamma(x)$.

\paragraph{Incomplete gamma function}
\label{sec:incomplete-gamma-function}
\index{functions!gamma!incomplete|textbf}

Upper incomplete gamma:
\begin{equation}
  \label{eq:incomplete-gamma-function}
  \Gamma(s, x) = \int_{x}^{\infty} t^{s-1} \me^{-t}\,\dif t
\end{equation}

Normalized incomplete gamma:
\begin{equation}
  \label{eq:normalized-incomplete-gamma-function}
  Q(s, x) = \frac{1}{\Gamma(s)}
  \int_{x}^{\infty} t^{s-1} \me^{-t}\,\dif t
\end{equation}

Lower incomplete gamma function:
\begin{equation}
  \label{eq:lower-incomplete-gamma-function}
  \gamma(s, x) = \int_{0}^{x} t^{s-1} \me^{-t}\,\dif t
\end{equation}
See Figure~\ref{fig:example-incomplete-gamma}.

\begin{figure}[ht]
  \centering
<<incomplete-gamma>>=
 ss <- c(0.5, 1, 2, 3)
ssc <- sprintf("%4.2f", ss)
pal <- RColorBrewer::brewer.pal(length(ss), "Set1")
xrange <- c(0,4)
upperInc <- function(x, a) gsl::gamma_inc(a, x)
p1 <- ggplot(data.frame(x=xrange)) +
   geom_function(aes(x, 
                     col=ssc[1]),
                    fun=upperInc, args=list(a=ss[1])) +
   geom_function(aes(x, 
                     col=ssc[2]),
                 fun=upperInc, args=list(a=ss[2])) +
   geom_function(aes(x, 
                     col=ssc[3]),
                 fun=upperInc, args=list(a=ss[3])) +
   geom_function(aes(x, 
                     col=ssc[4]),
                 fun=upperInc, args=list(a=ss[4])) +
   guides(color="none") 
lowerInc <- function(x, a) gamma(a) - gsl::gamma_inc(a, x)
p2 <- ggplot(data.frame(x=xrange)) +
   labs(color=expression(s)) +
   geom_function(aes(x, 
                     col=ssc[1]),
                    fun=lowerInc, args=list(a=ss[1])) +
   geom_function(aes(x, 
                     col=ssc[2]),
                 fun=lowerInc, args=list(a=ss[2])) +
   geom_function(aes(x, 
                     col=ssc[3]),
                 fun=lowerInc, args=list(a=ss[3])) +
   geom_function(aes(x, 
                     col=ssc[4]),
                 fun=lowerInc, args=list(a=ss[4]))
grid.arrange(p1, p2, layout_matrix=matrix(c(1,1,1, 2,2,2,2), nrow=1))
@   
\caption{
  Upper incomplete gamma functions $\Gamma(s, x)$ (left) and lower
  function $\gamma(s, x)$ (right) as functions of $x$ (horizontal
  axis) and $s$ (color).
}
\label{fig:example-incomplete-gamma}
\end{figure}


\paragraph{Modified Bessel Functions}
\label{sec:modified_bessel_function}
\index{functions!Bessel!modified|textbf}

Modified Bessel function of the first kind:
\begin{equation}
  I_{\alpha}(x) =
  \sum_{k=0}^{\infty} 
  \frac{1}{k! \cdot\Gamma(\alpha + k + 1)}
  \left( \frac{x}{2} \right)^{2k + \alpha}
\end{equation}
Modified Bessel function of the second kind:
\begin{equation}
  \label{eq:modified-bessel-function-2nd-kind}
  K_{\alpha}(x) =
  \frac{\pi}{2}
  \frac{I_{-\alpha}(x) - I_{\alpha}(x)}
  {\sin \alpha\pi}
\end{equation}

\paragraph{Beta function $B(a,b)$}
\label{sec:beta-function}
\index{functions!beta|textbf}

\begin{equation}
  \label{eq:beta-function}
B(p,q) = \frac{\Gamma(p) \Gamma(q)}{\Gamma(p+q)} =
  \int_0^1 x^{p-1} (1-x)^{q-1} \,\dif x,
\end{equation}
where $p>0$ and $q>0$.


\subsection{Other functions}
\label{sec:other-functions}

\subsubsection{Economics-related functions}
\label{sec:economics-related-functions}

\paragraph{Constant Elasticity of Substitution utility function}
\index{constant elasticity of substitution|textbf}

For $\rho < 1$ and goods $x_{i}$ and parameters $a_{i} \ge 0$, $i=1\dots n$, we have
\begin{equation}
  \label{eq:ces-utility}
  U(x_{1}, x_{2}, \dots x_{n}) =
   \left( \sum_{i}
    a_{i} x_{i}^{\rho} \right)^{1/\rho}.
\end{equation}
$e = 1/(1-\rho)$ is \emph{elasticity of substitution}. \index{elasticity of
substitution|textbf}
If $e = 1$ (i.e. $\rho = 0$), CES utility reduces to Cobb-Douglas utility
\begin{equation}
  \label{eq:ces-cobb-douglas-utility}
  U(x_{1}, x_{2}, \dots x_{n}) =
  \prod_{i} x_{i}^{a_{i}}
\end{equation}
(see \hyperref[sec:ces-limit-cobb-douglas]{CES limit}).

Figure~\ref{fig:ces-utility} plots
three cases with different $\rho$.  $\rho=1$ is linear
utility with perfect substitutability, $\rho=0$ is Cobb-Douglas
utility, and $\rho=-\infty$ is Leontieff's utility where goods are
perfect complements.

\begin{figure}[ht]
\begin{center}
<<h=h1.3>>=
U <- function(x, y, r=1) {
   (ax*x^r + ay*y^r)^(1/r)
}
nGrid <- 100
ax <- ay <- 1
x <- y <- seq(0, 1, length.out=nGrid)
xy <- expand.grid(x=x, y=y)
#
r <- 0.95
xy$u0.9 <- U(xy$x, xy$y, r)
p1 <- ggplot(xy) + 
   stat_contour(aes(x, y, z=u0.9)) +
   ggtitle(substitute(paste(rho, " = ", r), list(r=r))) +
   theme(axis.title = element_blank(),
         axis.text = element_blank(),
         axis.ticks = element_blank())
#
r <- 0.05
xy$u0.1 <- U(xy$x, xy$y, r)
p2 <- ggplot(xy) + 
   stat_contour(aes(x, y, z=u0.1)) +
   ggtitle(substitute(paste(rho, " = ", r), list(r=r))) +
   theme(axis.title = element_blank(),
         axis.text = element_blank(),
         axis.ticks = element_blank())
#
r <- -10
xy[["u3"]] <- U(xy$x, xy$y, r)
p3 <- ggplot(xy) + 
   stat_contour(aes(x, y, z=u3)) +
   ggtitle(substitute(paste(rho, " = ", r), list(r=r))) +
   theme(axis.title = element_blank(),
         axis.text = element_blank(),
         axis.ticks = element_blank())
grid.arrange(p1, p2, p3, nrow=1)
@ 
\end{center}
  \caption{CES utility function for three different $\rho$ values.}
  \label{fig:ces-utility}
\end{figure}

\subsubsection{Statistics-related functions}
\label{sec:statistics-related-functions}

\paragraph{Softmax function}
\label{sec:softmax-function}

\index{softmax|textbf}
For $K$-dimensional input vector $\vec{x}$, the $i$-th component of softmax is
\begin{equation}
  \label{eq:softmax-function}
  S(\vec{x})_{i} = \frac{\me^{x_{i}}}{\sum_{j} \me^{x_{j}}}
  \qquad i \in \{1, 2, \dots, K\}.
\end{equation}
This describes discrete probability distribution between $K$ states
with corresponding weight $\me^{x_{k}}$.  Softmax transformation is
the same as multinomial logit transformation.
In case of $S(\vec{x}/T)$,
at the limit where $T\to 0$, all the mass is put on the state with the
largest weight.  If $T\to\infty$, all mass is spread uniformly across
all the states.

Derivatives:
\begin{align}
  \label{eq:softmax-derivatives}
  \pderiv{x_{j}} S(x)_{i} &=
                             \begin{cases}
                               -S(x)_{i} \cdot S(x)_{j} & \text{if}\quad
                               i\not=j\\
                               S(x)_{i} \cdot (1 -  S(x)_{j}) & \text{if}\quad
                               i=j\\
                             \end{cases}
  \\
  \pderiv{x_{j}} \log S(x)_{i} &=
                             \begin{cases}
                               -S(x)_{j} & \text{if}\quad
                               i\not=j\\
                               1 -  S(x)_{j} & \text{if}\quad
                               i=j\\
                             \end{cases}
\end{align}


\paragraph{Softplus function}
\label{sec:softplus-function}

In 1-dimensional case
\begin{equation}
  \label{eq:softplus-function}
  f(x) = \log(1 + \me^{x}).
\end{equation}
It is asymptotically equal to $f(x) = 0$ if $x \to -\infty$ and $f(x)
= x$ if $x\to\infty$ and can be used as a smooth replacement for
ReLU. 


\subsection{Variable transformations}
\label{sec:variable-transformations}

\paragraph{Logistic transformation}
\index{logistic transformation|textbf}
Logistic transformation can be used to map $\Real \to [0,1]$:
\begin{equation}
  \label{eq:logistic-transformation}
  y = \Lambda(x) = \frac{1}{1 + \me^{-x}}
  \qquad
  x = \log\frac{y}{1 - y}
\end{equation}




\newpage
\section{Calculus}
\label{sec:calculus}

\subsection{Limits}
\label{sec:limits}

\subsubsection{Limits in general}
\label{sec:limits-in-general}

\paragraph{$\liminf$ and $\limsup$}
\index{lim inf|textbf}
\index{lim sup|textbf}

Definition:
\begin{equation}
  \label{eq:lim-inf}
  \liminf_{n\to\infty} \equiv \lim_{n\to\infty} \left( \inf_{m\ge n} x_{m} \right)
\end{equation}

\paragraph{$\plim$: convergence in probability}

A sequence of RV-s $\{X_{n}\}$ \emph{converges in probability}
\index{convergence in probability|textbf}
\index{plim|see {convergence in probability}}
to a RV $X$ if for all
$\epsilon>0$ 
\begin{equation}
  \label{eq:convergence-in-probability}
  \lim_{n\to\infty} \Pr( |X_{n} - X| > \epsilon) = 0.
\end{equation}
In that case we write
\begin{equation}
  \label{eq:plim}
  X_{n} \xrightarrow{p} X
  \quad\text{or}\quad
  \plim_{n\to\infty} X_{n} = X.
\end{equation}


\paragraph{L'Hôpital's rule}
\label{sec:lhopitals-rule}
\index{lhopitals rule@L'Hôpital's rule|textbf}
Let $f$ and $g$ be differentiable functions, except possibly at $c$,
and $g'(x) \not=0$ if $x \not = c$, then
\begin{equation}
  \label{eq:lhopitals-rule}
  \lim_{x\to c}
  \frac{f(x)}{g(x)} =
  \frac{f'(x)}{g'(x)}
\end{equation}
if $\lim_{x\to c} f(x) = \lim_{x\to c}g(x) = 0$, or $\infty$.

\subsubsection{Other limits}
\label{sec:other-limits}

\begin{equation}
  \lim_{x\to 0} x \cdot \log x = 0
\end{equation}
\begin{proof}
  Write $x \log x = (\log x)/(1/x)$ and use \hyperref[sec:lhopitals-rule]{L'Hospital's rule}.
\end{proof}

\paragraph{Constant Elasticity of Substitution}
\label{sec:ces-limit-cobb-douglas}
\index{constant elasticity of substitution}

If elasticity of substitution $1/(1- \rho) = 1$ (i.e. $\rho = 0$), CES
utility is Cobb-Douglas utility:

\begin{equation}
  \label{eq:ces-limit}
  \lim_{\rho\to 0}\left( \sum_{i} 
    a_{i}\, x_{i}^{\rho} \right)^{1/\rho} =
  \prod_{i} x_{i}^{a_{i}}
\end{equation}
\begin{proof}
  Take log of the sum and use use the \hyperref[sec:lhopitals-rule]{L'Hospital's rule}.
\end{proof}

\paragraph{Normal Distribution}

\begin{equation}
  \label{eq:phi(x)/Phi(x)}
  \lim_{x\to \infty}
  \frac{1}{x} \frac{\phi(x)}{1 - \Phi(x)} = 1,
\end{equation}
where $\phi(\cdot)$ and $\Phi(\cdot)$ are normal density and
distribution functions.

\begin{proof}
  The fraction can be written as
  \begin{equation*}
    \frac{\phi(x)}{1 - \Phi(x)}
    =
    \frac{\phi(x)}{\int_{x}^{\infty} \phi(s) \dif s}.
  \end{equation*}
  The integral can be expressed as the Rieman limit
  \begin{equation*}
    \int_{x}^{\infty} \phi(s) \dif s
    =
    \lim_{\delta \to 0} 
    \big[
    \phi(x) \delta +
    \phi(x + \delta) \delta +
    \phi(x + 2 \delta) \delta +
    \dots 
    \big].
  \end{equation*}
  Using the expression for $\phi(\cdot)$ we get
  \begin{equation*}
    \phi(x + \delta) = \phi(x) \me^{-x \delta} \me^{-\delta^{2}/2}
  \end{equation*}
  and hence we may write the denominator in \eqref{eq:phi(x)/Phi(x)}
  as
  \begin{equation*}
    x \phi(x)
    \lim_{\delta \to 0} 
    \big[
    1 +
    \me^{-x \delta} \me^{-\delta^{2}/2} +
    \me^{-2 x \delta} \me^{-4 \delta^{2}/2} +
    \me^{-3 x \delta} \me^{-9 \delta^{2}/2} +
    \dots 
    \big] \delta.
  \end{equation*}
  This expressions $\me^{-x \delta}$, $\me^{-2 x \delta}$ and so on
  for a geometric sequence with sum $1/(1 - \me^{-x \delta}) \approx 1/x
  \delta$ as $x \delta \to 0$.  Accordingly, we let $\delta \to 0$ and $x
  \to \infty$ in such as way that $x \delta \to 0$.  We have to show
  that the other terms $\me^{n^{2} \delta^{2}/2}$ do not ``disturb''
  the geometric sequence too much.

  Now find $n^{*}$, starting of which the residual sum on the
  geometric sequence $1 + \me^{-x \delta} + \me^{-2 x \delta} + \dots$
  is less than $\varepsilon > 0$:
  \begin{equation*}
    \sum_{n=n^{*}}^{\infty} \me^{-n x \delta} < \varepsilon
    \qquad\Rightarrow\qquad
    n^{*} > -\frac{\log \varepsilon + \log(1 - \me^{-x \delta})}{x \delta}
  \end{equation*}
  choose $n^{**} > -\frac{\log \varepsilon}{x \delta} + 1 > n^{*}$.
  Now find 
  \begin{equation*}
    \exp\frac{{n^{**}}^{2} \delta^{2}}{2}
    =
    \exp\left(
      \frac{\log^{2}\varepsilon}{2 x^{2}}
      +
      \delta\frac{\log\varepsilon}{x}
      +
      \frac{\delta^{2}}{2}
    \right).
  \end{equation*}
  Because all the terms in parenthesis converge to as $x\to\infty$ and
  $\delta \to 0$, the exponent converges to 1.  Hence, at the limit, the
  Rieman sum is solely determined by the geometric sequence, and we
  have denominator in \eqref{eq:phi(x)/Phi(x)} equal to $\phi(x)$.
\end{proof}

\subsubsection{Limits Involving Factorial}
\label{sec:factorial}


\paragraph{Stirling's formula}
\index{Stirling's formula|textbf}
\begin{equation}
  \log n! = n \log n - n + O( \log n)
\end{equation}

Or a more precise version
\begin{equation}
  \label{eq:more-precise-stirling-formula}
  \log n! \approx \frac{1}{2} \log(2\pi n) + n\log n - n
\end{equation}

\selectlanguage{estonian}
\paragraph{faktoriaalide jagatis}
\begin{equation}
\lim_{n\to\infty} \frac{n! }{ (n-m)!} = n^m
\label{eq:faktoriaalide jagatis}
\end{equation}
Märkus: siin on eeldatud, et $m \not\to \infty$. Jaga läbi, arvesta, et
$n-1 \approx n$.
\selectlanguage{english}
A special case:
\begin{equation}
 \lim_{n\to\infty} \binom{n}{m} = \lim_{n\to\infty} \frac{n! }{ (n-m)! m!}
= \frac{n^m }{ m!}.
\end{equation}


\subsection{Boundedness and Convergence}
\label{sec:convergence}

\subsubsection{General Concepts}
\label{sec:boundedness-convergence-general-concepts}

\paragraph{Uniform Boundedness}
\index{uniform boundedness|textbf}

Let $X$ be a set and $Y$ a metric space with metric $d(\cdot,\cdot)$.
A family of functions $\mathcal{F} = \{f_{i}: X \to Y, i \in
\mathcal{I}\}$ is uniformly bounded if there exist $a\in Y$ and $M \in
\Real$ such that
\begin{equation}
  \label{eq:uniform_boundedness}
  d( f_{i}(x), a) \le M
  \quad 
  \forall i \in \mathcal{I}, x \in X
\end{equation}
In particular, $M$ and $a$ must be a common elements for each $i$.


\paragraph{Lipschitz Condition}
\index{Lipschitz condition|textbf}
(also \emph{Lipschitz continuity})
\index{Lipschitz continuity|see {Lipschitz condition}}
Function $f(x)$ satisfies Lipschitz condition if exists $L > 0$ such that
\begin{equation}
  \label{eq:lipschitz_condition}
  |f(\vec{x}_{1}) - f(\vec{x}_{2})| 
  \le L \cdot d(\vec{x}_{1}, \vec{x}_{2})
  \quad \forall \vec{x}_{1}, \vec{x}_{2} 
  \text{ in range of $f(\cdot)$}
\end{equation}
where $d(\cdot,\cdot)$ is the metric distance.


\paragraph{Equicontinuity}

\subparagraph{Pointwise equicontinuity}
Let $X$ and $Y$ be metric spaces with metric $d(\cdot,\cdot)$.
A family of functions $\mathcal{F} = \{f_{i}: X \to Y, i \in
\mathcal{I}\}$ is equicontinuous at point $x$ if for every $\epsilon >
0$ there exists $\delta(\epsilon, x)$ such that
\begin{equation}
  \label{eq:pointwise-equicontinuity}
  \sup_{i \in \mathcal{I}} d( f_{i}(\tilde x), f_{i}(x)) \le \epsilon
  \quad 
  \forall \tilde x: d(\tilde x, x) < \delta(\epsilon, x).
\end{equation}
Note: $\delta$ must not depend on $i$, but may depend on $x$, $\epsilon$.


\subsubsection{Tests}
\label{sec:boundedness-convergence-tests}

\paragraph{Cauchy root test}
\index{Cauchy root test|textbf}

Series $\sum_{n=1}^{\infty} a_{n}$ converges if
\begin{equation}
  \label{eq:cauchy-root-test-condition}
  R = \limsup_{n\to\infty} \sqrt[n]{|a_{n}|} < 1.
\end{equation}
If $R > 1$ the series diverges.



\clearpage
\subsection{Differentiation}
\index{differentiation}

\subsubsection{Concepts}
\label{sec:differentiation-concepts}

\paragraph{Directional Derivative}

\index{directional derivative|textbf}

\begin{equation}
  f'(\vec{x}; \vec{u})
  =
  \lim_{h \to 0}
  \frac{f(\vec{x} + h\vec{u}) - f(\vec{x})}
  {h}
\end{equation}


\paragraph{Subderivative}
\label{sec:subderivative}

For a convex function, \emph{subderivative} \index{subderivative|textbf}
$f:I\to\Real$ at a point $x_{0}$ in the open interval $I$ is a real number $c$
such that
\begin{equation}
  \label{eq:subderivative}
    f(x)-f(x_{0})\geq c(x-x_{0})  
\end{equation}
for all $x$ in $I$.

\paragraph{Jacobian Matrix}
\label{sec:jacobian_matrix}

\index{Jacobian|textbf}
is the derivative matrix of a vector function w.r.t. a vector
argument.  It is layed out in numerator layout (see
\fullref{sec:gradient-of-scalar-function}). 
\index{numerator layout}. 

\begin{equation}
  \mathcal{J} = 
  \begin{bmatrix}
    \dfrac{\partial \mathbf{f}}{\partial x_1} & \cdots &
    \dfrac{\partial \mathbf{f}}{\partial x_n} 
  \end{bmatrix}
  = 
  \frac{\partial (f_1, \dots, f_n)}
  {\partial (x_1, \dots, x_n)}
  =
  \vec{D}\,\vec{f}(\vec{x})
  =
  \begin{bmatrix}
    \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
    \vdots & \ddots & \vdots\\
    \dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial
      f_m}{\partial x_n} 
  \end{bmatrix}
\end{equation}

\paragraph{Envelope theorem}
\label{sec:envelope_theorem}

\selectlanguage{estonian}
Olgu $M$ defineeritud kui optimum funktsioonist $f$:
\begin{equation}
  M(a) = \max_{x} f(x, a),
\end{equation}
kus $a$ on parameeter.  Siis
\begin{equation}
  \frac{\dif M(a)}{\dif a} =  \frac{\partial f(x^*, a)}{ \partial a} \Bigg|_{x^* = x(a)}.
\end{equation}
\selectlanguage{english}




\subsubsection{Simple Derivatives}

\begin{align}
  \pderiv{x} a^{x} &= a^{x} \log a
  \\
  \tan \phi' &= \frac{1}{\cos^2 \phi} 
  \\
  \arcsin x' &= \frac{1}{\sqrt{1 - x^{2}}}
  \\
  \arctan x' &= \frac{1}{1+x^2}
  \\
  \pderiv{x} \sqrt{1 - x^{2}}
                   &=
                     -\frac{x}{\sqrt{1-x^2}}
                     = -\frac{x}{y}
  \\
  \pderiv[][2]{x} \sqrt{1 - x^{2}}
                    &= 
                      -\frac{1}{\sqrt{1-x^2}}
                      \left(1 + \frac{x^{2}}{1 - x^{2}} \right)
                      =
                      -\frac{1}{y^{3}}
  \\[2ex]
  \pderiv{x} \sqrt{\frac{1-x^{2}}{1 + x^{2}}}
  &=
    - \frac{2}{\sqrt{(1-x^{2})(1 + x^{2})}}
    \frac{x}{1 + x^{2}}
    =
    -\frac{x}{y} \frac{1 + y^{2}}{1 + x^{2}}
\end{align}


\subsubsection{Normal Density Related Derivatives}

\paragraph{One-Dimensional Case}
\begin{align}
  \pderiv{x} \Phi(x)
  &=
    \phi(x)
  \\[1.7ex]
  \pderiv{\vec{x}} \Phi( \vec{x}^{\transpose} \cdot \vec{\beta})
  &=
    \phi(\vec{x}^{\transpose} \cdot \vec{\beta}) \vec{\beta}
  \\[1.7ex]
%
  \pderiv{x}
  \phi \left( \frac{x-\mu}{\sigma} \right)
                            &=
  - \frac{1}{\sigma} 
    \left( \frac{x-\mu}{\sigma} \right)
                              \phi \left( \frac{x-\mu}{\sigma} \right)
  \\[1.7ex]
  \pderiv{x} \phi( \vec{x}^{\transpose} \cdot \vec{\beta})
  &=
    - \phi( \vec{x}^{\transpose} \cdot \vec{\beta})
    (\vec{x}^{\transpose} \cdot \vec{\beta} \cdot)
    \vec{\beta}
\\[1.7ex]
\frac{\dif}{\dif \sigma} 
\frac{1}{\sigma} 
  \phi \left( \frac{x-\mu}{\sigma} \right)
                            &=
    \frac{1}{\sigma^{2}} 
    \left[ \left( \frac{x-\mu}{\sigma} \right)^{2} - 1 \right]
                                \phi \left( \frac{x-\mu}{\sigma} \right)
  \\[1.7ex]
  \pderiv{x} \log \Phi(x)
  &=
    \lambda(x)
  \\[1.7ex]
  \pderiv{\vec{x}} \log \Phi( \vec{x}^{\transpose} \cdot \vec{\beta})
  &=
    \lambda( \vec{x}^{\transpose} \cdot \vec{\beta}) \vec{\beta}
  \\[1.7ex]
  \pderiv{x} \lambda(x)
  &= -\lambda(x) [\lambda(x) + x]
  \\[1.7ex]
  \pderiv{\vec{x}} \lambda( \vec{x}^{\transpose} \cdot \vec{\beta})
  &= -\lambda( \vec{x}^{\transpose} \cdot \vec{\beta})
    [\lambda( \vec{x}^{\transpose} \cdot \vec{\beta}) +
    \vec{x}^{\transpose} \cdot \vec{\beta}]
    \vec{\beta}
  \\[1.7ex]
  \frac{\partial}{\partial \vec{x}\, \partial\vec{x}^{\transpose}}
  \log \Phi( \vec{x}^{\transpose} \cdot \vec{\beta})
  &=
    -\lambda( \vec{x}^{\transpose} \cdot \vec{\beta})
    \left[
    \lambda( \vec{x}^{\transpose} \cdot \vec{\beta}) +
    \vec{x}^{\transpose} \cdot \vec{\beta}
    \right] \vec{\beta}\cdot\vec{\beta}^{\transpose}
  \\[1.7ex]
  % notation
  \text{where}\qquad &
                 \lambda(x) = \frac{\phi(x)}{\Phi(x)}
\end{align}

\paragraph{Multi-Dimensional Case}

Let 
\begin{math}
  \vec{x} = \left(
    \begin{array}{r}
      x_{1} \\ x_{2}
    \end{array}
  \right),
\end{math}
$\mat{\Sigma} = \left[
 \begin{array}{rr} 1 & \rho \\
 \rho & 1
\end{array}\right]$ be the 2-dimensional variance-covariance matrix
and $\phi(\cdot,\cdot)$ the 2-D normal density, defined
in~\eqref{eq:N-Dimensional_normal}:
\begin{align}
  \pderiv{x_{1}}
  \phi(\vec{x}, \mat{\Sigma})
  &=
  \phi(\vec{x}, \mat{\Sigma})
  \frac{\rho x_{2} - x_{1}}{1 - \rho^{2}}
  \\
  \pderiv{\rho}
  \phi(\vec{x}, \mat{\Sigma})
  &=
  \phi(\vec{x}, \mat{\Sigma})
  \Bigg[
  \frac{\rho}{1 - \rho^{2}}
  \left(
    1 - \vec{x}' \mat{\Sigma}^{-1} \vec{x}
  \right)
  +
  \frac{x_{1} x_{2}}{1 - \rho^{2}}
  \Bigg]
  =
  \notag\\
  &=
  \phi(\vec{x}, \mat{\Sigma})
  \Bigg[
  \frac{\rho}{1 - \rho^{2}}
  -
  \frac{\rho}{(1 - \rho^{2})^{2}}
  (x_{1}^{2} - 2 \rho x_{1} x_{2} + x_{2}^{2})
  +
  \frac{x_{1} x_{2}}{1 - \rho^{2}}
  \Bigg]
  \\
  \frac{\partial^{2}}{\partial x_{1} \partial x_{2}}
  \phi(\vec{x}, \mat{\Sigma})
  &=
  \phi(\vec{x}, \mat{\Sigma})
  \left[
    \frac{(x_{1} - \rho x_{2})(x_{2} - \rho x_{1})}
    {(1 - \rho^{2})^{2}}
    +
    \frac{\rho}{1 - \rho^{2}}
  \right]
\end{align}


\subsubsection{Derivatives of Gamma Function}
\begin{eqnarray}
\Gamma'(x) &=&
  \psi(x) \Gamma(x)\\
\Gamma''(x) &=&
  \Gamma(x) \left[ \psi'(x) + \psi^2(x) \right],
\end{eqnarray}
where $\psi(x)$ is the \hyperref[sec:digamma]{digamma function}.
\index{digamma function}


\subsubsection{Differentiation of Sums}
\begin{align}
  \pderiv{x_{i}}\sum_{j} x_{j} &= 1
  \\
  \pderiv{x_{i}} 
  \left( \sum_{j} x_{j} \right)^{2}
  &= 2 \sum_{j} x_{j}
\end{align}


\subsubsection{General Differentiation Rules}

\selectlanguage{estonian}
\paragraph{Pöördfunktsiooni tuletis}
\begin{equation}
\frac{\dif}{\dif x} f^{-1}(x) = 
  \left. \frac{1}{\frac{\dif}{\dif x} f(x)} \right|_{x=f^{-1}(y)} =
  \frac{1}{f'[f^{-1}(y)]}
\end{equation}

\selectlanguage{english}
\paragraph{Chain Rule}
\label{sec:chain-rule}
\index{chain rule|textbf}

Let $y=y(x)$ and $x = x(z)$ so $y = y(x(z)) \equiv f(z)$.  The derivatives are:
\begin{align}
  \label{eq:chain-rule}
  \pderiv[f(z)]{z} =& 
  \frac{\partial y(x)}{\partial x} \cdot \pderiv[x(z)]{z}
                      \equiv
                      y'(x) \cdot x'(z)
  \\
  \frac{\partial^2 y(x(z))}{\partial z^2} 
  \equiv
  \frac{\partial^2 f(z)}{\partial z^2} 
  =&
  \frac{\partial^2 y(x)}{\partial x^2} 
  \left(\frac{\partial x(z)}{\partial z} \right)^2 
     +
     \pderiv[y(x)]{x} \frac{\partial^2 x(z)}{\partial z^2}
\end{align}
See also the \hyperref[sec:matrix-calculus-chain-rule]{vector form} in
Section~\ref{sec:matrix-calculus-chain-rule}. 

If $x(z)$ is a linear function, then the second term of
$\displaystyle\frac{\partial^2 y(x(z))}{\partial b^2}$ will fall away and we have
$\displaystyle \frac{\partial^2 f(z)}{\partial z^2} 
=
  \frac{\partial^2 y(x)}{\partial x^2} 
  \left(\frac{\partial x(z)}{\partial z} \right)^2$.

\paragraph{Implicit Function Differentiation}
\label{sec:implicit-function-differentiation}
\index{implicit function}

Define $y = g(x)$ implicitly through $f(x, y) = 0$.  The first
differential
\begin{equation}
  \label{eq:implicit-first-differential}
  \dif f(x,y) = f_{x}(x,y)\,\dif x + f_{y}(x, y)\,\dif y
  = 0
  \quad\Rightarrow\quad
  g'(y) = \frac{\dif y}{\dif x} =
  -\frac{f_{x}(x,y)}{f_{y}(x,y)}
\end{equation}
The second differential
\begin{multline}
  \label{eq:implicit-second-differential}
  \dif(f_{x}+ f_{y} y') =
  \\
  f_{xx}\,\dif x + f_{yx} y' \,\dif x + f_{y} y_{x}'\,\dif x +
  f_{yy} (y')^{2}\,\dif y + f_{xy}\,\dif y + f_{y} y_{y}'\,\dif y
  \\
  = 0
\end{multline}
Note that $y_{y}'$, the partial derivative of $y'$ w.r.t $x$, is 0.  Now
\begin{equation}
  \label{eq:implicit-second-derivative}
  y''= y_{x}' =
  -\frac{1}{f_{y}} \left(
    f_{xx} + 2 f_{xy} y' + f_{yy} y'^{2} 
  \right)
\end{equation}



\subsection{Matrix Calculus}

\subsubsection{Gradient of scalar function}
\label{sec:gradient-of-scalar-function}

Let $f: \Real^{K} \to \Real$ be a scalar-valued function on
$K$-dimensional space $\Real^{K}$.  We denote this by $f(\vec{x})$
where $\vec{x} \in \Real^{K}$.  Define the gradient
\begin{equation}
  \nabla_{\vec{x}} f(\vec{x})
  \equiv
  \frac{\partial f(\vec{x})}{\partial \vec{x}}
  =
  \begin{bmatrix}
    \displaystyle\pderiv{x_1} f(\vec{x})\\
    \dots\\
    \displaystyle\pderiv{x_K} f(\vec{x}).
  \end{bmatrix}
\end{equation}
This is called \emph{denominator layout} \index{denominator
  layout|textbf} (Hessian formulation).  Alternatively one can use
\emph{numerator layout} \index{numerator layout|textbf} (Jacobian
formulation): 
\begin{equation}
  \nabla_{\vec{x}} f(\vec{x})
  \equiv
  \frac{\partial f(\vec{x})}{\partial \vec{x}}
  =
  \begin{bmatrix}
    \displaystyle\pderiv{x_1} f(\vec{x}),
    \dots,
    \displaystyle\pderiv{x_K} f(\vec{x}).
  \end{bmatrix}
\end{equation}
Below we stay with denominator layout.


\subsubsection{Chain Rule}
\label{sec:matrix-calculus-chain-rule}

Let $y = y(\vec{x})$ and $\vec{x} = \vec{x}(z)$ where $y$ is
a scalar and $\vec{x}$ and $\vec{z}$ are vectors.
The chain rule for the first derivative:
\index{chain rule}
\begin{equation}
  \label{eq:matrix-calculus-chain-rule-1}
  \pderiv[y(\vec{x}(z))]{z} 
  =
  \pderiv[\vec{x}(z)^{\transpose}]{z}\cdot
  \pderiv[y(\vec{x})]{\vec{x}}.
\end{equation}
Note the first derivative is a scalar.  See also the
\hyperref[sec:chain-rule]{chain rule in scalar form} in
Section~\ref{sec:chain-rule}. 

Chain rule for the second derivative:
\begin{equation}
  \label{eq:matrix-calculus-chain-rule-2}
  \frac{\partial^{2} y(\vec{x}(z))}{\partial z^{2}}
  =
  \frac{\partial^{2} \vec{x}(z)^{\transpose}}{z^{2}} \cdot
  \pderiv[y(\vec{x})]{\vec{x}}
  +
  \frac{\partial \vec{x}(z)^{\transpose}}{z} \cdot
  \frac{\partial^{2} y(\vec{x})}{\partial \vec{x}^{\transpose}\partial\vec{x}} \cdot
  \frac{\partial \vec{x}(z)}{z}.
\end{equation}
Note: this is a scalar.


\subsubsection{Other Rules}
\label{sec:matrix-calculus-other-rules}

Let $\vec{x}$ and $\vec{\beta}$ be a $K \times 1$
a vectors.  Then:
\begin{alignat}{2}
  \pderiv[\vec{x}]{\vec{x}^{\transpose}}
  &=
  \pderiv[\vec{x}^{\transpose}]{\vec{x}} = 
  \begin{bmatrix}
    \pderiv[x_{1}]{x_{1}} & \pderiv[x_{1}]{x_{2}} & \hdots & \pderiv[x_{1}]{x_{K}}\\[1.2ex]
    \pderiv[x_{2}]{x_{1}} & \pderiv[x_{2}]{x_{2}} & \hdots & \pderiv[x_{2}]{x_{K}}\\[1.2ex]
    \vdots               & \vdots              & \ddots & \vdots\\[1.2ex]
    \pderiv[x_{K}]{x_{1}} & \pderiv[x_{K}]{x_{2}} & \hdots & \pderiv[x_{K}]{x_{K}}\\
  \end{bmatrix}
  =
  \mat{I}
  \\
  \frac{\partial \vec{x}^{\transpose} \vec{\beta}}{\partial \vec{x}} &= 
  \pderiv[  \vec{\beta}^{\transpose} \vec{x}]{\vec{x}} = 
  \begin{bmatrix}
    \pderiv{x_{1}}( \beta_{1} x_{1} + \beta_{2} x_{2} + \hdots + \beta_{K} x_{K})\\[1.2ex]
    \pderiv{x_{2}}( \beta_{1} x_{1} + \beta_{2} x_{2} + \hdots + \beta_{K} x_{K})\\[1.2ex]
    \vdots\\
    \pderiv{x_{K}}( \beta_{1} x_{1} + \beta_{2} x_{2} + \hdots + \beta_{K} x_{K})\\[1.2ex]
  \end{bmatrix}
  =
  \vec{\beta}
  \\
  \pderiv[\;\vec{\beta}^{\transpose} \vec{x} \, \vec{x}^{\transpose} \vec{\beta}]
  {\vec{x}}
  &= 
  2 (\vec{x}^{\transpose} \vec{\beta}) \cdot \vec{\beta}
\end{alignat}


Let $\mat{A}$ be a $K\times N$ matrix, $\mat{B}$ be a $K\times K$ matrix
and $\vec{x}$ be a $N\times1$ vector.
\begin{alignat}{2}
  % 
  \pderiv[\mat{A} \vec{x}]{\vec{x}^{\transpose}} &=
  \mat{A} 
  &
  \pderiv[\vec{x}^{\transpose}\mat{A}^{\transpose}]{\vec{x}} &= \mat{A}^{\transpose}
  \\
  \frac{\partial \vec{x}^{\transpose}\vec{x}}{\partial \vec{x}} &= \mat{I}\vec{x} + \vec{x}\mat{I} = 
  2\vec{x} 
  & \qquad
  \pderiv[\vec{x}^{\transpose}\,\vec{x}]{\vec{x^{\transpose}}}
  &=
  2\vec{x}^{\transpose}
  \\
  \pderiv[\vec{x}^{\transpose} \mat{B} \vec{x}]{\vec{x}} 
  &= (\mat{B} + \mat{B}^{\transpose}) \vec{x}
\end{alignat}

Let $\vec{F}: \Real^{N\times 1} \to \Real^{M\times 1}$ and
$\vec{x} \in \Real^{N\times 1}$:
\begin{equation}
  \label{eq:matrix-diff-function-squared}
  \pderiv{\vec{x}}[\vec{F}(\vec{x})^{\transpose} \cdot \vec{F}(\vec{x})]
  =
  2 \pderiv[\vec{F}(\vec{x})^{\transpose}]{\vec{x}} \cdot \vec{F}(\vec{x})
\end{equation}

Let
\begin{equation}
  \mat{A} =
  \begin{pmatrix}
    a_{1\bullet}^{\transpose} \\[1.2ex] a_{2\bullet}^{\transpose} \\ \dots \\
    a_{k\bullet}^{\transpose} \\ \dots \\ a_{N\bullet}^{\transpose}
  \end{pmatrix}
\end{equation}
be a $N\times M$ matrix where $a_{i\bullet} = (a_{i1}, a_{i2}, \dots, a_{ik},
\dots, a_{iM})^{\transpose}$ is it's $i$-th row as $M\times1$ column vector.  Let $\mat{B}$ also be a $N\times M$ matrix. 
Let $\vec{z}$ be length-$N$ vector $\vec{z} = (z_{1}, z_{2}, \dots, z_{N})^{\transpose}$ .
Let $\vec{x}$ and $\vec{y}$ be length-$M$
vectors 
$\vec{x} = (x_{1}, x_{2}, \dots, x_{M})^{\transpose}$ 
and
$\vec{y} = (y_{1}, y_{2}, \dots, y_{M})^{\transpose}$.  Further, denote by $\vec{a}_{\bullet j}$ the
$j$-th column of matrix $\mat{A}$ in for of $N\times1$ column vector.
Now
\begin{equation}
  \pderiv{a_{j\bullet}^{\transpose}} \mat{A} \vec{a_{k\bullet}}
  = \delta_{kj} \mat{A} + 
  \begin{pmatrix}
    \vec{0}^{\transpose}\\
    \vec{0}^{\transpose}\\
    \vdots\\
    \vec{a}_{k\bullet}^{\transpose}\\
    \vdots\\
    \vec{0}^{\transpose}\\
  \end{pmatrix}
  \qquad\text{and}\qquad
  \pderiv{a_{j\bullet}^{\transpose}} \vec{z}^{\transpose} \mat{A} \vec{a_{k\bullet}}
  = \kronDelta_{kj} \vec{z}^{\transpose} \mat{A} +
  z_{j} \vec{a}_{k\bullet}^{\transpose}
\end{equation}
where $\vec{0}$ is a vector of zeros of suitable length and
$\kronDelta_{kj}$ is Kronecker delta. \index{Kroenecker symbol}
\begin{equation}
  \pderiv{\mat{A}} \vec{z}^{\transpose} \mat{A} \vec{a_{k\bullet}}
  = 
  \begin{pmatrix}
    \vec{0}^{\transpose}\\
    \vec{0}^{\transpose}\\
    \vdots\\
    \vec{z}^{\transpose} \cdot \mat{A}\\
    \vdots\\
    \vec{0}^{\transpose}\\
  \end{pmatrix}
  +
  (\vec{1}_{N} \kronProd \vec{a}_{k\bullet}^{\transpose}) 
  \elemProd
  (\vec{z} \kronProd \vec{1}_{M}^{\transpose})
\end{equation}
where $\kronProd$ is Kronecker product, $\elemProd$ is element-wise product,
and the the first matrix contains zeros everywhere except in the $k$-th row.

\begin{align}
  \pderiv{a_{ij}} \vec{x}^{\transpose} \mat{A}^{\transpose} \mat{A} \vec{y} 
  &=
  x_{j} ( \vec{a}_{i\bullet}^{T} \cdot \vec{y} ) +
   ( \vec{x}^{T} \cdot\vec{a}_{i\bullet} ) y_{j}
  \\[1.5ex]
  \pderiv{\vec{a}_{i\bullet}} \vec{x}^{\transpose} \mat{A}^{\transpose} \mat{A} \vec{y} 
  &=
  \vec{x}^{\transpose} \kronProd ( \vec{a}_{i\bullet}^{T} \cdot \vec{y} ) +
   ( \vec{x}^{T} \cdot\vec{a}_{i\bullet} ) \kronProd \vec{y}
    \label{eq:d-xAAy_dai.}
  \\[1.5ex]
  \pderiv{\mat{A}} \vec{x}^{\transpose} \mat{A}^{\transpose} \mat{A} \vec{y} 
  &=
  \vec{x} \cdot \vec{y}^{\transpose} \cdot \mat{A}^{\transpose} +
  \vec{y} \cdot \vec{x}^{\transpose} \cdot \mat{A}^{\transpose}
  \\[1.5ex]
  \pderiv{\mat{A}} \vec{x}^{\transpose} \mat{A}^{\transpose} \mat{B} \vec{y} 
  &=
  \vec{x} \cdot \vec{y}^{\transpose} \cdot \mat{B}^{\transpose}
  \\[1.5ex]
  \pderiv{\mat{B}} \vec{x}^{\transpose} \mat{A}^{\transpose} \mat{B} \vec{y} 
  &=
  \vec{y} \cdot \vec{x}^{\transpose} \cdot \mat{A}^{\transpose}
\end{align}
Note: as $\vec{a}_{i\bullet}^{T} \cdot \vec{y}$ and $\vec{x}^{T} \vec{a}_{i\bullet}$ are scalars, the Kronecker
product in \eqref{eq:d-xAAy_dai.} are equivalent to scalar product.
\begin{multline}
  \pderiv{\mat{A}} \left[
    \log \frac{1}{1 + \exp(-\vec{x}^{\transpose}\mat{A}^{\transpose} \mat{A} \vec{y})}
  \right]
  =\\=
  \frac{1}{1 + \exp(\vec{x}^{\transpose}\mat{A}^{\transpose} \mat{A} \vec{y})} \cdot
  \left[
    \vec{x} \cdot \vec{y}^{\transpose} \cdot \mat{A}^{\transpose} +
    \vec{y} \cdot \vec{x}^{\transpose} \cdot \mat{A}^{\transpose}
  \right]
\end{multline}


\paragraph{Traces and such}

\begin{alignat}{2}
  \pderiv{\mat{A}}\Tr (\mat{B}\mat{A})
  &=
  \mat{B}^{\transpose}
  &\qquad
  \pderiv{\mat{A}} \log|\mat{A}|
  &=
  \left(\mat{A}^{-1}\right)^{\transpose}
\end{alignat}
Let $\vec{f}(\vec{x})$ be a $M\times 1$ vector function and $\mat{B}$
a $M\times M$ matrix.
\begin{equation}
  \pderiv{\vec{x}} \left[ \vec{f}(\vec{x})' \mat{B} \vec{f}(\vec{x}) \right]
  = 
  \left[ \vec{D}\,\vec{f}(\vec{x}) \right]'
  (\mat{B} + \mat{B}') \vec{f}(\vec{x}),
\end{equation}
where $\vec{D}\,\vec{f}(\vec{x})$ is \hyperref[sec:jacobian_matrix]{Jacobian Matrix}.

\selectlanguage{estonian}
Oluline: ühte korrutamist ei tohi teiseks muuta. Näiteks kui avaldis
sisaldab nii maatrikskorrutist kui skalaariga korrutamist (skalaariga
korrutamine on põhimõtteliselt sama mis Kroneckeri korrutis
$\otimes$), ei tohi endist skalaariga korrutamist diferentseerimise
järel tõlgendada maatrikskorrutisena.  Mis siis et skalaari asemel on
nüüd maatriks:
\begin{equation}
\frac{\partial}{\partial \vec{\beta}'} \left[ ( \vec{\beta}'\vec{x} ) \otimes \vec{y}
  \right] =
  \frac{\partial \vec{\beta}'\vec{x}}{\partial \vec{\beta}'} \otimes \vec{y} =
  \vec{x}' \otimes \vec{y} = \vec{y}\vec{x}'.
\end{equation}
Skalaariga korrutamisel korrutatakse kõik maatriksi elemendid läbi
sama skalaariga, seega pääle tuletise võtmist tuleb kõik vektori $\vec{y}$
elemendid läbi korrutada tuletisvektoriga $\vec{x}'$.


\clearpage
\subsection{Integration}

\subsubsection{Simple Integrals}
\label{sec:simple_integrals}

\begin{align*}
  \int x^{\alpha} &= \frac{1}{\alpha + 1} x^{\alpha+1} + C,
  \quad \alpha \not= -1
  \\
  \int \log x &= x \log x -x + C
                \quad \text{proof: integrate by parts}
\end{align*}

\paragraph{Leibnitz' Rule}
\begin{eqnarray}
  \pderiv{x}
  \int_{a(x)}^{b(x)} f(y) \;\dif y
  & = &
  f[b(x)] b'(x) - f[a(x)]a'(x)\\
%
  \pderiv{x}
  \int_{a(x)}^{b(x)} f(y,x) \;\dif y 
  & = &
  f[b(x),x] b'(x) - f[a(x),x]a'(x) + \notag\\
  & + & 
  \int_{a(x)}^{b(x)} \frac{\partial}{\partial x} f(y,x) \;\dif y
\end{eqnarray}



\subsubsection{Integrals related to probability distributions}

\paragraph{Normal distribution}
Let
\begin{equation}
  \phi(x)
  =
  \frac{1}{\sqrt{2\pi}}
  \me^{-\frac{1}{2} x^{2}}
\end{equation}
and define
\begin{equation}
  \label{eq:normal-cdf}
  \int \phi(x) \,\dif x
  \equiv
  \Phi(x)
\end{equation}
To prove the following, simply integrate $\phi(\cdot)$, in most cases
by parts.
\begin{align}
  \int_{a}^{b} \phi\left(\frac{x - \mu}{\sigma} \right) \,\dif x
  &=
  \sigma \Phi \left(\frac{b - \mu}{\sigma} \right) -
  \sigma \Phi \left(\frac{a - \mu}{\sigma} \right)
  \\
  \int \phi^2(x) \,\dif x
  &=
  \frac{1}{2\mpi} \Phi(\sqrt{2} x)
  \\
  \int x \phi(x) \,\dif x
  &=
  -\phi(x)
  \\
  \int_{-\infty}^\infty x \phi \left( \frac{x-\mu}{\sigma} \right) 
  \dif x 
  &=
  \sigma \mu \\
  % integral x phi(.)
  \int  x \phi \left( \frac{x-\mu}{\sigma} \right) \dif x 
  &=
  \sigma \mu
  \Phi \left( \frac{x-\mu}{\sigma} \right) 
  -
  \sigma^2
  \phi \left( \frac{x-\mu}{\sigma} \right) 
  + C
  \\
  %
  \int_a^b  x \phi \left( \frac{x-\mu}{\sigma} \right) \dif x 
  &=
  \sigma \mu \left[
    \Phi \left( \frac{b-\mu}{\sigma} \right) -
    \Phi \left( \frac{a-\mu}{\sigma} \right) \right] + \notag\\
  &+
  \sigma^2 \left[
    \phi \left( \frac{a-\mu}{\sigma} \right) -
    \phi \left( \frac{b-\mu}{\sigma} \right) \right]
  \\
  % x^2 phi(.)
  \int (x - \mu)^2 \phi (x - \mu) \, \dif x 
  &= 
    \Phi(x - \mu) -(x - \mu) \phi (x - \mu)
  \\
  % x^2 phi(x - mu / sigma)
  \int x^2 \phi \left( \frac{x - \mu}{\sigma} \right) \, \dif x 
  &= 
  \sigma (\mu^{2} + \sigma^{2}) 
    \Phi \left( \frac{x - \mu}{\sigma} \right) 
    -
    \sigma^2 (x + \mu) \phi \left( \frac{x - \mu}{\sigma} \right)
  \\
  \int_a^b x^2 \phi(x) \,\dif x
  &=
  a\phi(a) - b\phi(b) + \Phi(b) - \Phi(a)
  \\
  \int_{-\infty}^\infty x^2 \phi \left( \frac{x-\mu}{\sigma} \right)
  \dif x 
  &= 
  \sigma ( \mu^2 + \sigma^2) \\
  % 
  \int_{-\infty}^\infty \me^{\alpha x} 
  \phi \left( \frac{x-\mu}{\sigma} \right) \,\dif x 
  &= 
  \me^{\alpha \mu + \frac{1}{2} \sigma^2 \alpha^2} \\
  % 
  \int_s^t \phi(u) \log\phi(u) \,\dif u 
  &=
  \frac{1}{2}\left[ \Phi(s) - \Phi(t)\right] \left(1+\log 2\pi \right)
  + \frac{1}{2}\left[ t\phi(t) - s\phi(s)\right] 
  \nonumber\\
  &\\
  % 
  \int_s^t \phi(u) \log\phi(u - \mu) \,\dif u 
  &=
  \frac{1}{2}\left[ \Phi(s) - \Phi(t)\right]
  \left( 1 + \log 2\pi + \mu^2 \right) + 
  \nonumber\\
  & + \frac{1}{2}\left[ t\phi(t) - s\phi(s)\right] +
  \mu \left[ \phi(s) - \phi(t) \right]\\
  % 
  \int x \phi^2(x) \,\dif x
  &=
  -\frac{1}{2} \phi^2(x)\\
  %
  \int \phi(x) \Phi(x) \,\dif x
  &=
  \frac{1}{2} \Phi^2(x)\\
  %
  \int x \phi(x) \Phi(x) \,\dif x
  &=
  -\phi(x) \Phi(x) + \frac{1}{2\sqrt{\mpi}} \Phi(\sqrt{2}\,x)\\
  \int x^2 \phi(x) \Phi(x) \,\dif x
  &=
  \frac{1}{2} \Phi^2(x)
  -x \phi(x) \Phi(x)
  -\frac{1}{2} \phi^2(x)
\end{align}


\paragraph{Log-normal density}

\index{integral!log-normal}

Let $f(\cdot)$ be the log-normal density.
\begin{multline}
\int_a^\infty x f(x) \,\dif x
  =
  \int_a^b
  \frac{1}{\sqrt{2\pi}\sigma}
  \exp\left[
    -\frac{1}{2}
    \left(
      \frac{\log x - \mu}{\sigma}
    \right)^2
  \right]
  \,\dif x
  =
  \\
  =
  \left[1 - \Phi \left(
      \frac{\log a - \mu - \sigma^2}{\sigma}
      \right)
    \right]    
    \me^{\mu + \frac{1}{2}\sigma^2} 
\end{multline}
\begin{multline}
  \int_a^b x f(x) \,\dif x
  =
  \int_a^b
  \frac{1}{\sqrt{2\pi}\sigma}
  \exp\left[
    -\frac{1}{2}
    \left(
      \frac{\log x - \mu}{\sigma}
    \right)^2
  \right]
  \,\dif x
  =
  \\
  =
  \me^{\mu + \frac{1}{2}\sigma^2}
  \left[
    -\Phi \left(
      \frac{\log a - \mu - \sigma^2}{\sigma}
    \right)
    +\Phi \left(
      \frac{\log b - \mu - \sigma^2}{\sigma}
    \right)
  \right]
\end{multline}


\subsubsection{Other integrals}

\begin{align}
  \int_0^t \me^{-rT} \,\dif T
  &=
  \frac{1}{r}
  \left[
    1 - \me^{-rt}
  \right]
  \\
  \int \frac{\dif x}{x \, (1-x)} &= \log x - \log (1 - x)
  \\
  \int_{t_{0}}^{T} \frac{\dif T}{T \, (1-T)}
  &=
    \log \left[ \frac{T}{1-T} \frac{1-t_{0}}{t_{0}} \right]
  \\
  \int \frac{\dif x}{(p + q^{ax})^2} \,\dif x 
  &=
  \frac{x}{p^2} +
  \frac{1}{ap(p + q\me^{ax})} -
  \frac{1}{ap^2} \log (p + q\me^{ax})
  \\
  \int x (1 + \beta x)^{\gamma} \, \dif x
  &=
  \frac{1}{\beta} \frac{x}{\gamma + 1} 
  (1 + \beta x)^{\gamma + 1}
  -
  \frac{1}{\beta^{2}} \frac{1}{(\gamma + 1)(\gamma + 2)}
  (1 + \beta x)^{\gamma + 2}
  \\
  \int x^{2} (1 + \beta x)^{\gamma} \, \dif x
  &=
    \frac{1}{\beta} \frac{1}{\gamma + 1}
    \Bigg[
    x^{2} (1 + \beta x)^{\gamma + 1} - \notag\\
  &-
    \frac{2}{\beta} \frac{x}{\gamma + 2}
     (1 + \beta x)^{\gamma + 2} +
  \frac{2}{\beta^{2}} \frac{1}{(\gamma + 2)(\gamma + 3)} 
  (1 + \beta x)^{\gamma + 3} 
    \Bigg]
  \\
  \int \log x \,\dif x 
  &=
  x \log x - x
  \\
  \int \sin^{2} x \,\dif x
  &= 
    \frac{1}{2} x - \frac{1}{4} \sin 2x
  \\
  \int \me^{-ax} \dif x 
  &=
  \frac{1}{a} \left[ 1 - \me^{-ax} \right]\\
                                %
  \int x \me^x \dif x 
  &=
  x \me^x - \me^x\\
                                %
  \int x \me^{-x} \dif x
  &=
  -x \me^{-x} - \me^{-x}
  \\
  \int x \me^{\alpha x} \,\dif x
  &=
  \frac{x}{\alpha} \me^{\alpha x} - \frac{1}{\alpha^2} \me^{\alpha x}
  \\
  \int_a^b x \me^{\alpha x} \,\dif x
  &=
  \frac{1}{\alpha}
  \left[
    \me^{\alpha b} 
    \left(b - \frac{1}{\alpha}\right)
    -
    \me^{\alpha a} 
    \left(a - \frac{1}{\alpha}\right)
  \right]
  \\
  \int x^2 \me^{\alpha x} \,\dif x
  &=
  \frac{1}{\alpha} x^2 \me^{\alpha x} -
  \frac{2}{\alpha} \int x \me^{\alpha x} \,\dif x
  \\
  \int_a^b x^2 \me^{\alpha x} \,\dif x
  &=
  \frac{1}{\alpha}
  \left[
    \me^{\alpha b}
    \left(
      b^2 - \frac{2b}{\alpha} + \frac{2}{\alpha^2}
    \right)
    -
    \me^{\alpha a}
    \left(
      a^2 - \frac{2a}{\alpha} + \frac{2}{\alpha^2}
    \right)
  \right]
  \\
  \int_0^\infty x^\alpha \me^{-\beta x} \dif x 
  &=
  \frac{\Gamma(\alpha + 1)}{\beta^{\alpha+1}}
\end{align}
Let $f(\cdot)$ be a distribution function and $\bar F(\cdot)$ the
corresponding survival function:
\begin{equation}
  \int_c^b \left[
    f(x)
    \int_c^x w(y) \,\dif y
  \right]
  \,\dif x
  =
  \int_c^b
  \bar F(x) w(x) \,\dif x
\end{equation}

\paragraph{Euler's constant}
\index{Euler's constant|textbf}
\begin{equation}
\int_0^\infty e^{-z} \log z \,\dif z = c \approx -0,5772 \qquad
\end{equation}


\selectlanguage{estonian}
\subsubsection{Üldised integreerimise reeglid}
\paragraph{Muutuja vahetus integraali all}
Olgu vaja üle minna muutujatelt $(x_1 \ldots, x_N)$ muutujatele $(y_1,
\ldots, y_N)$.  Sel juhul
\begin{multline}
\int_V f(x_1, \ldots, x_N) \dif x_1 \ldots \dif x_N =\\
%
  = \int_V f[ y_1(x_1, \ldots, x_N), \ldots, y_N(x_1, \ldots, x_N) ]
    \frac{\dif y_1 \ldots \dif y_N}{|\mathcal{J}|} =\\
%
  = \int_V g(y_1, \ldots, y_N) 
    \frac{\dif y_1 \ldots \dif y_N}{|\mathcal{J}|},
\end{multline}
kus
\begin{equation}
|\mathcal{J}| = \left| \frac{\partial(y_1, \ldots, y_N)}
  {\partial(x_1, \ldots, x_N)} \right|
\end{equation}
on koordinaatteisenduse jakobjaani absoluutväärtus.


\selectlanguage{english}
\paragraph{Leibnitz' rule}
\label{sec:leibnitz-rule}
\index{Leibnitz rule|textbf}

Differentiation of integral where both the function and the bounds
depend on the variable we are differenting with respect to:
\begin{multline}
  \label{eq:leibnitz-rule}
  \frac{\dif}{\dif x} \left (\int_{a(x)}^{b(x)} f(x,t)\,\dif t \right )
  =\\=
  f\big(x,b(x)\big)\cdot \frac{\dif}{\dif x} b(x) - 
  f\big(x,a(x)\big)\cdot \frac{\dif}{\dif x} a(x) +
  \int_{a(x)}^{b(x)}\frac{\partial}{\partial x}
  f(x,t) \, \dif t
\end{multline}
where
\begin{math}
  -\infty < a(x), b(x) < \infty
\end{math}


\selectlanguage{estonian}
\subsubsection{Laplace'i teisendus}
\index{Laplace transform|textbf}

Laplace'i teisendus juhusliku muutuja $X$ jaotusfunktsioonist on
\begin{equation}
L_f (s) = \E \me^{-sX} = 
  \int \me^{-sx} dF_X (x).
\end{equation}
Laplace'i teisendus on sama mis momendifunktsioon.


\subsubsection{Numbriline integreerimine}

\paragraph{Monte-Carlo integraal}

Olgu vaja leida
\begin{equation}
  I = \int_a^b f(x)\, \dif x =
  (b - a) \int_a^b f(x) \frac{1}{b - a}\, \dif x =
  (b - a) \E [f(X)],
\end{equation}
kus $X \sim U(a, b)$.  Valimis suurusega $N$ olgu $x_1, \ldots x_N
\sim i.i.d U(a, b)$.  Siis integraali hinnanguks on funktsiooni
väärtuste keskmine ja veahinnanguks tema standardhälve valimis:
\begin{eqnarray}
  \hat I_N &=& (b - a) \frac{1}{N} \sum_{i=1}^N f(x_i)\\
                                %
  \widehat{\var \hat I_N} &=&
  \frac{(b - a)^2}{N} \frac{1}{N} \sum_{i=1}^N 
  \left[f(x_i) - \frac{1}{N} \hat I_N \right]^2
\end{eqnarray}



\clearpage

\subsection{Differential Equations}
\label{sec:differential_equations}

Let $c$ be a constant.
\begin{align}
  \begin{split}
    y(x)' + P y &= Q
    \\
    y(x) &= \frac{Q}{P} + c \me^{-Px}
  \end{split}
  \\
  \begin{split}
    y(x)' + P(x) y &= Q(x)
    \\
    y(x) &= \me^{-\int P(x)\,\dif x}
    \int Q(x) \me^{\int P(x)\,\dif x} \,\dif x
    + c \me^{-\int P(x)\,\dif x}
  \end{split}
\end{align}



\clearpage
\subsection{Optimization}

\subsubsection[Second-order conditions]{Second-order maximum/minimum conditions for constrained
  optimisation}

The problem is
\begin{equation}
  \begin{split}
    \max z = f(x_1, x_2, \dots, x_n)\\
    \text{s.t. } 
    g(x_1, x_2, \dots, x_n) = 0.
  \end{split}
\end{equation}
Corresponding Lagrangian is
\begin{equation}
  Z = f(x_1, x_2, \dots, x_n) - \lambda g(x_1, x_2, \dots, x_n).
\end{equation}
Corresponding \emph{bordered Hessian} is:
\begin{equation}
  \begin{vmatrix}
    \bar{\mat H}
  \end{vmatrix}
  =
  \begin{vmatrix}
    0   & g_1    & g_2    & \dots & g_n\\
    g_1 & Z_{11} & Z_{12} & \dots & Z_{1n}\\
    g_2 & Z_{21} & Z_{22} & \dots & Z_{2n}\\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    g_n & Z_{n1} & Z_{n2} & \dots & Z_{nn}
  \end{vmatrix}
\end{equation}
and successive \emph{principal minors} are:
\begin{equation}
  \begin{vmatrix}
    \bar{\mat{H}}_2
  \end{vmatrix}
  =
  \begin{vmatrix}
    0   & g_1    & g_2\\
    g_1 & Z_{11} & Z_{12}\\
    g_2 & Z_{21} & Z_{22}
  \end{vmatrix}
  \qquad
  \begin{vmatrix}
    \bar{\mat{H}}_3
  \end{vmatrix}
  =
  \begin{vmatrix}
    0   & g_1    & g_2    & g_3\\
    g_1 & Z_{11} & Z_{12} & Z_{13}\\
    g_2 & Z_{21} & Z_{22} & Z_{23}\\
    g_3 & Z_{31} & Z_{32} & Z_{33}
  \end{vmatrix}
  \qquad
  \dots
  \qquad
  \begin{vmatrix}
    \bar{\mat{H}}_n 
  \end{vmatrix}
  =
  \begin{vmatrix}
    \bar{\mat H}
  \end{vmatrix}
\end{equation}
The second derivative $\dif^2 z$ is positive definite iff
\begin{equation*}
  \bar{\mat{H}}_2 < 0,
  \quad
  \bar{\mat{H}}_3 < 0,
  \quad
  \dots,
  \quad
  \bar{\mat{H}}_n < 0,
  \quad
\end{equation*}
and negavitve definite iff
\begin{equation*}
  \bar{\mat{H}}_2 > 0,
  \quad
  \bar{\mat{H}}_3 < 0,
  \quad
  \bar{\mat{H}}_n > 0,
  \quad
  \dots
  \quad
\end{equation*}
Note that $\bar{\mat{H}}_1$ is always negative.

Proof: \citet{chiang1984}.



\newpage
\subsubsection{Optimal control (dynamic optimisation)}

The problem is:
\begin{equation}
  \begin{split}
    \max_u & \int_0^T F(t, y, u) \,\dif t\\
    \text{s.t. }
    \dot y & = f(t, y, u)\\
    y(0) & = A
    \qquad
    y(T) \text{ free}.
  \end{split}
\end{equation}
Corresponding \emph{Hamiltonian} is
\begin{equation}
  H(t, y, u, \lambda)
  \equiv
  F(t, y, u) +
  \lambda(t) f(t, y, u).
\end{equation}
The first order conditions for optimum are
\begin{enumerate}
\item $\max_u H(t, y, u, \lambda) \quad\forall t \in [0,T]$ or, less
  generally, $\pderiv[H]{t} = 0$ (optimality condition).
\item $\dot y = \pderiv[H]{\lambda}$ (equation of motion for $y$).
\item $\dot \lambda = - \pderiv[H]{y}$ (equation of motion for $\lambda$).
\item $\lambda(T) = 0$ (transversality condition).
\end{enumerate}

Proof: \citet{miller1979}



\subsubsection{Newton-Raphson maximization}
\label{sec:Newton-Raphson}

Non-linear continuous function of $N$-dimensional parameter can,
under suitable assumptions, be approximated as $N$-dimensional parabola.
When running non-linear maximization, we may approximate the function
in this way at the initial value of the parameter vector.  The maximum
of the approximation can be used as the initial value for the next
step.

Let us maximise a function $l(\vec{\vartheta})$ where $\vec{\vartheta}$
is a $N$-dimesional parameter vector.  Let the initial value of the
parameter be $\vec{\vartheta}_0$.  From Taylor's approximation:
\begin{equation}
  l(\vec{\vartheta}) \approx
  l(\vec{\vartheta}_0) 
  + 
  \left.
    \frac{\partial l(\vec{\vartheta})}
    {\partial\vec{\vartheta}}
  \right|_{\vec{\vartheta} = \vec{\vartheta}_0}
  (\vec{\vartheta} - \vec{\vartheta}_0) 
  +
  \frac{1}{2}
  (\vec{\vartheta} - \vec{\vartheta}_0)'
  \left.
    \frac{\partial^2 l(\vec{\vartheta})}
    {\partial\vec{\vartheta} \partial\vec{\vartheta}'}
  \right|_{\vec{\vartheta} = \vec{\vartheta}_0}
  (\vec{\vartheta} - \vec{\vartheta}_0)
\end{equation}
At the maximum $\partial l(\vec{\vartheta}) /
\partial\vec{\vartheta} = 0$ and hence the parameter value at the
maximum (the initial value for the next iteration):
\begin{equation}
  \vec{\vartheta}_1 = 
  \vec{\vartheta}_0 -
  \left[
    \left.
      \frac{\partial^2 l(\vec{\vartheta})}
      {\partial\vec{\vartheta} \partial\vec{\vartheta}'}
    \right|_{\vec{\vartheta} = \vec{\vartheta}_0}
  \right]^{-1}
  \left.
    \frac{\partial l(\vec{\vartheta})}
    {\partial\vec{\vartheta}}
  \right|_{\vec{\vartheta} = \vec{\vartheta}_0}
\end{equation}

The algorith requires either programming the analytical Hessian matrix
$\frac{\partial^2 l(\vec{\vartheta})} {\partial\vec{\vartheta}
  \partial\vec{\vartheta}'}$, or calculating the Hessian matrix by
numeric differentiation.  The first way may be complicated, the latter
one slow and subject to numerical errors.


\paragraph{BHHH maximization}
\index{BHHH|textbf}

BHHH is a particular version of Newton-Raphson algorithm, suitable for
maximizing log-likelihood function only.  BHHH uses the information
equality \index{information equality}
for approximating the Hessian:
\begin{equation}
  \E
  \left[
    \frac{\partial^2 l(\vec{\vartheta})}
    {\partial\vec{\vartheta} \partial\vec{\vartheta}'}
  \right]_{\vec{\vartheta} = \vec{\vartheta}_0}
  =
  - \E
  \left[
    \left.
      \frac{\partial l(\vec{\vartheta})}
      {\partial\vec{\vartheta}'}
    \right|_{\vec{\vartheta} = \vec{\vartheta}_0}
    % 
    \left.
      \frac{\partial l(\vec{\vartheta})}
      {\partial\vec{\vartheta}}
    \right|_{\vec{\vartheta} = \vec{\vartheta}_0}
  \right]
\end{equation}
This algorithm does not require Hessian matrix (this is approximated).
However, it typically requires around 10 times more iterations for
convergence as the approximation may be quite imprecise when initial
values are far off the target.  Note also that while the estimates are
exactly the same as in the case of NR algorithm, the standard errors
may be different on a finite sample \citep{calzolari+fiorentini1993}.






\newpage
\selectlanguage{estonian}
\section{Algebra}

\subsection{Mõisted}
\label{sec:algebra_mqisted}

\begin{description}
\item[proper subset] $A$ on $B$ \emph{proper subset} kui $A\subseteq
  B$ kui $B \not\subseteq A$.
\item[proper subspace] Kui $A$ ja $B$ ja $A$ on $B$ \emph{proper
    subset}.  Näiteks tasandi tõeline (lineaarne) alamruum on sirge.
\end{description}


\selectlanguage{english}
\subsection{Sets}

\subsubsection{Definitions}

\subparagraph{Lower Bound}

$a$ is lower bound of $S$ if $a \le x$ for all $x \in S$.

\subparagraph{Infimum}

\begin{equation}
  \label{eq:infimum}
  a = \inf S
  \quad
  \Leftrightarrow
  \quad
  \text{$a$ is the largest lower bound of $S$}
\end{equation}

\subsubsection{Basic Operations}
\label{sec:set_operations}

\paragraph{De Morgan's Laws}

\begin{align}
\big( \bigcap_{A \in F} A \big)^C & = 
  \bigcup_{A \in F} A^C
\\
\left( \bigcup_{A \in F} A \right)^C & = 
                                       \bigcap_{A \in F} A^C
\end{align}

\paragraph{Other}

\begin{align}
  A \setminus B &= A \cap B^C
                  \quad\text{\emph{set difference} or \emph{relative complement}}
  \\[1.3ex]
  A \triangle B
                &= (A \setminus B) \cap (B \setminus A)
                  \quad\text{symmetric difference}
  \\
                &= \{x : x \in A \oplus x \in B \}
                  \quad\text{where $\oplus$ is XOR operator}
  \\[1.3ex]
  \big(\bigcup_{i} A_{i}\big) \setminus (\bigcup_{i} B_{i}) &\subseteq
                                          \bigcup_{i} (A_{i} \setminus B_{i})
\end{align}

\paragraph{Algebra}

A collection $\mathcal{A}$ of subsets of $X$ is \emph{algebra}
\index{algebra|textbf}
(Boolean algebra) if
\begin{enumerate}
\item For each $A, B \in \mathcal{A}$, their union $A \cup B \in
  \mathcal{A}$.  (Finite union)
\item For each $A \in \mathcal{A}$, its complement $A^{C} = \Omega \setminus A
  \in \mathcal{A}$.
\item For each $A, B \in \mathcal{A}$, their intersection $A \cap B \in
  \mathcal{A}$.  Follows from 1, 2, and De Morgan's laws.
\end{enumerate}

\paragraph{Sigma Field}

A set $S$ is \emph{sigma field} iff
\index{sigma-field@$\sigma$-field|textbf}
\begin{enumerate}
\item $\Omega \in S$ ($\Omega$ is the base set)
\item $S$ is closed under countable union: $\cup_{i}^{n} E_{i} \in S
  \quad E_{i} \in S, i = 1,2,\dots$.  (Algebra was finite unions!)
\item $S$ is closed under complement: $E \in S \Rightarrow E^{C} \in S
  \quad \forall E \in S$
\item $S$ is closed under countable intersection: 
  $\cap_{i}^{n} E_{i} \in S \quad E_{i} \in S, i = 1,2,\dots$.  This
  follows from 2,3 and De Morgan's laws.
\end{enumerate}

\paragraph{Borel set}

Borel $\sigma$-field is
\begin{equation}
  \label{eq:sigma-field-all-intervals}
  \mathcal{B} = \cap \{
  \mathcal{F} : \text{
    $\mathcal{F}$ is a $\sigma$-field containing all intervals}
  \}.
\end{equation}
Elements of $\mathcal{B}$ are \emph{Borel sets}.
\index{borel set|textbf}
Borel sets are closed under coutable unions and countable
intersections, 
if sets $S_{1}, S_{2}, \dots \in B$, then als $\cup_{n} S_{n} \in B$
and $\cap_{n} S_{n}\in B$.

\subsection{Simple Algebra}

\subsection{Logarithm}

\index{logarithm|textbf}
Properties
\begin{equation}
  \log x^\alpha = \alpha \log x 
  \qquad
  \text{and}
  \qquad
  \log( xy) = \log x + \log y
\end{equation}

\paragraph{Binomial Theorem}
\begin{equation}
(a+b)^n = \sum^n_{i=0} \binom{n}{i} \, a^{n-i} b^i
\end{equation}
where $\binom{n}{i}$ is the \emph{binomial coefficient},
\index{binomial coefficient|textbf}
number of
distinct combinations of $i$ elements out of $n$ elements in total:
$\binom{n}{i} \equiv C_n^i = \frac{\displaystyle n!}{
  \displaystyle (n-i)! i!}$.
\begin{align}
  \sum_{i=0}^{n} \binom{n}{i}
  &=
    2^{n}
  \\
  \sum_{i=0}^{n} \binom{n}{i}\: \me^{\alpha i}
  &=
  (1 + \me^{\alpha})^{n}
  \\
  \sum_{i=0}^{n} i \, \binom{n}{i}\: \me^{\alpha i}
  &=
  n \me^{\alpha}(1 + \me^{\alpha})^{n-1}
\end{align}


\selectlanguage{estonian}
\paragraph{Geomeetrilise jada summa}
\begin{equation}
S=1+q+q^2+q^3+... = \frac{1 }{ 1-q}.
\end{equation}
Tõestus: kirjuta välja $qS$, lahuta ja avalda $S$. Märkus: kui jada on
kujul $S'=q+q^2+q^3+...$, siis $S'=qS$. Oluline erijuht kui $q=\frac{1 }{
1+r}$:
\begin{equation}
 S = 1 + \frac{1 }{ 1+r} + \frac{1 }{ (1+r)^2} + ... = 1 + \frac{1 }{ r}.
\end{equation}
\begin{equation}
  \sum_{i=0}^\infty i p^i = \frac{p}{(1 - p)^2}
\end{equation}
Tõestus: kui $S$ on antud summa, siis avalda $S - pS$ \ldots

\paragraph{Eksponent piirväärtusena}
\begin{equation}
 \lim_{n\to\infty} \left( 1 + \frac{a}{ n} \right)^n = e^a.
\end{equation}
Tõestus: arenda Newtoni binoomvalemiga ritta, arvesta
\eqref{eq:faktoriaalide jagatis}.


\subsection{Taylori rida}
\label{sec:taylor-series}

\paragraph{Taylori rida}
\index{Taylor series|textbf}
Iga funktsiooni võib punkti $x_0$ ümbruses esitada astmereana:
\begin{align}
f(x) & =f(x_0) + f'(x_0)(x-x_0) + \frac{1}{ 2}f''(x_0)(x-x_0)^2 +
  ... = \notag\\
  & =\sum^\infty_{i=0} f^{(i)}(x_0) \frac{(x-x_0)^i }{ i!}.
\end{align}
Tõestus: kirjuta samasugune
astmerida tundmatute kordajatega välja, võrruta $f(x-x_0)$-ga ja võta
järjest tuletisi. 
\selectlanguage{english}

Multivariate case
\begin{align}
f(\vec{x}) & =
             f(\vec{x}_0) + 
             (\vec{x} - \vec{x}_0)^{\transpose} \,
             \Dif f(\vec{x}) \,\big|_{\vec{x} = \vec{x}_0} 
             +
             \frac{1}{2}
             (\vec{x} - \vec{x}_0)^{\transpose} \,
             \Dif^{2} f(\vec{x}) \,\big|_{\vec{x} = \vec{x}_0}
             (\vec{x} - \vec{x}_0)
             + \dots
\end{align}
where $\Dif f(\vec{x})$ is
\hyperref[sec:gradient-of-scalar-function]{gradient} \index{gradient} in 
\hyperref[sec:gradient-of-scalar-function]{denominator layout}.

A special case of Taylor series where $x_0=0$ is called Maclaurin series
\index{Maclaurin series|textbf}
\begin{equation}
f(x) =f(0) + f'(0)x + \frac{1}{ 2} f''(0)x^2 + ...
  = \sum^\infty_{i=0} f^{(i)}(0) \frac{x^i }{ i!}.
\end{equation}

\selectlanguage{estonian}
\paragraph{Eksponendi astmerida}
\begin{equation}
 e^x = 1 + x + \frac{1}{ 2}x^2 + \frac{1}{ 3!}x^3 + ... = \sum^\infty_{i=0}
\frac{x^i }{ i!}.
\end{equation}
Tõestus: arenda $e^x$ Taylori ritta.

\paragraph{Eksponendi piirväärtus}
\begin{equation}
 \lim_{x\to 0} e^x = 1+x.
\end{equation}
Tõestus: eksponendi astmereast. Märkus: piir\-väärtus $1+x$ on kõige
tavalisem, mida on vaja kasutada. Olenevalt ülesandest tuleb arvestada
rohkem (või ka vähem) astmerea liikmeid.


\paragraph{Logaritmi astmerida}
\begin{align}
 \log x 
  &=
    \log x_{0} +
    \frac{1}{x_{0}} (x - x_{0}) -
    \frac{1}{2x_{0}^{2}} (x - x_{0})^{2} +
    \frac{1}{3x_{0}^{3}} (x - x_{0})^{3} -
    \dots
  \\
  &= (x-1) - 
    \frac{(x-1)^2 }{ 2} + 
    \frac{(x-1)^3 }{ 3} - 
    \frac{(x-1)^4 }{4} + ...
\end{align}
Tõestus: arenda Taylori ritta $x_0=1$ ümbruses.

\selectlanguage{english}
\subsection{Fourier series}
\label{sec:fourier-series}

\index{Fourier series|textbf}
In complex numbers, functions
\begin{equation}
  \label{eq:fourier-complex-base}
  \phi_{n}(x) =
  \frac{1}{\sqrt{2\mpi}}
  \me^{inx}
  \qquad
  n = 0, \pm 1, \pm 2, \dots
\end{equation}
for an orthonormal basis in function space on interval $[0, 2\mpi]$.
Every function $f(x)$ on this interval can be approximated as
\begin{equation}
  \label{eq:fourier-approximation}
  f(x) \approx
  \sum_{-n}^{n} b_{n} \phi_{n}(x)
\end{equation}
where $b_{n}$ is computed using inner product $\inner{\cdot}{\cdot}$:
\begin{equation}
  \label{eq:fourier-series-coefficient}
  b_{n} = \inner{f}{\phi_{n}}
\end{equation}


\newpage
\subsection{Powers of sums}
\index{sums}
\index{powers of sums}

\begin{align}
\left( \sum_{i=1}^N x_i \right)^2 = &
  \sum_{i=1}^N x_i^2 + \sum_{\substack{i;j=1 \\ i\ne j}}^N
  x_i x_j
  \label{eq:(sum x)^2}\\
\left( \sum_{i=1}^N x_i \right)^3 = &
  \sum_{i=1}^N x_i^3 + 3 \sum_{\substack{i;j=1 \\ i\ne j}}^N
  x_i x_j^2 + \sum_{\substack{i;j;k=1 \\ i\ne j; j\ne k; k \ne i}}^N
  x_i x_j x_k\\
\left( \sum_{i=1}^N x_i \right)^4 = &
  \sum_{i=1}^N x_i^4 +
  4\sum_{\substack{i;j=1 \\ i\ne j}}^N x_i x_j^3 +
  3\sum_{\substack{i;j=1 \\ i\ne j}}^N x_i^2 x_j^2 +
  6\sum_{\substack{i;j;k=1 \\ i\ne j; j\ne k; k \ne i}}^N
    x_i x_j x_k^2 + \notag\\
  + &
  \sum_{\substack{i;j;k;l=1\\ i;j;k;l\ne}}^N x_i x_j
    x_k x_l
\end{align}
\selectlanguage{estonian}
Ühekordsetes summades on $N$ liiget, kahekordsetes $N(N-1)$,
kolmekordsetes $N(N-1)(N-2)$ ning neljakordses $N(N-1)(N-2)(N-3)$.

Tuletuskäik lähtub viimasel juhul niisugustest mõtetest:
\begin{itemize}
\item Kui komponentide indeksid ei tohi olla võrdsed, siis on järgmist
komponenti võimalik valida ühe võrra vähem
\item Esimesel liikmel võetakse sisse kõik komponendid, seega on
$C_0^4$ varianti.
\item Teisel liikmel on kaks komponenti ($x_i$ ja $x_j$), ühte
võetakse kolm korda, teist korra.  Seega tuleb valida üks, mis iga
kord välja jäetakse.  Seega $C_1^4$ võimalust.
\item Kolmandal liikmel valitakse mõlemad komponendid kahe kaupa.
Kokku on $C_2^4=6$ võimalust kahe kaupa valida, kuna aga pole vahet
kumb komponentidest on kumb, siis jääb järele pool nendest.
\item Neljandal liikmel on kolm komponenti, ruutliikme valimiseks on
$C_2^4=6$ võimalust.  Kuna teised liikmed on esimeses astmes, siis on
küll vahe, kumbad me välja valime.  Jääb 6.
\item Viimane, kõiki üks kord, $C_4^4=1$.
\end{itemize}

Kui $X \sim i.i.d$, siis
\begin{equation}
  \label{eq:E(sum x)^2}
  \E \left( \sum^N x_i \right)^2 =
  N \E X^2 + N(N - 1) (\E X)^2 =
  N^2 (\E X)^2 + N \var X
\end{equation}
\selectlanguage{english}

\newpage
\subsection{Equations}
\label{sec:equations}

\subsubsection{Cramer's Rule}
\label{sec:cramers-rule}

Consider equation 
\begin{equation}
  \label{eq:cramers-rule-equation}
  \mat{A} \vec{x} = \vec{b}
\end{equation}
where $\mat{A}$ is a $n\times n$ matrix, and $\vec{x}$ and $\vec{b}$
are $n\times 1$ vectors.  Cramer's rule \index{Cramer's rule|textbf}
is the solution
\begin{equation}
  \label{eq:cramers-rule-solution}
  x_{i} = \frac{|\mat{A}_{i}|}{|\mat{A}|}
\end{equation}
where $\mat{A}_{i}$ is matrix $\mat{A}$ with $i$-th column replaced by
$\vec{b}$. 


\subsubsection{Relationship between matrix elements}
\label{sec:matrix-element-relationship}

Consider matrix $\mat{R} =
\begin{pmatrix}
  r_{11} & r_{12} \\ r_{21} & r_{22}
\end{pmatrix}$.  We require:
\begin{enumerate}
\item Sum of rows:
  \begin{equation}
    \label{eq:matrix-relationship-rowsum}
    \frac{r_{21}+ r_{22}}{r_{11}+ r_{12} + r_{21} + r_{22}} = \alpha
  \end{equation}
\item First column ratio:
  \begin{equation}
    \label{eq:matrix-relationship-col1}
    \frac{r_{21}}{r_{11} + r_{21}} = \beta
  \end{equation}
\item Second column ratio:
  \begin{equation}
    \label{eq:matrix-relationship-col1}
    \frac{r_{22}}{r_{12} + r_{22}} = 1 - \beta.
  \end{equation}
\end{enumerate}
Then
\begin{align}
  r_{11} &= \frac{\alpha + \beta - 1}{\beta - \alpha} r_{22}\\
  r_{12} &= \frac{\beta}{1 - \beta} r_{22}\\
  r_{21} &= \frac{\beta}{1 - \beta} r_{11} =
           \frac{\beta}{1 - \beta}
           \frac{\alpha + \beta - 1}{\beta - \alpha} r_{22}.
\end{align}
If the elements must be positive, then either
\begin{equation}
  \label{eq:matrix-relationship-pos1}
  \alpha + \beta - 1 > 0
  \quad\text{and}\quad
  \beta > \alpha
\end{equation}
or
\begin{equation}
  \label{eq:matrix-relationship-pos2}
  \alpha + \beta - 1 < 0
  \quad\text{and}\quad
  \beta < \alpha.
\end{equation}


\newpage
\subsection{Linear Algebra}

\subsubsection{Vectors}
\index{vector|textbf}

\paragraph{Cross product}
(also \emph{vector product})
\index{vector!cross product|textbf}
\index{vector!vector product|textbf}

\begin{equation}
  \label{eq:cross-product}
  \vec{a}\times\vec{b}
  =
  \begin{pmatrix}
    a_{2}b_{3} - a_{3} b_{2} \\
    a_{3}b_{1} - a_{1}b_{3}\\
    a_{1}b_{2} - a_{2}b_{1}
  \end{pmatrix} =
  \begin{vmatrix}
    \vec{i} & \vec{j} & \vec{k}\\
    a_{1} & a_{2} & a_{3}\\
    b_{1} & b_{2} & b_{3}
  \end{vmatrix}
\end{equation}

\paragraph{Triple product}
\index{vector!triple product|textbf}

Triple product of three vectors is a scalar product of one with the
cross product of the two others:
\begin{equation}
  \label{eq:triple-product-definition}
  \vec{a} \cdot (\vec{b} \times \vec{c})
\end{equation}
Properties:
\begin{align}
  \label{eq:triple-product-properties}
  \vec{a} \cdot (\vec{b} \times \vec{c}) 
  &=
    \vec{b} \cdot (\vec{c} \times \vec{a}) 
    =
    \vec{c} \cdot (\vec{a} \times \vec{b}) 
    \\
  \vec{a} \cdot (\vec{b} \times \vec{c}) 
  &=
    -\vec{a} \cdot (\vec{c} \times \vec{b}) .
\end{align}
Geometrically, it describes the volume of the parallepiped, defined by
these three vectors.


\subsubsection{Matrices}
\index{matrix|textbf}

\paragraph{Toeplitz matrix}

\index{matrix!Toeplitz|textbf} (diagonal-constant matrix) is a matrix
where the elements are constant along the diagonals:
\begin{equation*}
  \mat{M} =
  \begin{bmatrix}
    a_0 & a_{-1}   & a_{-2} & \cdots & \cdots & a_{-(n-1)}  \\
    a_1 & a_0      & a_{-1} & \ddots &        & \vdots \\
    a_2 & a_1      & \ddots & \ddots & \ddots & \vdots \\ 
    \vdots & \ddots & \ddots & \ddots & a_{-1} & a_{-2}\\
    \vdots &        & \ddots & a_1    & a_0    & a_{-1} \\
    a_{n-1} & \cdots & \cdots & a_2    & a_1    & a_0
  \end{bmatrix}
\end{equation*}

\subsubsection{Determinant}
\label{sec:determinant}

Let
\begin{equation}
  \mat{M} =
  \begin{bmatrix}
    \mat{A}      & \mat{B} \\
    \mat{C}      & \mat{D}
  \end{bmatrix}
\end{equation}
Now
\begin{equation}
  \label{eq:block-determinant}
  | \mat{M} | =
  |\mat{A} - \mat{B} \mat{D}^{-1} \mat{C} | \cdot |\mat{D}|
\end{equation}


\subsubsection{Inverse Matrix}
\label{sec:inverse-matrix}


\paragraph{Inverse matrix}
\index{matrix!inverse|textbf}


\begin{align}
 \mat{A}^{-1} = 
 \begin{pmatrix}
 a_{11} & a_{12}\\
 a_{21} & a_{22}
 \end{pmatrix}
        & =
 \frac{1}{|A|}
 \begin{pmatrix}
  a_{22}        & -a_{12}\\
 -a_{21}        & a_{11}
 \end{pmatrix}.
\end{align}

Partitioned inverse 
\index{matrix!partitioned inverse|textbf}
formula
\begin{align}
\begin{pmatrix}
\mat{A}      & \mat{B} \\
\mat{C}      & \mat{D}
\end{pmatrix}^{-1} 
               &=
\begin{pmatrix}
\mat{E}^{-1}                      & -\mat{E}^{-1}\mat{B}\mat{D}^{-1} \\
-\mat{D}^{-1}\mat{C}\mat{E}^{-1}  & \mat{F}^{-1}
\end{pmatrix}
\end{align}
where
\begin{align*}
  \mat{E} &= \mat{A} - \mat{B}\mat{D}^{-1}\mat{C} 
               \equiv \mat{M}/\mat{D}
               \quad\text{is the Schur complement of
               $\mat{M}$ wrt $\mat{D}$}
\\
\mat{E}^{-1} &= \mat{A}^{-1} + \mat{A}^{-1}\mat{B}\mat{F}^{-1} \mat{C}
               \mat{A}^{-1} 
\\
  \mat{F} &= \mat{D} - \mat{C} \mat{A}^{-1} \mat{B} 
            \equiv \mat{M}/\mat{A}
\\
  \mat{F}^{-1} &= \mat{D}^{-1} +
                 \mat{D}^{-1} \mat{C} \mat{E}^{-1} \mat{B}
                 \mat{D}^{-1} 
\end{align*}
Proof: \citet[pp 118-119]{murphy2012}
\begin{equation}
\begin{pmatrix}
\mat{A}      & \mat{0} \\
\mat{0}      & \mat{D}
\end{pmatrix}^{-1} 
=
\begin{pmatrix}
\mat{A}^{-1}                      & \mat{0} \\
\mat{0}  & \mat{D}^{-1}
\end{pmatrix}.
\end{equation}



\paragraph{Moore-Penrose pseudoinverse}
\index{Moore-Penrose pseudoinverse|textbf}

\begin{equation}
  \label{eq:moore-penrose-pseudoinverse}
  \mat{A}^{+} =
  \lim_{\alpha\downarrow 0}
  \left( \mat{A}^{T} \mat{A} + \alpha \mat{I} \right)^{-1} \mat{A}^{T}
\end{equation}

M-P pseudoinvese solves the equation
\begin{equation}
  \label{eq:moore-penrose-solution}
  \vec{y} = \mat{A} \vec{x}
  \quad\Rightarrow\quad
  \vec{x} = \mat{A}^{+} \vec{y}
\end{equation}
in general case.  If $\mat{A}$ has more columns than rows (infinite
number of solutions), it picks the one that has minimal Euclidean norm
of $\vec{x}$.  If $\mat{A}$ has more rows than columns (no solutions),
it picks the one with minimal Euclidean norm of $\vec{y} - \mat{A}
\vec{x}$. 



\subsubsection{Matrix norm}
\label{sec:matrix-norm}

\paragraph{Frobenius norm}
\index{Frobenius norm|textbf}

\emph{Frobenius norm} is an analogue of $L_{2}$ norm for matrices:
\begin{equation}
  \label{eq:frobenius-norm-trace}
  || \mat{A} ||_{F} =
  \sqrt{\sum_{i,j} a_{ij}^{2}}
  =
  \sqrt{\Tr ( \mat{A}\cdot \mat{A}^{T})}
\end{equation}


\subsection{Võrratused}


\selectlanguage{english}
\subsubsection{Hölder's inequality}
Let $X$ and $Y$ be random variables.
\begin{equation}
  \E |XY| 
  \le 
  \left\{ \E \left[ |X|^\frac{1}{\alpha} \right] \right\}^\alpha 
  \left\{ \E \left[ |X|^\frac{1}{1 - \alpha} \right] \right\}^{1 -\alpha}
\end{equation}

\subsubsection{Jensen's Inequality}
\index{Jensens inequality@Jensen's inequality|textbf}
Let $f(\cdot)$ be a concave function:

\begin{align}
\sum_{i} \lambda_{i} f(\vec{x}_{i}) 
& \le f\left(\sum_{i} \lambda_{i} \vec{x}_{i}\right),
\qquad \sum \lambda_{i} = 1
\\
\E f(x) & < f(\E x).
\end{align}
Proof: definition of concavity, induction.


\subsubsection{Triangle Inequality}
\begin{equation}
  |x + y| \le |x| + |y|
\end{equation}

\selectlanguage{estonian}
\subsubsection{Cauchy-Schwartzi võrratus}
\begin{equation}
  |<x,y>| \le \|x\| \cdot \|y\|
\end{equation}
\selectlanguage{english}
For sequences
\begin{equation}
\left( \sum a_i b_i \right)^2 \le
  \left( \sum a_i \right)^2 \left( \sum b_i \right)^2.
\end{equation}
For functions:
\begin{equation*}
  <x,y> = \int x \cdot y \, \dif x
  \qquad
  \| x \| = \sqrt{<x,x>}
\end{equation*}
\selectlanguage{estonian}
Vektorkujul:
\begin{equation}
(\vec a \cdot \vec b)^2 \le \|\vec a\|^2 \|\vec b\|^2
\end{equation}
ehk siis ka
\begin{equation}
\sum_i \vec z_i \vec z_i' \ge
  \frac{\sum_i a_i \vec z_i \sum_i a_i \vec z_i'}{\sum_i a_i^2}
\end{equation}


\selectlanguage{english}
\subsubsection{Inequalities, containing exponent}
Proof in most cases by analysing the corresponding function.
\begin{align}
  \me^a &\ge a\\
  %
  1 - \me^{-a} &< a\\
  %
  (1 - \me^{-a}) \me^{-a} &< a\\
  (1 + a) \me^a & \ge (1 + 2a)\\
  %
  (1 + a) \me^{-a} &< 1
  \qquad \text{if} \quad a > 0\\
  %
  (a - 1) \me^a &> -1
  \qquad \text{if} \quad a > 0\\
  %
  (1 + a^2) \me^{-a} &< 1
  \qquad \text{if} \quad a > 0\\
  %% e^a - a^b
  \me^{-a} - \me^{-b} &< -a + b 
  \qquad \text{if} \quad 0 < a < b\\
  %
  \me^a - \me^b 
  &= 
  (a-b) + \frac{1}{2}(a^2 - b^2) + \frac{1}{6}(a^3 - b^3) + \dots\\*
  &\gtrless (a - b)
  \qquad \text{if} \quad a \gtrless b\\
  % 
  a \me^{-b} - b\me^{-a}
  & \gtrless
  a - b
  \qquad \text{if} \quad a \gtrless b\\
  %
  a^2 \me^{b} - b^2 \me^{a}
  & \gtrless
  a^2 - b^2 +
  ab(a - b) +
  \frac{1}{6}a^2 b^2 (b - a) +
  \frac{1}{24}a^2 b^2 (b^2 - a^2) + \dots
  \notag\\*
  & \text{if} \quad b \gtrless a
\end{align}



\subsection{Differential Equations}
\label{differential-equations}

\subsubsection{Linear Equations with Constant Coefficients}
\label{sec:linear-constant}

\paragraph{Homogeneous linear differential equations with constant
  coefficients}
are of form
\begin{equation}
  \label{eq:de-lin-homog-const}
  a_{n} y^{(n)} + a_{n-1} y^{(n-1)} + \dots + a_{1} y = 0
\end{equation}
It's  particular solution is in the form
\begin{equation}
  y = \me^{c x}
\end{equation}
where $c$ is a root of the characteristic polynomial
\begin{equation}
  \label{eq:de-lin-homog-characteristic}
  a_{n} c^{n} + a_{n-1} c^{n-1} + \dots + a_{1} c = 0.
\end{equation}
The general solution is
\begin{equation}
  \label{eq:de-lin-homog-general}
  y(x) = u_{1} \me^{c_{1} x} + u_{2} \me^{c_{2} x} + \dots + u_{n} \me^{c_{n}x}
\end{equation}
where $c_{i}, i=1\dots n$, are the characteristics roots of
\eqref{eq:de-lin-homog-characteristic} and $u_{i}$ are constants to be
determined. 

Proof: insert $y(x) = \me^{cx}$ into \eqref{eq:de-lin-homog-const} and
take derivatives.


\paragraph{First order linear differential equation}
has a general form
\begin{equation}
  \label{eq:de-1st-lin-general}
  y'(x) + p(x) y(x) = q(x)
\end{equation}
It's solution is
\begin{equation}
  \label{eq:de-1st-lin-solution}
  y(x) =
  \frac{\int u(x) q(x) \dif x + C}{u(x)}
\end{equation}
where the \emph{integrating factor}, $u(x)$, is
\begin{equation}
  \label{eq:de-1st-lin-if}
  u(x) = \me^{\int p(x) \dif x}
\end{equation}


\newpage
\section{Probability and Statistics}
\label{sec:probatility_and_statistics}

\subsection{Combinatorics}
\label{sec:combinatorics}

\paragraph{Combinations}

Number of combinations of selecting $k$ items out of $n$ where order
does not matter:
\begin{equation}
  \begin{pmatrix}
    n \\ k \\
  \end{pmatrix}
  \equiv
  C_{k}^{n}
  =
  \frac{n!}{k! (n-k)!}
\end{equation}


\subsection{Basic statistics}
\label{sec:basic-statistics}

\paragraph{Sample mean}
\label{sec:sample-mean}

Let $\vec{x}$ be a $N\times1$ vector.  Its mean
\index{sample mean|textbf}
\begin{equation}
  \label{eq:mean}
  \bar{x} = \frac{1}{N}\vec{1}^{\transpose}\,\vec{y}
\end{equation}
Its deviations from the mean 
\begin{equation}
  \label{eq:deviation-from-mean}
  \vec{x} - \bar{x} = 
  \mat{D}\,\vec{x} =
  \left(
    \mat{I} - \frac{1}{N} \vec{1}\,\vec{1}^{\transpose}
  \right) 
  \vec{x}
\end{equation}
This is a conclusion from the linear regression residuals' formula
\eqref{eq:linear-regression-residuals}.  Its variance
\begin{equation}
  \label{eq:vector-variance}
  \var \vec{x} =
  \frac{1}{N}
  (\vec{x} - \bar{x})^{\transpose} \, (\vec{x} - \bar{x})
  =
  \frac{1}{N}
  \vec{x}^{\transpose}\,\mat{D}^{\transpose} \,
  \mat{D}\,\vec{x} =
  \frac{1}{N}
  \vec{x}^{\transpose}\,\mat{D}^2 \,\vec{x}
\end{equation}
as $\mat{D}$ is symmetric.

\paragraph{Sample variance}
\label{sec:sample-variance}

Biased sample variance
\begin{equation}
  \label{eq:biased-sample-variance}
  s^{2} =
  \frac{1}{N} \sum(x_{i} - \bar x_{i})^{2}.
\end{equation}

It is distributed as $N\cdot s^{2} \sim \chi_{N-1}^{2}$.


\subsection{Random Variables}
\label{sec:random_variables}

\subsubsection{General Concepts}
\label{sec:RV_general_concepts}

\paragraph{Variance}

\begin{equation}
  \var X = \E (X - \E X)^{2}
\end{equation}

\paragraph{Other Moments}

properties:
\begin{align}
  \E (X - \E X + \alpha)^{2} &= \var X + \alpha^{2}
  \\
  \E (X - \E X + \alpha)^{3} &= \E(X - \E X)^{3} + 3\alpha\,\var X + \alpha^{3}
\end{align}
where $\alpha$ is a constant.

\paragraph{Stochastic dominance}
\index{stochastic dominance}
Let $X$ and $Y$ be random variables with the corresponding c.d.f-s
$F_{X}$ and $F_{Y}$.  $X$ 1st-order dominates $Y$ if $F_{X}(x) \le
F_{Y}(x) \quad \forall x$.  Intuitively this means $Y$ realizations
are larger than $X$ realizations.



\subsubsection{Information and Entropy}
\label{sec:information}

\paragraph{Entropy}
\index{entropy|textbf}
of random variable $X$ describes how much information we will
gain from an observation of $X$, and is defined as
\begin{equation}
  \label{eq:entropy-def}
  \entropy(X) = -\E \log f_{X}
\end{equation}
For discrete distribution 
\begin{equation}
  \label{eq:information}
  \entropy(X) = -\sum_k \Pr(X = k) \log \Pr(X = k).
\end{equation}
In case of discrete uniform with $K$ possible states, $\entropy(X) = \log K$.
For continuous distribution
\begin{equation}
  \label{eq:entropy-continuous}
  \entropy(X) = -\int f_{X}(x) \cdot \log f_{X}(x) \,\dif x.
\end{equation}
A draw from uniform distribution gives maximum possible information of
all distributions as the prior is the
least informative.

\paragraph{Cross entropy }
\index{cross entropy|textbf}
\begin{equation}
  \label{eq:cross-entropy}
  \entropy(p, q) = -\sum_{k} p_{k} \log q_{k}
\end{equation}

\subsubsection{Kullback-Leibler divergence}
\label{sec:kullback-leibler-divergence}
\index{Kullback-Leibler divergence|textbf}

A measure of dissimilarity between distributions $p$ and $q$:
\begin{equation}
  KL(p || q) 
  = 
  \E_p
  \left[
    \log \frac{p(X)}{q(X)}
  \right]
  =
  \sum_{k} p_{k} \log \displaystyle \frac{p_{k}}{q_{k}}
\end{equation}
where $\E_{p}$ means expectation over $X$ according to distribution
$p$.  The second equality is true if $X$ is discrete.

Note: it is not a metric distance as $KL(p || q) \not= KL(q || p)$.


\subsubsection{Mutual Information}
\label{sec:mutual-information}

Mutual Information
\index{mutual information|textbf}
\index{Shannon mutual information|textbf}
(MI) for two random variables $X$ and $Y$ is the
\hyperref[sec:kullback-leibler-divergence]{Kulback-Leibler divergence}
between $P_{XY}(x, y)$ and $P_{X}(x) P_{Y}(y)$:
\begin{equation}
  \label{eq:mutual-information}
  MI(X,Y) = KL(P_{XY}(x, y) || P_{X}(x) P_{Y}(y))
  =
  \sum_{x} \sum_{y} p(x, y) 
  \log \displaystyle\frac{P_{XY}(x, y)}{P_{X}(x) P_{Y}(y)}.
\end{equation}
Properties:
\begin{itemize}
\item If $X$, $Y$ are independent, $MI(X,Y) = 0$.  
\item $MI(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$ where $H(X|Y) =
  \sum_{x} p(x) H(Y|X= x)$.
\end{itemize}


\subsubsection{Sufficient statistics}
\label{sec:sufficient-statistics}

$\vec{s}(\vec{X})$ is a \emph{sufficient statistics} of $\vec{X}$ if 
\begin{equation}
  \label{eq:sufficient-statistics}
  P(\vec{\theta}|\vec{X}) = P(\vec{\theta}|\vec{s}(\vec{X}))
\end{equation}
where $\vec{\theta}$ is the parameter for distribution of $\vec{X}$.


\subsection{Stochastic Boundedness and Convergence in Probability}
\label{sec:stochastic_boundedness}

\paragraph{Stochastic Boundedness}

Random sequence 
\begin{math}
\left\{\frac{X_{n}}{a_{n}} \right\}  
\end{math}
 is stochastically bounded if:
$\forall \epsilon >0$, $\exists M > 0$, $N>0$:
\begin{equation}
  \label{eq:stochastic_boundedness}
  \Pr( | X_{n}/a_{n}| > M) < \epsilon
  \quad \forall n > N
  \qquad
  \text{or alternatively:}
  \qquad
  X_{n} = O_{p}(a_{n})
\end{equation}


\paragraph{Convergence in Probability}

Random sequence $\{X_{n}\}$ converges in probability to 0 as $n\to\infty$
if
\begin{equation}
  \label{eq:convergence_in_probability}
  \lim_{n\to\infty} \Pr( | X_{n}| \ge \epsilon) = 0
  \quad \forall \epsilon > 0
  \qquad
  \text{or alternatively:}
  \qquad
  X_{n} = o_{p}(1)
\end{equation}

\paragraph{Pointwise Convergence in Probability}

Random sequence $\{Q_{n}(\theta)\}$ converges in probability pointwise
to $Q(\theta)$ on $\Theta$ as $n\to\infty$
iff
\begin{equation}
  \label{eq:pointwise-convergence-in-probability}
  \forall \theta\in\Theta, \epsilon>0, \eta > 0
  \quad
  \exists N:
  \Pr( | Q_{n}(\theta) - Q(\theta)| \ge \epsilon) < \eta
  \quad \forall n > N
\end{equation}
or alternatively: $Q_{n}(\theta) - Q(\theta) = o_{p}(1)$. 


\paragraph{Uniform Convergence in Probability}

Random sequence $\{Q_{n}(\theta)\}$, $n = \{1,2,\dots\}$, $\theta\in\Theta$ converges to
$Q(\theta)$ in probability uniformly iff
\begin{equation}
  \label{eq:uniform_convergence_in_probability}
  \sup_{\theta\in\Theta}
  \left| Q_{n}(\theta) - Q(\theta) \right| = o_{p}(1)
\end{equation}

\paragraph{Stochastic Equicontinuity}

Random sequence $\{Q_{n}(\theta)\}$, $n = \{1,2,\dots\}$,
$\theta\in\Theta$ is stochastically equicontinuous if $\forall
\epsilon, \eta > 0$ there exists random $\Delta_{n}(\epsilon, \eta)$
and a constant $n_{0}(\epsilon,\eta)$ such that 
\begin{equation}
  \Pr(|\Delta_{n}(\epsilon,\eta)| > \epsilon) < \eta
  \quad \forall n > n_{0}(\epsilon,\eta),
\end{equation}
and for each $\theta$ there exists an open set $\mathscr{N}(\theta,
\epsilon, \eta)$ containing $\theta$ where
\begin{equation}
  \label{eq:stochastic_equicontinuity}
  \sup_{\tilde\theta \in \mathscr{N}(\theta,\epsilon,\eta)}
  \left|
    \hat Q_{n}(\tilde \theta)
    -
    \hat Q_{n}(\theta)
  \right|
  < \Delta_{n}(\epsilon,\eta)
  \quad
  \forall n > n_{0}(\epsilon,\eta)
\end{equation}

See \citet{newey1991Econometrica}


\subsection{Distributions: General Concepts}


\selectlanguage{estonian}

\label{sec:jaotused_mqisted}

\subsubsection{Sõltumatud juhuslikud muutujad}
$X$ ja $Y$ on sõltumatud $\Leftrightarrow f(x,y) = f_X(x) f_Y(y)
\Leftrightarrow F(x,y) = F_X(x) F_Y(y)$.

\selectlanguage{english}
\subsubsection{Expectations}
Let support of random variable $X$ be $[a, b]$.  Expectation of $X$
\begin{align}
  \E X
  &=
  \int_a^b x \, \dif F_X(x)\\
  &=
  a + \int_a^b \bar F_X(x) \,\dif x.
\end{align}

\paragraph{Law of iterated expectations}
\begin{equation}
  \E_X
  \left[ \E [Y|X] \right]
  =
  \E [Y]
\end{equation}


\subsubsection{Central and Non-Central Moments}

Definition: $n$-th non-central moment is $\mu_{n} = \E X^{n}$, and the
corresponding central moment is $m_{n} = \E ( X - \E X)^{n}$.

Relationships:
\begin{align}
  \mu_{1} &\equiv \mu
  \\
  \sigma^{2} \equiv m_{2} &= \mu_{2} - \mu^{2}
  \\
  m_{3} &= \mu_{3}- 3 \mu \mu_{2} + 2\mu^{3}
  \\
  m_{4} &= \mu_{4} - 4 \mu_{3} \mu + 6 \mu_{2} \mu^{2} -3 \mu^{4}
\end{align}


\subsubsection{Characteristic  Function}
\label{sec:characteristic_function}

Let $X$ be a random variable.  It's characteristic function:
\begin{equation}
  \label{eq:characteristic_function}
  \phi_{X}(t) = \E \me^{itX}
\end{equation}
\index{characteristic function|textbf}

Properties: for independent random variables $X_{1}$, $X_{2}$
\begin{equation}
  \phi_{X_{1} + X_{2}}(t) = \phi_{X_{1}}(t) \cdot \phi_{X_{2}}(t)
\end{equation}

\subsubsection{Moment Generating Function}

In 1-D case the moment-generating function (MGF) of RV $X$ is
\begin{equation}
M_X (s) = \E \me^{s X} = \int \me^{s x} f(x) \dif x.
\end{equation}
\index{moment generating function|textbf}
MGF can be used to compute moments:
\begin{equation}
M_X'(0) = \E X
\qquad
M_X''(0) = \E X^2 
\qquad 
M_X^{(n)}(0) = \E X^n.
\end{equation}

Sometimes one needs MFG of $\log X$:
\begin{equation}
M_{\log X}(s) = \E \me^{s \log X} = \E X^s.
\end{equation}

In $N$-dimensional case MGF is
\begin{multline}
M( s_1,s_2,\ldots,s_N) = \E e^{\sum^N s_i x_i} = \\
= \idotsint e^{s_1 x_1}
  e^{s_2 x_2} \ldots e^{s_N x_N} f(x_1,x_2,\ldots, x_N) dx_1 dx_2 \ldots
  dx_N. 
\end{multline}

\selectlanguage{estonian}
\subsubsection{Kumulandifunktsoon (\emph{cumulant-generating function})}

KGF avaldub MGF-i kaudu:
\begin{equation}
K_x(s) = \log M(s) 
  \qquad \mbox{või} \qquad
K(s_1,s_2,\ldots,s_N) = \log M( s_1,s_2,\ldots,s_N).
\end{equation}
KGF-i omadus (ühemõõtmelisel juhul):
\begin{eqnarray}
K_x'(0) = \E x \\
K_x''(0) = \var x\\
K_x'''(0) = \E(x-\E x)^3
\end{eqnarray}
ja kahemõõtmelisel juhul:
\begin{equation}
\frac{\partial^2 K(0,0)}{\partial s_1 \partial s_2} =
  \cov( x_1, x_2).
\end{equation}

\selectlanguage{english}

\subsubsection{Probability Generating Function}
\label{sec:probability_generating_function}

For discrete, non-negative random variables
\begin{equation}
  G(z) = \E (z^X) = \sum_{x=0}^{\infty}p(x)z^x.
\end{equation}
Properties:
\begin{align}
  p(k) &= \operatorname{Pr}(X = k) = \frac{G^{(k)}(0)}{k!}
\end{align}

\subsubsection{
  Distribution Function of a Function of Random Variable
}

Consider RV-s $X$ and $Y$, where $X$ is a function of $Y$: $X=X(Y)$.
Assume $X(\cdot)$ is a monotonic function.
Now the distribution function
\begin{equation}
  F_X(x) = \Pr[X < x] = \Pr[X < x(y)] = F_x[x(y)] = F_y(y).
\end{equation}
The density
\begin{equation}
  f_y(y) = F_y'(y) = \frac{\dif}{\dif y} F_x[x(y)] =
  \frac{\dif}{\dif x} F_x(x) \frac{\dif x}{\dif y} = f_x[x(y)]
  \frac{\dif x}{\dif y}. 
\end{equation}

Special cases: Let $X = Y^{2}$.  Now
\begin{equation}
  \label{eq:cdf-x2}
  F_{X}(x) = F_{y}(y) - F_{y}(-y).
\end{equation}
If $f_{y}(y)$ is symmetric around 0 then density
\begin{equation}
  \label{eq:pdf-x2}
  d_{X}(x) =
  \frac{f_{Y}(y)}{y} =
  \frac{f_{Y}(X^{-1}(x))}{X^{-1}(x)}.
\end{equation}

\subsubsection{Pareto ratio}
\label{sec:80-20-rule}
\index{Pareto ratio|textbf}

This is a rule describing inequality: the upper $\tau$\% of cases posess
$(100 - \tau)$\% of resources.  More generally, define $x^{*}$ through equation
\begin{equation}
  \label{eq:80-20-rule-def}
  \frac{\bar F(x^{*}) \E X|X > x^{*}}{\E X} = F(x^{*}).
\end{equation}
Then the upper $\bar F(x^{*})$ of cases possess $F(x^{*})$ of the
resources, where resources are defined as $x$ values.


\subsubsection{Quintile Share Ratio (QSR)}
\label{sec:qsr}
\index{quintile share ratio|textbf}

QSR is the wealth of the top quitile divided by the wealth of the
bottom quintile
\begin{equation}
  \label{eq:qsr}
  \mathit{QSR} =
  \frac{
    \E X|X > q_{0.8} \cdot \Pr(X > q_{0.8})
  }{
    \E X|X < q_{0.2} \cdot \Pr(X < q_{0.8})
  }
  =
  \frac{
    \E X|X > q_{0.8}
  }{
    \E X|X < q_{0.2}
  }.
\end{equation}


\subsection{One-dimensional discrete distributions}

\subsubsection{Bernoulli}

Is the simplest binary distribution: event $E$ happens with probability
$\mu$ and does not happen with $1-\mu$.  The random variable
\begin{equation}
  X = \indic(E) =
  \begin{cases}
    1 & \text{if $E$},\\
    0 & \text{if $\bar E$.}
  \end{cases}
\end{equation}
has Bernoulli distribution.

Properties:
\begin{align}
  \label{eq:bernoulli-properties}
  \E X = \mu \\
  \var X = \mu (1 - \mu)
\end{align}

\hyperref[sec:exponential-family]{Exponential family} minimal
canonical parameters
\begin{align}
  \label{eq:bernoulli-exponential}
  \eta &= \log \left( \frac{\mu}{1 - \mu} \right) 
         \qquad
         \mu = \Lambda(\eta)
  \\
  A(\eta) &= \log(1 + \me^{\eta}) = \log\left(\frac{1}{1 - \mu} \right)
  \\
  h(x) &= 1
  \\
  T(x) &= x
\end{align}
where $\Lambda(\cdot)$ is the logistic function.


\subsubsection{Binomial}
\index{distributions!binomial|textbf}
\selectlanguage{estonian}
On $N$ ühesuguse sõltumatu Bernoulli jaotusega juhusliku muutuja summa
jaotus.  Olgu $X = \sum^N Y_i$ kus $Y_i$ on Bernoulli jaotusega
parameetriga $p$.  
\selectlanguage{english}

\paragraph{Properties}
\begin{align*}
  \Pr(X = n) &= \binom{N}{n}\; p^n (1-p)^{N-n} \quad n\in \{0, \ldots ,N\}
  \\
  \E X &= Np 
  \\
  \var X &= Np(1-p)
\end{align*}

\paragraph{Moments}
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-Central & Central\\
    \midrule
    1 & $Np$ & $0$
    \\
    2 & $Np(1 - p + Np)$ & $N p( 1 - p)$
    \\
    3 && $N p( 1 - p)( 1 - 2p)$
    \\[1.1ex]
    4 && $N p( 1 - p)( 1 - 3p + 3p^{2})$
    \\[1.1ex]
    5 && $N p( 1 - p)(1 - 2p)( 1 - 2p + 2p^{2})$
    \\[1.1ex]
    6 && $N p( 1 - p) (1 - 5p + 10p^2 - 10p^3 + 5p^4)$
    \\
    \bottomrule
  \end{tabular}
\end{center}
Proof: for a single Bernoulli trial write $(Y - p)^{n}$, note that $\E Y^{n} = p$
($i\not=0$).  If trials are independent, one can just sum the expectations.

\paragraph{Binomial mixture}

Let $X$ be a mixture of $\mathit{Binom(N, p_{1})}$ with probability
$\alpha$ and $\mathit{Binom(N, p_{2})}$ with probability $1-\alpha$.
Let $p = \alpha p_{1} + (1 - \alpha) p_{2}$
Properties:
\begin{align*}
  \E X &= N p
  \\[1.1ex]
  \var X &= N p (1 - p) + N (N-1) \alpha(1-\alpha) (p_{1} - p_{2})^{2}
\end{align*}
If $N > 1$, the variance is larger than for $\mathit{Binomial(N, p)}$,
$Np(1-p)$.  
If $N=1$, $\var X$ is equal to the variance of a Bernoulli RV with the
average probability $p$.  


\subsubsection{Discrete}
\index{distributions!discrete|textbf}
This is a distribution where RV may have a finite number of discrete values. 

\subsubsection{Generalized Poisson}
\label{sec:generalized-poisson-distribution}
\index{distributions!generalized Poisson|textbf}
It is a generalization of~\hyperref[sec:poisson-distribution]{Poisson distribution}, it
allows overdispersion.

\paragraph{Properties}

\begin{align}
  \label{eq:generalized-poisson-properties}
  \text{pdf}\quad
  f(n; \lambda, \eta) &=
                        \frac{\lambda (\lambda + \eta n)^{n-1} \,
                        \me^{-\lambda - \eta n}}{n!}
  \\
  \E n &= \frac{\lambda}{1 - \eta}
         \\
  \var n &= \frac{\lambda}{(1 - \eta)^{3}}
           \\
  \text{overdispersion}\quad
  D &= \frac{\E n}{\sqrt{\var n}} = \frac{1}{\sqrt{\lambda(1 - \eta)}}
\end{align}
\begin{itemize}
\item GP is a mixture of Poisson distribution \citep{joe+zhu2005BJ}.
\end{itemize}
\citet{tuenter2000SN} shows that $\sum_{n=0}^{\infty} = 1$.

\paragraph{Alternative parameterizations}
\subparagraph{Scale and shape}
Define scale (mean) $\mu = \displaystyle\frac{\lambda}{1 - \eta} > 0$ and shape $\alpha
= \displaystyle\frac{\eta}{\lambda}$.  $\alpha$ measures the deviation
from standard Poisson (where $\alpha=0$), measured in expected value.
(The reverse transformation $\lambda =
\displaystyle\frac{\mu}{1 + \alpha\mu}$ and $\eta = \displaystyle\frac{\alpha\mu}{1 +
  \alpha\mu}$.)  Now 
\begin{align}
  \label{eq:gpois-scale-shape-pmf}
  \text{pdf}\quad
  f(n; \mu, \alpha) &=
  \left( \frac{n}{1 + \alpha n} \right)^{n}
  \frac{(1 + \alpha n)^{n-1}}{n!}
  \exp\left(-n\frac{\mu(1+\alpha n)}{1+\alpha}\right)
  \\
  \E n &= \mu
         \\
  \var n &= \mu (1 + \alpha \mu)^{2}
  \\
  \text{overdispersion}\quad
  D &= \frac{1 + \alpha\mu}{\sqrt{\mu}}
\end{align}
\selectlanguage{english}

\subparagraph{Mean and variance}
Let $\mu$ be the expectation and $\sigma^{2}$ the variance.  The
reverse transformation $\lambda =
\displaystyle\frac{\mu^{3/2}}{\sigma}$ and $\eta = 1 - \displaystyle\frac{\mu^{1/2}}{\sigma}$.


\subsubsection{Geomeetriline jaotus ($Geo(p)$)}
Geomeetriline jaotus kirjeldab mingi hulga Bernoulli jaotusega
suuruste järjest esinemist.  Olgu sündmuse tõenäosus $p$.  Tõenäosus,
et järjest toimub $n$ sündmust ja seejärel sündmuste jada katkeb on:
\begin{equation}
f(n) = (1-p)p^n.
\end{equation}
Jaotuse omadused:
\begin{eqnarray}
\E n = \frac{1-p}{p}\\
\var n = \frac{1-p}{p^2}
\end{eqnarray}


\subsubsection{Multinoomjaotus}
Olgu üksikul katsel $M$ võimalikku tulemust $A_1 \ldots A_M$ vastavate
tõenäosustega $p_1 \ldots p_M$, kusjuures $\sum^M p_i = 1$.  Olgu
$N$-katselises seerias $N_i$  realiseerunud sündmuste $A_i$ arv.
Siis:
\begin{align}
  \E N_i &= Np_i \\
\var N_i &= N p_i ( 1 - p_i) \\
&\ldots \notag\\
\cov( N_i, N_j) &= -N p_i p_j
\end{align}
Tõestus: momentide arvutamisel võib multinoomjaotuse taandada
binoomjaotuseks, kovariatsiooni jaoks kirjuta definitsioon lahti,
arvesta et $E A_i A_j = 0$.

\selectlanguage{english}

\subsubsection{Negative Binomial}
\label{sec:negative-binomial-distribution}
\index{distributions!negative binomial|textbf}

Negative binomial is an overdispersed distribution counts.  It arises
as a number of failures in Bernoulli process for a given number of
successes.  Let $n$ be the number of failures $F$ till we get to $s$
successes $S$.  Let $p$ be the probability of success.
The probability to acheve $s$-th success after $n$
failures is $\Pr(S = s-1, F = n)\Pr(S) = \binom{s + n - 1}{s - 1}\,
p^{s-1}(1-p)^{n}\, p = \binom{s + n - 1}{s - 1}\, p^{s}(1-p)^{n}$.

Properties:
\begin{align}
  \label{eq:negative-binomial-properties}
  \text{pmf}\quad 
  f(n; s, p) &= \binom{s + n - 1}{s - 1}\; p^{s}(1-p)^{n}
               \\[1.2ex]
             &= \frac{\Gamma(s + n) \, p^{s} (1-p)^{n}}
               {\Gamma(s) \, n!}
               \quad\text{for continuous $s$}
  \\
  \E n & = \frac{s (1 - p)}{p}
  \\
  \var n &= \frac{s (1 - p)}{p^{2}}
           \\
  \text{mode}
  &= \left[ \frac{s (1 - p) - 1}{p} \right] + 1
\end{align}
\begin{enumerate}
\item Negative binomial is a \hyperref[sec:poisson-distribution]{Poisson} mixture where the mixing
  distribution $\lambda \sim \mathit{Gamma}(\theta, p/(1-p))$.  
\end{enumerate}


\subsubsection{Poisson}
\label{sec:poisson-distribution}
\index{distributions!Poisson|textbf}

Poisson distribution describes a sum of independent rare events, its
pdf describes the
probability to observe $n$ events with its parameter, $\lambda$,
describing the expected number of events.  This can be applied to
different time periods, if the expectations is $\mu$ events per time
unit, then for time period $t$, the expected number (Poisson
parameter) is $\lambda = \mu t$.

Properties:
\begin{align}
  \label{eq:poisson-properties}
  \text{pdf}\quad 
  f(n) &= \frac{\lambda^n\, \me^{-\lambda}}{n!}
  \\
  \E n & = \lambda
  \\
  \var n &= \lambda
\end{align}
\begin{itemize}
\item Poisson pdf is log-concave
\item Sum of Poisson variables is a Poisson variable:
  \begin{equation}
    \label{eq:sum-of-poisson-variables}
    \text{if}\quad
    X \sim \mathit{Pois}(\lambda_{1}),
    Y \sim \mathit{Pois}(\lambda_{2})
    \quad\text{then}\quad
    X + Y \sim \mathit{Pois}(\lambda_{1} + \lambda_{2})
  \end{equation}
\item Difference of two Poisson variables is
  \hyperref[sec:skellam-distribution]{Skellam distributed}:
  \index{distributions!Skellam}
  \begin{equation}
    \label{eq:difference-of-poisson-variables}
    \text{if}\quad
    X \sim \mathit{Pois}(\lambda_{1}),
    Y \sim \mathit{Pois}(\lambda_{2})
    \quad\text{then}\quad
    X - Y \sim \mathit{Skellam}(\lambda_{1}, \lambda_{2})
  \end{equation}
\end{itemize}

Characteristic function:
\begin{equation}
  \label{eq:poisson-characteristic-function}
  \phi(t) = \me^{\displaystyle\lambda(\me^{it} - 1)}
\end{equation}
  
\selectlanguage{estonian}
ML-hinnang: Kui vaatluse $i$ jooksul, mille kestus on $t_i$ 
toimub $n_i$ sündmust, siis kõigi vaatluste ML hinnang on:
\begin{equation}
  \hat\lambda = \frac{\sum n_i}{\sum t_i} 
  \qquad \mbox{ja} \qquad
  \var\hat\lambda = \frac{\sum n_i}{\left( \sum t_i \right)^2}
\end{equation}

Poissoni summa tuletis aja järgi: Olgu
\begin{equation}
  \vartheta(t) =
  \sum_{s=0}^S Q(s) \frac{(\lambda t)^2}{s!} \me^{-\lambda t} = 
  \E_s Q(s)
\end{equation}
siis
\begin{eqnarray}
  \pderiv{t} \vartheta(t) &=&
  \lambda \sum_{s=0}^{S - 1}
  \left[ Q(s + 1) - Q(s) \right] p_p(s) -
  \lambda Q(S) p_p(S) =\\
                                %
  &=&
  \lambda \sum_{s=1}^S Q(s) 
  \left[ p_p(s - 1) - p(s) \right]
  - \lambda Q(0) p_p(0)\\
                                %
  \pderiv{t} P_p(s) &=& -\lambda p_p(s)\\
                                %
  \pderiv{t} p_p(s) &=&
  \begin{cases}
    -\lambda p_p(s) & \text{kui $s$ = 0}\\
    -\lambda p_p(s) + \lambda p_p(s-1) & \text{kui $s$ > 0}\\
  \end{cases}
\end{eqnarray}

\selectlanguage{english}
Poisson distribution can be generalized to
\hyperref[sec:generalized-poisson-distribution]{generalized Poisson distribution}.


\subsubsection{Skellam Distribution $PD(\lambda,\delta)$}
\label{sec:skellam-distribution}
\index{distributions!Skellam|textbf}

Skellam distribution is the distribution of difference of two independent
\hyperref[sec:poisson-distribution]{Poisson}
RV-s.  Let $N = X - Y$ where $X \sim \mathit{Pois}(\lambda)$ and $Y
\sim \mathit{Pois}(\delta)$:
\begin{align}
  P(N=n) &= 
  \me^{-(\lambda + \delta)}
  \left(\frac{\lambda}{\delta} \right)^{\frac{n}{2}}
  I_{n}(2\sqrt{\lambda\delta})
  \\
  \E N &= \lambda - \delta
  \\
  \var N &= \lambda + \delta
\end{align}



\newpage
\subsection{1D Continuous Distributions}

\subsubsection{Beta distribution $\mathcal{B}(a,b)$}
\label{sec:beta-distribution}
\index{distributions!beta|textbf}

\begin{align}
\text{pdf}\qquad
f(x) &= \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1}
       \quad x\in[0,1]
  \\
  \E X &= \displaystyle\frac{a}{(a + b)}
  \\
  \var X &=
           \displaystyle\frac{ab}{(a + b)^{2} (a + b + 1)}           
  \\
\text{mode}\qquad
m &= \frac{a-1}{a + b - 2}
\end{align}
where $B(a,b)$ is is the \hyperref[sec:beta-function]{beta function} \eqref{eq:beta-function}.

Parameters $a$ and $b$ can be computed from the expected value $\mu$
and variance $\sigma^{2}$ as
\begin{equation}
  \label{eq:beta-parameters-from-exp-var}
  b = \frac{\mu(1 - \mu)^{2} - \sigma^{2}(1 - \mu)}{\sigma^{2}}
  \quad\text{and}\quad
  a = \frac{\mu b}{1 - \mu}
\end{equation}

Moments:
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-Central & Central\\
    \midrule
    1 & $\displaystyle\frac{a}{(a + b)}$ & $0$ \\[1.2ex]
    2 &  & $\displaystyle\frac{ab}{(a + b)^{2} (a + b + 1)}$
    \\[1.2ex]
    \bottomrule
  \end{tabular}
\end{center}

\paragraph{Properties}
$B(1,1)$ is uniform distribution.  Larger parameter values
concentrate the probability mass more toward the center, smaller ones
to the extremes.  If parameters are unequal, the mass tends more
toward the smaller value.

Multivariate generalization of beta distribution is \hyperref[sec:dirichlet-distribution]{Dirichlet
distribution}.
\index{distributions!Dirichlet}

Examples densities:

<<betaDistExamples>>=
par(mar=c(3,3,0,0)+0.1,
    mgp=c(2,1,0))
ab <- list(c(1,1), c(0.5, 0.5), c(3, 3), c(0.7, 1.5), c(1.7, 2.5))
pal <- RColorBrewer::brewer.pal(length(ab), "Set1")
names(ab) <- pal
x <- seq(0, 1, length.out=100)
y <- sapply(ab, function(alpha) dbeta(x, alpha[1], alpha[2]))
matplot(x, y,
        type="l", lty=1, lwd=2, ylab="", col=pal)
legend("topright",
       legend=sapply(
          ab,
          function(alpha) paste0("B(", alpha[1], ",", alpha[2], ")")
       ),
       lty=1, lwd=2, col=pal, bty="n")
@ 

\subsubsection{Cauchy distribution}
\label{sec:cauchy-distribution}

\index{distributions!Cauchy|textbf}
Cauchy distribution is a fat-tailed bell curve-like distribution.  It
is a special case of \hyperref[sec:t-distribution]{$t$-distribution} with degrees of freedom = 1.
\begin{align}
\text{pdf}\qquad
f(x) &= \frac{1}{\mpi \gamma
       \left[ 1 + \left(\frac{x - x_{0}}{\gamma}\right)^{2} \right]}
\\
\text{mode}\qquad
m &= x_{0}
  \\
  \E X & \quad\text{undefined}
\end{align}
where $x_{0}$ is location and $\gamma$ is scale.


\subsubsection{Chi-squared distribution $\chi_{k}^2$}
\label{sec:chi-squared-distribution}
\index{distributions!chi-squared|textbf}

Sum of $k$ squared independent standard
normals is $\chi_{k}^2$-distributed.
Properties: 
\begin{align}
\text{pdf}
\qquad &
f_X (x) = \frac{1}{2^\frac{k}{2}} \frac{1}{
         \Gamma\left( \frac{k}{2}\right)
         }
  x^{\frac{k}{2} - 1} \me^{-\frac{x}{2}}
\\
  \text{cdf}
  \qquad &
           F_{X}(x) = \frac{1}{2^{\frac{k}{2}}}
           \frac{1}{\Gamma\left(\frac{k}{2}\right)}
           \gamma\left(\frac{k}{2},\frac{x}{2}\right)
  \\
\text{MGF}
\qquad &
         M(s) = (1 - 2s)^{-\frac{k}{2}}
  \\
  & \E X = k
\end{align}
where $\gamma(s, x)$ is
\hyperref[sec:incomplete-gamma-function]{incomplete gamma
  function}.  
\index{incomplete gamma function}
See example pdf-s on Figure~\ref{fig:chisq-pdf}.

\begin{figure}[ht]
  \centering  
<<>>=
ks <- c(1,2,4)
pal <- RColorBrewer::brewer.pal(length(ks), "Set1")
p <- ggplot(data.frame(x=c(0,5))) +
   labs(y = "Density", 
        color=expression(ks)) +
   geom_function(aes(x, 
                     col=as.character(ks[1])),
                 fun=dchisq, args=list(df = ks[1])) +
   geom_function(aes(x, 
                     col=as.character(ks[2])),
                 fun=dchisq, args=list(df = ks[2])) +
   geom_function(aes(x, 
                     col=as.character(ks[3])),
                 fun=dchisq, args=list(df = ks[3]))
p1 <- p + guides(col="none")
p2 <- p + scale_y_log10() +
   theme(axis.title.y=element_blank())
grid.arrange(p1, p2, nrow=1, 
             layout_matrix=matrix(c(1,1,1,1,2,2,2,2,2), nrow=1))
@   
\caption{
  Example $\chi^{2}$ pdf-s.  Linear (left) and log scale (right).
}
\label{fig:chisq-pdf}
\end{figure}

Other properties:

\begin{itemize}
\item if $X \sim N(0, \sigma^{2})$ then $\displaystyle\frac{X^{2}}{\sigma^{2}} \sim
  \chi_{1}^{2}$. 
\item if $X \sim \chi_{1}^{2}$ then
  \begin{equation}
    \label{eq:chisq1-conditional-probability}
    \Pr(X < \alpha\theta | X < \theta)
    =
    \frac{\gamma(\frac{1}{2}, \frac{1}{2}\alpha\theta)}
    {\gamma(\frac{1}{2}, \frac{1}{2}\theta)}
  \end{equation}
  where $0 < \alpha \le 1$.
  This converges to $\sqrt{\alpha}$ if $\theta \to 0$.  Proof: use
  \hyperref[sec:lhopitals-rule]{l'Hôpital's rule} and
  \hyperref[sec:leibnitz-rule]{Leibnitz rule}.
\end{itemize}


\selectlanguage{estonian}
\subsubsection{Eksponentjaotus $\mathcal{E}(\theta)$}
Eksponentjaotus kirjeldab konstantse kiirusega hääbuvaid protsesse.

\selectlanguage{english}
\begin{align}
  \text{pdf}\qquad
  f(t) &= \theta e^{-\theta t}, \qquad t \ge 0, \theta > 0
  \\
  \text{cdf}\quad
  F(t) &= 1 - e^{-\theta t}
  \\
  \text{median}\quad
  \mathit{med}(X) &= \frac{\log 2}{\theta}
  \\
  \text{MGF}\quad
  M_T (s) &= \frac{1}{1-\frac{s}{\theta}}, \quad s<\theta.
\end{align}

Moments:
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-Central & Central\\
    \midrule
    1 & $\displaystyle\frac{1}{\theta}$ & $0$ \\[1.4ex]
    2 & $\displaystyle\frac{2}{\theta^{2}}$ & $\displaystyle\frac{1}{\theta^{2}}$
    \\[1.4ex]
    3 & $\displaystyle \frac{6}{\theta^{3}}$  & $\displaystyle\frac{2}{\theta^{3}}$\\[1.3ex]
    4 & $\displaystyle \frac{24}{\theta^{4}}$ & $\displaystyle\frac{9}{\theta^{4}}$ \\
    \bottomrule
  \end{tabular}
\end{center}

\selectlanguage{estonian}
$\log T$ on esimest liiki ekstreemväärtuste jaotusega. $\log T$-ga
seotud suurused on:
\begin{eqnarray}
M_{\log T} (s) &=& \frac{\Gamma(s+1)}{\theta^s}\\
K_{\log T} (s) &=& \log \Gamma(s+1) - s \log \theta\\
\E \log T &=& \psi(1) - \log \theta\\
\var \log T &=& \psi'(1),
\end{eqnarray}
kus $\psi$ on digamma funktsioon.

Kui $z_1 \sim \mathcal{E}(\theta_1)$ ja $z_2 \sim
\mathcal{E}(\theta_2)$ siis
\begin{equation}
  \log z_1 - \log z_2 \sim \frac{\theta_1}{\theta_1 + \theta_2
  \me^{-x}} \sim \Lambda(x), \qquad \mbox{kui} \quad \theta_1 = \theta_2
\end{equation}
Tõestus: arvesta et $\Pr(z_1/z_2 < \alpha) = \Pr(z_1 < \alpha z_2)$ ja
integreeri. 


\subsubsection{F-jaotus $F(n_1, n_2)$}
F-jaotus tekib kahe $\chi^2$ jaotusega suuruse jagamisel.  Kui $w_1
\sim \chi_{n_1}^2$ ja $w_2 \sim \chi_{n_2}^2$ siis
\begin{equation}
\frac{\frac{w_1}{n_1}} {\frac{w_2}{n_2}} \sim F( n_1, n_2).
\end{equation}
Tihedusfunktsioon:
\begin{equation}
f(x) = \frac{
  \left(\frac{n_1}{n_2}\right)^\frac{n_1}{2} x^{\frac{n_1}{2}-1}}
  {B\left( \frac{n_1}{2}, \frac{n_2}{2} \right)
    \left( 1 + \frac{n_1}{n_2}x\right)^\frac{n_1+n_2}{2} }
\end{equation}
\selectlanguage{english}


\subsubsection{Gamma distribution $\mathcal{G(\alpha,\beta)}$}
\label{sec:gamma-distribution}
\index{distributions!gamma|textbf}

Is sometimes used to describe the unobserved heterogeneity in duration
models.  
\begin{equation}
\text{pdf}\quad 
f(x) = \frac{1}{\beta^\alpha} \frac{1}{\Gamma( \alpha)} x^{\alpha-1}
  e^{-\frac{x}{\beta}} \qquad x > 0,
\end{equation}
where $\alpha >0$ is shape, $\beta>0$ is scale, and $\Gamma(\cdot)$ is
\hyperref[sec:gamma-function]{gamma function}.  Alternatively, one
uses parameterization $(\alpha, \rho = \frac{1}{\beta})$ where $\rho$
is rate.

\paragraph{Properties}

\begin{eqnarray}
\E X &=& \beta \alpha \\
\E X^2 &=& \beta^2 \alpha( \alpha + 1) \\
\var X &=& \beta^2 \alpha\\
M_X (s) &=& \frac{1}{(\beta s - 1)^\alpha}\\
K_X (s) &=& -\alpha \log(\beta s - 1)\\
\end{eqnarray}

Properties related to $\log x$
\begin{eqnarray}
\E\log x &=& \log\beta + \psi(\alpha)\\
\var\log x &=& \psi'(\alpha)\\
M_{\log x} (s) &=& \frac{\Gamma(s + \alpha)}{\Gamma(\alpha)}\beta^s\\
K_{\log x} (s) &=& s\log \beta + \log \Gamma(s + \alpha) -
  \log\Gamma(\alpha)\\
\end{eqnarray}

\paragraph{Special cases}

\emph{Exponential distribution}: $\text{Gamma}(1, 1/\beta)$ is the same as $\text{Exp}(\beta)$.

\selectlanguage{estonian}
\emph{normaalne gammajaotus}, mille keskväärtus on 1.  Sel
juhul 
\begin{equation}
\alpha = \frac{1}{\beta} \equiv \eta
\end{equation}
ja jaotusfunktsioon
\begin{equation}
f_x(x) = \eta^\eta \frac{1}{\Gamma(\eta)} x^{\eta - 1} \me^{-\eta x}.
\end{equation}
Sel juhul:
\begin{eqnarray}
  \E x &=& 1\\
  \E x^2 &=& 1 + \frac{1}{\eta}\\
  \var x &=& \frac{1}{\eta}
\end{eqnarray}

Teine oluline erijuht on $\chi^2$-jaotus.  Kui $\alpha=\frac{k}{2}$ ja
$\beta=2$, siis $X$ jaotusfunktsioon on
\begin{equation}
f_x (x) = \frac{1}{2^\frac{k}{2}} \frac{1}{\Gamma(\frac{k}{2})}
  y^{\frac{k}{2} - 1} \me^{-\frac{y}{2}}.
\end{equation}

\selectlanguage{english}
This is also known as $\chi^2(k)$ distribution.

Gamma distribution 
generalized to matrices is \hyperref[sec:wishart-distribution]{Wishart
distribution}.

Density examples:

<<gammaDistExamples>>=
par(mar=c(3,3,0,0)+0.1,
    mgp=c(2,1,0))
ab <- list(c(0,0), c(0.5, 0.5), c(1, 1), c(0.5,1), c(1, 0.5))
pal <- RColorBrewer::brewer.pal(length(ab), "Set1")
names(ab) <- pal
x <- seq(0, 1, length.out=100)
y <- sapply(ab, function(alpha) dgamma(x, alpha[1], alpha[2]))
matplot(x, y,
        type="l", lty=1, ylab="", col=pal)
legend("topright",
       legend=sapply(ab, function(alpha) paste0("Ga(", alpha[1], ",", alpha[2], ")")),
       lty=1, col=pal, bty="n")
@ 


\subsubsection{Generalized Extreme Value Distribution }
\label{sec:generalized-extreme-value-distribution}
\index{distributions!generalized extreme value|textbf}

cdf:
\begin{equation}
  \label{eq:generalized-extreme-value-cdf}
  F(x) = \me^{-t(x)}
\end{equation}
where
\begin{equation}
  t(x) = \left( 1 + x \xi \right)^{-1/\xi}
\end{equation}
If $\xi = 0$, it is equivalent to
\hyperref[sec:type-1-extreme-value-distribution]{Type-1 extreme value distribution (Section~\ref{sec:type-1-extreme-value-distribution})},
otherwise to 
\hyperref[sec:type-2-extreme-value-distribution]{Type-2 extreme value distribution (Section~\ref{sec:type-2-extreme-value-distribution})}.


\subsubsection{Inverse gamma distribution}
\label{sec:inverse-gamma-distribution}

If $X \sim Ga(a,b)$ is
\hyperref[sec:gamma-distribution]{gamma distributed}
then it's inverse $1/X \sim IG(a,b)$ is inverse-gamma distributed.
\index{distributions!inverse gamma|textbf}
It
has two parameters, \emph{shape} $\alpha > 0$ and \emph{rate} $r > 0$.
Properties:
\begin{align}
  \label{eq:inverse-gamma}
  \text{density}\quad
  & f(x) = \frac{r^{\alpha}}{\Gamma(\alpha)} 
  x^{-(\alpha +1)}
    \me^{-\frac{r}{x}}
  \\
  & \E X = \frac{r}{\alpha - 1}
  \quad \alpha > 1
  \\
  \text{mode}\quad
  & \frac{r}{\alpha + 1}
  \\
  & \var X =
    \frac{r^{2}}{
    (\alpha-1)(\alpha-2)}
    \quad \alpha > 2
\end{align}

Generalization of inverse gamma to matrices is
\hyperref[sec:inverse-wish-distr]{inverse Wishart distribution}.
\index{distributions!inverse Wishart}
(Note: here these are parameterized differently).

Density examples for different $\alpha$, $r=1$:

<<invgamma-dist-examples>>=
par(mar=c(3,3,0,0)+0.1,
    mgp=c(2,1,0))
shapes <- list(c(1,1), c(2,1), c(4, 1), c(10, 3))
pal <- RColorBrewer::brewer.pal(length(ab), "Set1")
names(ab) <- pal
x <- seq(0, 1, length.out=100)
y <- sapply(seq(along=shapes),
            function(i) {
   shape <- shapes[[i]][1]
   rate <- shapes[[i]][2]
   invgamma::dinvgamma(x, shape, rate)
})
matplot(x, y,
        type="l", lty=1, ylab="", col=pal)
legend("topright",
       legend=sapply(seq(along=shapes),
                     function(i) {
          shape <- shapes[[i]][1]
          rate <- shapes[[i]][2]
          sprintf("IG(%4.2f, %3.1f)", shape, rate)
       }),
       lty=1, col=pal, bty="n")
@

Note that two last distributions, $\mathit{IG(4,1)}$ and
$\mathit{IG(10,3)}$ have the same expected value, 3.  Hence larger
$\alpha$ and $r$ make the mass more concentrated, given the expected
value is hold constant.

Inverse gamma is a conjugate distribution for variance of zero-mean
\hyperref[sec:normal-distribution]{normals}.
\index{distributions!normal}
Let the prior for $\Sigma^{2}$ be
\begin{equation}
  \label{eq:inv-gamma-prior}
  \Sigma^{2} \sim \mathit{IG}(\alpha, r).
\end{equation}
After collecting vector $\vec{x}$,
$n$ observations from $N(0, \sigma^{2})$, the
posterior is
\begin{equation}
  \label{eq:inv-gamma-posterior}
  \Sigma^{2}|\vec{x} \sim
  \mathit{IG}(\beta, s)
\end{equation}
where $\beta = \alpha + n/2$ and $s = r + 1/2\cdot
\vec{x}^{\transpose}\;\vec{x}$.  The joint distribution of $\vec{X}$
and $\Sigma^{2}$ is
\begin{equation}
  \label{eq:inv-gamma-joint}
  P(\vec{x}, \sigma^{2}) =
  \frac{1}{\big(\sqrt{2\pi}\big)^{n}}
  \frac{r^{\alpha}}{\Gamma(\alpha)}
  \sigma^{-2(\alpha + 1 + n/2)}
  \me^{\displaystyle
    -\frac{1}{2}
    \frac{\vec{x}^{\transpose}\vec{x} + 2r}{\sigma^{2}}
  }
\end{equation}


\subsubsection{Laplace distribution}
\label{sec:laplace-distribution}

\index{distributions!Laplace|textbf}
Also double-sided exponential distribution.  Density:
\begin{equation}
f_X (x; \mu, b) = \frac{1}{2 b}
  \me^{\displaystyle-\frac{|x - \mu|}{b}}.
\end{equation}
Properties:
\begin{equation}
\E X = \mu \qquad \var X = 2b^{2}
\end{equation}


\subsubsection[Log-normal]{Log-normal $LN(\mu,\sigma^2)$}
\index{distributions!log-normal|textbf}
Distribution of RV $X$ if $\log X \sim N()$.
\begin{align}
\text{cdf}\qquad &
                   F_X (x) = \Phi \left(
                   \frac{\log x - \mu}{\sigma}
                   \right)
  \\
\text{pdf}\qquad &
                   f_x (x) = \frac{1}{\sqrt{2\mpi}\sigma x}
  \me^{-\frac{1}{2}\frac{(\log x - \mu)^{2}}{\sigma^2}}
  \\
  \text{quantiles}\qquad&
                          \tau_{q} = \me^{\displaystyle \Phi^{-1}(q)\, \sigma + \mu}
\end{align}
where $\Phi(\cdot)$ is the normal \cdf.
  Example pdf-s:
  \begin{center}
<<>>=
mus <- c(0, 0, 0, 0)
sigmas <- c(0.2, 0.5, 1, 1.5)
pal <- RColorBrewer::brewer.pal(length(sigmas), "Set1")
p <- ggplot(data.frame(x=c(0,3))) +
   labs(y = "Density", 
        color=expression(sigma),
        linetype=expression(mu)) +
   geom_function(aes(x, 
                     col=as.character(sigmas[1]),
                     linetype=factor(mus[1])),
                 fun=dlnorm, args=list(meanlog = mus[1], sdlog=sigmas[1])) +
   geom_function(aes(x, 
                     col=as.character(sigmas[2]),
                     linetype=factor(mus[1])),
                 fun=dlnorm, args=list(meanlog = mus[2], sdlog=sigmas[2])) +
   geom_function(aes(x, 
                     col=as.character(sigmas[3]),
                     linetype=factor(mus[1])),
                 fun=dlnorm, args=list(meanlog = mus[3], sdlog=sigmas[3])) +
   geom_function(aes(x, 
                     col=as.character(sigmas[4]),
                     linetype=factor(mus[1])),
                 fun=dlnorm, args=list(meanlog = mus[4], sdlog=sigmas[4]))
p
@   
  \end{center}
  Changing $\mu$ changes the horizontal scale but not the shape.

  Properties:
\begin{align}
\E X &= \me^{\mu + \frac{1}{2}\sigma^2}\\
\var X &= \me^{2\mu + \sigma^2} (\me^{\sigma^2} - 1)\\
  \text{median } X &= \me^{\mu}
\end{align}
Conditional expectations:
\begin{align}
\E X | X > a & =
  \frac{1 - \Phi \left(
      \frac{\log a - \mu - \sigma^2}{\sigma}
      \right)}
    {1 - \Phi \left(
      \frac{\log a - \mu}{\sigma}
      \right)}
    \me^{\mu + \frac{1}{2}\sigma^2} 
\end{align}

\hyperref[sec:80-20-rule]{80-20 rule}: 
  \index{80-20 rule}
  The critical $x$ threshold $x_{*}$
  is given by
  \begin{equation}
    \label{eq:lognormal-80-20-threshold}
    1 - \Phi \left(
        \frac{\log x_{*} - \mu - \sigma^2}{\sigma}
      \right) 
      =
      \Phi \left(
        \frac{\log x_{*} - \mu}{\sigma}
      \right)
  \end{equation}
  Example solutions are:
<<>>=
criticalEq <- function(x, mu, sigma)
   pnorm((log(x) - mu)/sigma) - (1 - pnorm((log(x) - mu - sigma^2)/sigma))
xCr <- function(mu, sigma)
   uniroot(criticalEq, c(0, 100000), mu=mu, sigma=sigma)$root
upper <- function(mu, sigma)
   plnorm(xCr(mu, sigma), mu, sigma, lower.tail=FALSE)
lower <- function(mu, sigma)
   plnorm(xCr(mu, sigma), mu, sigma, lower.tail=TRUE)
sigmas <- c(0.1, 1, 2, 3.29, 4)
@   
  \begin{center}
    \begin{tabular}{llll}
      \toprule
      $\mu$ & $\sigma$ & $x_{cr}$ & upper/lower\\
      \midrule
<<>>=
mu <- 0; sigma <- sigmas[1]
@       
      \Sexpr{mu} & \Sexpr{sigma} & \Sexpr{round(xCr(mu,sigma), 3)} & \Sexpr{round(upper(mu,sigma),4)}/\Sexpr{round(lower(mu,sigma),4)}\\
<<>>=
mu <- 0; sigma <- sigmas[2]
@       
      \Sexpr{mu} & \Sexpr{sigma} & \Sexpr{round(xCr(mu,sigma), 3)} & \Sexpr{round(upper(mu,sigma),4)}/\Sexpr{round(lower(mu,sigma),4)}\\
<<>>=
mu <- 0; sigma <- sigmas[3]
@       
      \Sexpr{mu} & \Sexpr{sigma} & \Sexpr{round(xCr(mu,sigma), 3)} & \Sexpr{round(upper(mu,sigma),4)}/\Sexpr{round(lower(mu,sigma),4)}\\
<<>>=
mu <- 0; sigma <- sigmas[4]
@       
      \Sexpr{mu} & \Sexpr{sigma} & \Sexpr{round(xCr(mu,sigma), 3)} & \Sexpr{round(upper(mu,sigma),4)}/\Sexpr{round(lower(mu,sigma),4)}\\
<<>>=
mu <- 0; sigma <- sigmas[5]
@       
      \Sexpr{mu} & \Sexpr{sigma} & \Sexpr{round(xCr(mu,sigma), 3)} & \Sexpr{round(upper(mu,sigma),4)}/\Sexpr{round(lower(mu,sigma),4)}\\
      \bottomrule
    \end{tabular}
  \end{center}
Changing $\mu$ will change the scale and $x_{cr}$ but not inequality.
  

\selectlanguage{estonian}
\subsubsection{Log-ühtlane jaotus}
Kasutatakse palgajaotuse kirjeldamiseks.  
\selectlanguage{english}
\begin{equation}
\text{pdf}\qquad f(x) = \frac{1}{x}\cdot\frac{1}{\log \beta - \log \alpha} 
\quad\text{where}\quad
  0 \le \alpha \le x \le \beta < \infty.
\end{equation}

\subsubsection{Logistic Distribution}
\index{distributions!logistic|textbf}
\begin{align}
  \text{cdf}\qquad & \Lambda(x) = \frac{e^x}{1+e^x} = \frac{1}{1 + \me^{-x}}
  \\
  \text{pdf}\qquad & f(x) = \frac{\me^x}{(1 + \me^x)^2} = 
                     \frac{\me^{-x}} {(1 + \me^{-x})^2} =
                     \notag\\
                   & = \Lambda(-x) \Lambda(x) =
                     [1 - \Lambda(x)]\Lambda(x)
  \\
  & f'(x) = \me^{-x} \frac{\me^{-x} - 1}{(\me^{-x} + 1)^3}
  \\
  \text{MGF}\qquad &
  M(s) = \int \frac{\me^{x(s+1)}}{(1 + \me^x)^2} \dif x
\end{align}
Logistic distribution is symmetric around 0, i.e. $\Lambda(x) = 1 -
\Lambda(-x)$ and $f(x) = f(-x)$.

\subsubsection{Lomax Distribution}
\label{sec:lomax-distribution}

\index{distributions!Lomax|textbf}
(also Pareto II distribution)
\index{distributions!Pareto II|textbf}

Describes the upper part of many highly unequal distributions where
the domain starts from 0.  It is a shift of
\hyperref[sec:pareto-distribution]{Pareto distribution}.  Let $x_{0}$
be scale.  Its cdf $F_{l}(x)$ is related to the Pareto cdf $F_{P}(x)$
as $F_{L}(x) = F_{P}(x + x_{0})$ and its expectation $\E_{L}X =
\E_{P}X - x_{0}$.
Properties:
\begin{align}
  F(x) &= 
         1 - \left(1 + \frac{x}{x_{0}} \right)^{-\alpha}
  \\
f_{X}(x) &=
\frac{\alpha}{x_{0}} 
           \left(1 + \frac{x}{x_{0}}\right)^{-\alpha-1}
\end{align}
Moments:
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-Central & Central\\
    \midrule
    1 & $\displaystyle\frac{1}{\alpha - 1} x_{0}, \quad\alpha > 1$ & $0$ \\[1.4ex]
    2 & $\displaystyle\frac{2 x_{0}^{2}}{(\alpha - 1)(\alpha - 2)}$ &
                                                                      $\displaystyle\frac{\alpha x_{0}^{2}}{(\alpha-1)^{2}(\alpha-2)},
                                                                      \quad \alpha>2$\\
    \bottomrule
  \end{tabular}
\end{center}
Properties:
\begin{itemize}
\item power law: $\log f_{X}(x)$ is linear on log-log scale
\item it is \emph{scale-free}: there is no features in the right tail,
  wherever you look, you have most observations that are smaller, but
  you also have observations that are way larger.
\end{itemize}

Shifted Lomax, where the $x$ values start from $x_{0}$, is
\hyperref[sec:pareto-distribution]{Pareto Distribution}
\index{distributions!Pareto}.


\subsubsection{Normal Distribution $N(\mu,\sigma^2)$}
\label{sec:normal-distribution}

Sum of many independent random disturbations tends to be normally
distributed (Central Limit Theorem.)
\begin{align}
  F(x; \mu, \sigma) \equiv & 
  \Phi\left( \frac{x - \mu}{\sigma} \right)
  &
  \text{cannot be expressed analytically}
  \\
  f(x; \mu, \sigma) \equiv & 
  \frac{1}{\sigma} \phi\left( \frac{x - \mu}{\sigma} \right)
  &=
  \frac{1}{\sqrt{2 \mpi}}\frac{1}{\sigma}
  \me^{-\scriptstyle\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2}}
\end{align}
Moments (from wikipedia):
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-central moment & Central moment\\
    \midrule
    1 & $\mu$ & $0$ \\
    2 & ${\displaystyle \mu ^{2}+\sigma ^{2}} $ & ${\displaystyle \sigma ^{2}}$ \\
    3 & ${\displaystyle \mu ^{3}+3\mu \sigma ^{2}} $ & $0$ \\
    4 & ${\displaystyle \mu ^{4}+6\mu ^{2}\sigma ^{2}+3\sigma ^{4}}$ & ${\displaystyle 3\sigma ^{4}}$ \\
    5 & ${\displaystyle \mu ^{5}+10\mu ^{3}\sigma ^{2}+15\mu \sigma ^{4}}$ & $0$ \\
    6 & ${\displaystyle \mu ^{6}+15\mu ^{4}\sigma ^{2}+45\mu ^{2}\sigma ^{4}+15\sigma ^{6}}$ & ${\displaystyle 15\sigma ^{6}}$ \\
    7 & ${\displaystyle \mu ^{7}+21\mu ^{5}\sigma ^{2}+105\mu ^{3}\sigma ^{4}+105\mu \sigma ^{6}}$ & $0$ \\
    8 & ${\displaystyle \mu ^{8}+28\mu ^{6}\sigma ^{2}+210\mu ^{4}\sigma ^{4}+420\mu ^{2}\sigma ^{6}+105\sigma ^{8}}$ & ${\displaystyle 105\sigma ^{8}}$ \\
    \bottomrule
  \end{tabular}
\end{center}
Characteristic function:
\begin{equation}
  \phi_{X}(t) = \me^{i t \mu - \frac{1}{2}\sigma^2 t^2}
\end{equation}
Moment generating function
\begin{equation}
M_x (s) = \me^{\mu s + \frac{1}{2}{\sigma^2 s^2}}
\end{equation}

Properties: if $X_{i} \sim N(\mu_{i}, \sigma_{i}^{2})$ are
independent normals
\begin{equation}
  \sum_{i} X_{i} \sim 
  N \left(\sum_{i} \mu_{i}, \sum_{i} \sigma_{i}^{2} \right).
\end{equation}

\paragraph{Conditional Expectations}
\label{sec:normal-cond-expect}

If $X \sim N(\mu, \sigma)$, 
\begin{eqnarray}
\E [X|X < a ]
&=&
 \mu - \sigma \frac {\phi( \frac{a - \mu}{\sigma} )} {\Phi(
   \frac{a - \mu}{\sigma})}
 =
 \mu - \sigma\lambda \left( \frac{a - \mu}{\sigma} \right)
\\
\E [X|X>a ]
&=&
\mu + \sigma \frac {\phi( \frac{a - \mu}{\sigma} )} 
{1 - \Phi(\frac{a - \mu}{\sigma})}
=
\mu + \sigma \lambda \left( \frac{ \mu - a}{\sigma} \right)
\\
\E [X|X \in [a,b]]
&=&
 \mu - \sigma \frac
 {\phi \left( \frac{b - \mu}{\sigma} \right) - 
   \phi\left( \frac{a - \mu}{\sigma}\right)}
 {\Phi\left(\frac{b - \mu}{\sigma}\right) - 
   \Phi\left(\frac{a - \mu}{\sigma}\right)}
\end{eqnarray}

If $X \sim N(0, \sigma)$, 
\begin{align}
\E[X|X<a]
 &=
 -\sigma \lambda(\frac{a}{\sigma})
\\
\E[X|X>a] 
&=
\sigma \lambda(-\frac{a}{\sigma})
\\
\E[X^2|X < a]
&=
\sigma^2 - \sigma a \lambda \left(\frac{a}{\sigma} \right)
\\
\E[X^2|X > a]
&=
\sigma^2 + \sigma a \lambda \left(-\frac{a}{\sigma} \right)
\\
\E[X^2|X > -a \land X < a]
&=
\sigma^2 - 
2 \frac{\sigma a \phi\left(\frac{a}{\sigma} \right)}
{1 - 2 \Phi\left(\frac{a}{\sigma} \right)}
\\
\E[X^2|X < -a \lor X > a] 
&=
\E[X^2|X > a]
\\
\var[X|X <a] 
&= 
\sigma^2 \left[1 - 
  \frac{a}{\sigma} \lambda\left(\frac{a}{\sigma}\right) -
  \lambda^2(\frac{a}{\sigma}) \right]
\\
\var[X|X > a] 
&=
\sigma^2 \left[1 + 
  \frac{a}{\sigma} \lambda \left(-\frac{a}{\sigma} \right) - 
  \lambda^2(-\frac{a}{\sigma}) \right] \\
\end{align}

Let $X \sim N(\mu_{X}, \sigma_{X}^{2})$ and $Y \sim N(\mu_{Y},
\sigma_{Y}^{2})$, $X \independent Y$.  Now
\begin{align}
  \E[X|X < Y] 
  &=
  \mu_{X}
  - \frac{\sigma_{X}^{2}}{\sqrt{\sigma_{X}^{2} + \sigma_{Y}^{2}}}
  \lambda\left( -\frac{\mu_{X} - \mu_{Y}}
    {\sqrt{\sigma_{X}^{2} + \sigma_{Y}^{2}}}
    \right)
  \\
  \E[X|X > Y] 
  &=
  \mu_{X}
  + \frac{\sigma_{X}^{2}}{\sqrt{\sigma_{X}^{2} + \sigma_{Y}^{2}}}
  \lambda\left( \frac{\mu_{X} - \mu_{Y}}
    {\sqrt{\sigma_{X}^{2} + \sigma_{Y}^{2}}}
    \right).
\end{align}
Proof: write $\E[X|X < Y] = \E[X|Z < 0]$ where $Z = X-Y$.  Now
follows from \eqref{eq:normal2d_E[X1|X2<a]}


\subsubsection{Pareto Distribution}
\label{sec:pareto-distribution}

\index{distributions!Pareto|textbf}
Describes the upper part of many highly unequal distributions. 
\begin{align}
  F_X (x) 
  &= 
    1 - \left( \frac{x_0}{x} \right)^\alpha, 
    \quad x \ge x_0 > 0; \quad \alpha > 0
  \\
  f_{X}(x) &=
             \alpha \, x_{0}^{\alpha} \, x^{-\alpha-1}
  \\
  \E X &=
         \frac{\alpha}{\alpha - 1} x_{0},
         \quad \alpha > 1
  \\
  \E X|X > q &= \frac{\alpha}{\alpha - 1} q, 
                   \quad\alpha > 1,
                   \quad\text{independent of $x_{0}$}
  \\
  \E X|X < q &= \frac{\alpha}{\alpha - 1}
               \frac{1 - q^{-\alpha+1}}{1 - q^{-\alpha}}
               , 
                   \quad\alpha > 1
  \\
  \text{quantile}\quad
  q_{\tau} &= x_{0} (1 - \tau)^{-\frac{1}{\alpha}},
             \quad\alpha > 0
\end{align}
Moments:
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-Central & Central\\
    \midrule
    1 & $\displaystyle\frac{\alpha}{\alpha - 1} x_{0}, \quad \alpha > 1$ & $0$ \\[1.4ex]
    2 &  & $\displaystyle\frac{\alpha x_{0}^{2}}{(\alpha-1)^{2}(\alpha-2)},
           \quad \alpha>2$\\
    \bottomrule
  \end{tabular}
\end{center}
Properties:
\begin{itemize}
\item power law:
  \begin{math}
    \log f_{X}(x) 
    = 
    \log \left( \alpha \, x_{0}^{\alpha} \right)
    -(\alpha + 1) \log x
  \end{math}
  is linear on log-log scale.
\item it is \emph{scale-free}: there is no features in the right tail,
  wherever you look, you have most observations that are smaller, but
  you also have observations that are way larger.
\item \hyperref[sec:80-20-rule]{Pareto ratio}: 
  \index{Pareto ratio}
  The critical $x$ threshold $x_{*}$
  is given by (setting $x_{0}=1$)
  \begin{equation}
    \label{eq:pareto-80-20-threshold}
    x_{*}^{\alpha} - x_{*} - 1 = 0
  \end{equation}
  Example solutions are:
<<>>=
f <- function(x)
alpha*x^(-alpha - 1)
Fbar <- function(x, alpha)
x^-alpha
F <- function(x, alpha)
1 - Fbar(x, alpha)
criticalEq <- function(x, alpha) {
   x^alpha - x - 1
}
xCr <- function(alpha)
   uniroot(criticalEq, c(1, 20), alpha=alpha)$root
upper <- function(alpha)
   Fbar(xCr(alpha), alpha)
lower <- function(alpha)
   F(xCr(alpha), alpha)
@   
  \begin{center}
    \begin{tabular}{lll}
      \toprule
      $\alpha$ & $x_{cr}$ & upper/lower\\
      \midrule
      3 & \Sexpr{round(xCr(3), 3)} & \Sexpr{round(upper(3), 3)}/\Sexpr{round(lower(3), 3)}\\
      2 & \Sexpr{round(xCr(2), 3)} & \Sexpr{round(upper(2), 3)}/\Sexpr{round(lower(2), 3)}\\
      1.5 & \Sexpr{round(xCr(1.5), 3)} & \Sexpr{round(upper(1.5), 3)}/\Sexpr{round(lower(1.5), 3)}\\
      1.2 & \Sexpr{round(xCr(1.2), 3)} & \Sexpr{round(upper(1.2), 3)}/\Sexpr{round(lower(1.2), 3)}\\
      1.1609 & \Sexpr{round(xCr(1.1609), 3)} & \Sexpr{round(upper(1.1609), 3)}/\Sexpr{round(lower(1.1609), 3)}\\
      1.1 & \Sexpr{round(xCr(1.1), 3)} & \Sexpr{round(upper(1.1), 3)}/\Sexpr{round(lower(1.1), 3)}\\
      \bottomrule
    \end{tabular}
  \end{center}
\end{itemize}

Shifted Pareto, where the $x$ values start from $0$, is called
\hyperref[sec:lomax-distribution]{Lomax Distribution} \index{distributions!Lomax} or Pareto-II distribution.


\selectlanguage{estonian}
\subsubsection{Pööratud normaaljaotus}
Tihedusfunktsioon:
\begin{equation}
f(t) = \frac{1}{t^\frac{3}{2}}
  \phi \left( \frac{ \mu t - 1}{\sigma\sqrt{t}} \right)
\end{equation}
ja jaotusfunktsioon:
\begin{equation}
F(t) = 
  \Phi \left( \frac{\mu t -1}{\sigma\sqrt{t}} \right) -
  e^{2\frac{\mu}{\sigma^2}}
    \Phi \left( - \frac{\mu t+1}{\sigma \sqrt{t}} \right).
\end{equation}
Kõik momendid on olemas kui $\mu>0$:
\begin{eqnarray}
\E T &=& \frac{1}{\mu} \\
\var T &=& \frac{\sigma^2}{\mu^3}.
\end{eqnarray}
Kui $\mu=0$, on jaotus korralik, positiivsed momendid aga puuduvad.  


\subsubsection{$t$-Distribution}
\label{sec:t-distribution}

\index{distributions!t@$t$|textbf}
Used for \emph{$t$-test}.  For $n$ \emph{degrees of freedom}:
\begin{align}
  f(x)
  &=
  \frac{\Gamma \left( \frac{n + 1}{2} \right)}
  {\sqrt{n \mpi} \Gamma \left( \frac{n}{w} \right)}
  \left(
    1 + \frac{x^{2}}{n}
  \right)^{-\frac{n + 1}{2}}
  \\
  \E X &= 0
  \\
  \var X &= \frac{n}{n - 2}
  \\
  \text{skewness } g_{1} &= 0
  \\
  \text{curtosis } g_{2} &= \frac{3n - 6}{n - 4} \qquad (n > 4)
\end{align}
Special case where $n=1$ is \hyperref[sec:cauchy-distribution]{Cauchy distribution}.

\subsubsection{Triangular Distribution}

\begin{align}
  F(x) &= 
         \begin{cases}
           0 & x < 0\\
           x^{2} & 0 \le x \le 1\\
           1 & x > 1
         \end{cases}
  \\               
  f(x) &= 
         \begin{cases}
           2x & 0 \le x \le 1\\
           0 & \text{elsewhere}
         \end{cases}
  \\
  \E X &= \frac{2}{3}
  \\
  \E X^{2} &= \frac{1}{2}
  \\
  \var X &= \frac{1}{18}
\end{align}

\subsubsection{Type-1 Extreme Value Distribution $EV_{1}$}
\label{sec:type-1-extreme-value-distribution}
(Also Gumbel distribution.)
\index{distributions!Gumbel|see {type-1 extreme value}}
\index{distributions!type-1 extreme value|textbf}

Describes $\log T$ when $T \sim \mathcal{E}(1)$.
\begin{align}
f(x) &= \me^{-x} \me^{-\me^{-x}}
\\
F(x) &=  \me^{-\me^{-x}}.
\\
\E x &= -c \approx 0,5772
\end{align}

\subsubsection{Type-2 Extreme Value Distribution $EV_{2}$}
\label{sec:type-2-extreme-value-distribution}
(Also Fréchet distribution or inverse Weibull distribution.)
\index{distributions!type-2 extreme value|textbf}

\begin{align}
  f(x) &= \alpha x^{-1-\alpha} \me^{-x^{-\alpha}}
  \\
  F(x) &= \me^{-x^{-\alpha}}
\end{align}


\subsubsection{Uniform Distribution $Unif(a,b)$}

\begin{align}
\text{pdf}\qquad
f(x) &= \frac{1}{b - a}, \quad a \le x \le b
\\
\text{MGF}\qquad
M(s) &=
\frac{\me^{tb} - \me^{ta}}{t(b - a)}
\end{align}

Moments:
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-Central & Central\\
    \midrule
    1 & $\frac{1}{2}(a + b)$ & $0$ \\[1.2ex]
    2 & $\frac{1}{3}(a^{2} + ab + b^{2})$ & $\frac{1}{12}(b - a)^{2}$
    \\[1.2ex]
    3 & $\displaystyle \frac{1}{4}\frac{b^{4} - a^{4}}{b-a} =
        \frac{1}{4}(b^{2} + a^{2})(b + a)$ \\[1.3ex]
    4 & $\displaystyle \frac{1}{5}\frac{b^{5} - a^{5}}{b-a}$ & $ \frac{1}{80}(b - a)^{4}$ \\[1.3ex]
    \bottomrule
  \end{tabular}
\end{center}

\paragraph{Properties:}

\hyperref[sec:80-20-rule]{80-20 rule}: \index{80-20 rule} upper $(3 - \sqrt{5})/2 \approx 0.382$ of cases possess
$(\sqrt{5} - 1)/2 \approx 0.618$ of resources.

Sum of RV-s with uniform distribution: Let
\begin{equation}
  \label{eq:sumUnif}
  X \sim Unif(a,b) \qquad Y \sim Unif(c,d)
\end{equation}
be independent and $d - c \ge b - a$.
The density of $Z = X + Y$ is \\
\begin{asy}
unitsize(40mm);
defaultpen(fontsize(9));

real a = 2;
real b = 2.4;
real c = 1;
real d = 3;
real h = 1/(d - c);
real x0 = 0.9*(a + c);

path axes = (x0,1.2*h)--(x0,0)--(b+d+0.4,0);

draw(axes, Arrows(TeXHead, 1));
label("$X$", point(axes, 2), SW);
label(rotate(90)*"density", point(axes, 0), SW);

pair ph = (b+d,h);
draw((x0,h)--ph, dotted);
label("$h = \displaystyle\frac{1}{d - c}$", ph, E);

path density = (a+c,0)--(b+c,h)--(a+d,h)--(b+d,0);
draw(density, blue);
label("$a+c$", point(density, 0), S);
pair bc = point(density, 1);
draw(bc--(bc.x,0), dotted);
label("$b+c$", (bc.x,0), S);
pair ad = point(density, 2);
draw(ad--(ad.x,0), dotted);
label("$a+d$", (ad.x,0), S);
label("$b+d$", point(density, 3), S);
\end{asy}


\selectlanguage{estonian}
\subsubsection{Weibulli jaotus}
Weibulli jaotus on eksponentjaotuse üldistus, kasutatakse ajas
ühtlaselt kahaneva hasardi kirjeldamiseks.  Omadused:
\begin{eqnarray}
  F(t) &=& 1 - \me^{-(\lambda t)^\alpha}
  \\
  f(t) &=& \alpha \lambda^\alpha t^{\alpha - 1}
  \me^{-(\lambda t)^\alpha}\\
                                %
  \theta(t) &=& \alpha \lambda^\alpha t^{\alpha - 1}
\end{eqnarray}
\selectlanguage{english}
where $\lambda$ is scale- and $\alpha$ is the shape parameter.


\clearpage
\subsection{Multivariate Continuous Distributions}

\subsubsection{Dirichlet distribution}
\label{sec:dirichlet-distribution}

\index{distributions!Dirichlet|textbf}
Multivariate generalization of \hyperref[sec:beta-distribution]{beta distribution}.

\begin{align}
\text{pdf}\qquad
f(\vec{x}; \vec{\alpha}) &= \frac{1}{B(\vec{\alpha})} 
\prod_{k=1}^{K} x_{k}^{\alpha_{k} - 1}
\indic(\vec{x} \in S_{k})
\end{align}
where $S_{K}$ is the probability simplex:
\begin{equation}
  \label{eq:probability-simplex}
  S_{K} = \{ \vec{x} : 0 \le x_{k} \le 1, \sum_{k=1}^{K} x_{k} = 1 \}
\end{equation}
and $B(\vec{\alpha})$ is $K$-variable generalization of the beta
function:
\begin{equation}
  \label{eq:beta-generalization}
  B(\vec{\alpha}) = \frac{\prod_{k=1}^{K}
    \Gamma(\alpha_{k})}{\Gamma(\alpha_{0})}
  \qquad
  \alpha_{0} = \sum_{k=1}^{K} \alpha_{k}
\end{equation}
Mode:
\begin{equation}
  \label{eq:dirichlet-mode}
  m_{k} = \frac{\alpha_{k} - 1}{\alpha_{0} - K}
\end{equation}

Moments:
\begin{center}
  \begin{tabular}{lll}
    \toprule
    Order & Non-Central & Central\\
    \midrule
    1 & $\displaystyle\frac{\alpha_{k}}{\alpha_{0}}$ & $0$ \\[1.2ex]
    2 &  & $\displaystyle\frac{\alpha_{k} (\alpha_{0} -
           \alpha_{k})}{\alpha_{0}^{2}(\alpha_{0} + 1)}$
    \\[1.2ex]
    \bottomrule
  \end{tabular}
\end{center}

\paragraph{Properties}

$\vec{\alpha} = (1, 1, 1)$ gives uniform distribution.


\subsubsection{Inverse Wishart Distribution
  $\mathcal{W}^{-1}(\mat{\Psi}, \nu)$}
\label{sec:inverse-wish-distr}
\index{distributions!inverse Wishart|textbf}
If matrix RV $\mat{X}$ is
\hyperref[sec:wishart-distribution]{Wishart-distributed},
\index{distributions!Wishart}
then
$\mat{X}^{-1}$ is inverse-Wishart distributed.
Generalization of
\hyperref[sec:inverse-gamma-distribution]{inverse gamma distribution}
\index{distributions!inverse gamma}
to positive definite matrices.  (Note: here these are parameterized
differently.)
It has two parameters, a $K\times K$ positive definite scale matrix
$\mat{\Psi}$, and degrees of freedom $\nu > K - 1$.

Properties:
\begin{align}
  \label{eq:inverse-wishart-distribution}
  \text{pdf}\qquad
  & f(\mat{X}) =
    \frac{
    \left|\mat{\Psi}\right|^{\nu/2}
    }{
    2^{\frac{1}{2}\nu K}\Gamma_K(\frac{\nu}{2})
    } \left|
    \mat{X}\right|^{
    -\frac{1}{2}(\nu + K +1)
    } e^{
    -\frac{1}{2}\Tr(\mat{\Psi}\mat{X}^{-1})
    }
\end{align}
where $\Gamma_{p}$ is
\hyperref[sec:multivariate-gamma]{multivariate gamma function}.
\index{functions!gamma!multivariate}

Inverse Wishart is a conjugate distribution for covariance
matrices.  Suppose we want to estimate covariance matrix
$\mat{\Sigma}$ and the
prior is that it is inverse Wishart--distributed:
\begin{equation}
  \label{eq:inverse-wishard-prior}
  \mat{\Sigma} \sim \mathcal{W}^{-1}(\mat{\Psi}, \nu)
\end{equation}
Where $\mat{\Psi}$ is a $K\times K$ positive definite symmetric matrix.
If we observe $\mat{X} = (\vec{x}_{1}, \vec{x}_{2}, \dots,
\vec{x}_{K})$, a $N\times K$ matrix of values generated from $N(0,
\mat{\Sigma})$, then the posterior estimate of $\mat{\Sigma}$ is
\begin{equation}
  \label{eq:inverse-wishart-posterior}
  \mat{\Sigma}|\mat{X} \sim
  \mathcal{W}^{-1}(\mat{\Psi} + \mat{X}^{\transpose}\;\mat{X}, \nu + N)
\end{equation}


\subsubsection{Multivariate Normal $N(\mu, \mat{\Sigma})$}

\selectlanguage{estonian}
N-mõõtmelise normaajaotuse jaotusfunktsioon: Olgu
\begin{equation}
  \vec{X} \sim N(\vec{\mu}, \mat{\Sigma}),
\end{equation}
kus $\vec\mu$ on keskväärtus ja $\mat\Sigma$ dispersioonimaatriks.
Siis:
\begin{equation}
f_{\vec{X}} (\vec x) = 
  (2\mpi)^{-\frac{n}{2}} 
  |\mat\Sigma|^{-\frac{1}{2}}
  \me^{-\frac{1}{2}(\vec x - \vec\mu)' \mat\Sigma^{-1} (\vec x -
    \vec\mu)}.
\label{eq:N-Dimensional_normal}
\end{equation}

\paragraph{2D Conditional Distributions}
Let
\begin{math}
  \vec{X}=
  \begin{pmatrix}
    X_1\\
    X_2
  \end{pmatrix}
  \sim
  N \left( 
    \begin{pmatrix}
      0\\0
    \end{pmatrix},
    \begin{pmatrix}
      \sigma_1^2 & \sigma_{12} \\ 
      \sigma_{12} & \sigma_2^2
    \end{pmatrix}
\right)
\end{math}.

The exponent in the distribution function can be expressed as
\begin{equation}
  -\frac{1}{2} \left(
    \frac{
      \sigma_{2}x_{1}^{2} - 2 \sigma_{12} x_{1} x_{2} +
      \sigma_{1}^{2} x_{2}^{2}}
    {\sigma_{1}^{2} \sigma_{2}^{2} - \sigma_{12}^{2}}
  \right)
  =
  -\frac{1}{2} \left[
    \frac{\left(
        x_{1} - 
        \displaystyle\frac{\sigma_{12}}{\sigma_{2}^{2}}x_{2}
      \right)^{2}}
    {\sigma_{1}^{2} - \displaystyle\frac{\sigma_{12}^{2}}{\sigma_{2}^{2}}}
    +
    \frac{x_{2}^{2}}{\sigma_{2}^{2}}
  \right].
  \label{eq:normal2D_exponent}
\end{equation}
Accordingly, based on Bayesian law the probability density of 2D normal
\begin{math}
f_{X_{1},X_{2}}(x_{1},x_{2}) = 
f_{X_{1}|X_{2}}(x_{1},x_{2}) f_{X_{2}}(x_{2}) =
f_{X_{2}|X_{1}}(x_{1},x_{2}) f_{X_{1}}(x_{1})
\end{math}
where all the conditional and marginal distribution functions are
normal:
\begin{align}
  (X_{1}|X_{2} = x_{2}) 
  &\sim
  N \left(
    \frac{\sigma_{12}}{\sigma_{2}^{2}} x_{2},
    \sigma_{1}^{2} - \frac{\sigma_{12}^{2}}{\sigma_{2}^{2}}
  \right)
  &
  X_{2} &\sim N(0, \sigma_{2}^{2})
  \\
  (X_{2}|X_{1} = x_{1}) 
  &\sim
  N \left(
    \frac{\sigma_{12}}{\sigma_{1}^{2}} x_{1},
    \sigma_{2}^{2} - \frac{\sigma_{12}^{2}}{\sigma_{1}^{2}}
  \right)
  &
  X_{1} &\sim N(0, \sigma_{1}^{2})
\end{align}

Distribution for $(X_{1}|X_{2} < a)$:
\begin{equation}
  f_{X1|X2 < a}(x_{1}) = 
  \frac{1}{\sigma_{1}}
  \frac{\phi \left( \frac{x_{1}}{\sigma_{1}} \right)}
  {\Phi \left( \frac{a}{\sigma_{2}} \right)}
  \Phi \left(
    \frac{a - \frac{\sigma_{12}}{\sigma_{1}^{2}} x_{1}}
    {\sqrt{\sigma_{2}^{2} - \frac{\sigma_{12}^{2}}{\sigma_{1}^{2}}}}
  \right)
\end{equation}

Distribution for $(X_{1}|X_{2} > a)$:
\begin{equation}
  f_{X1|X2 > a}(x_{1}) = 
  \frac{1}{\sigma_{1}}
  \frac{\phi \left( \frac{x_{1}}{\sigma_{1}} \right)}
  {\Phi \left( -\frac{a}{\sigma_{2}} \right)}
  \Phi \left(
    - \frac{a - \frac{\sigma_{12}}{\sigma_{1}^{2}} x_{1}}
    {\sqrt{\sigma_{2}^{2} - \frac{\sigma_{12}^{2}}{\sigma_{1}^{2}}}}
  \right)
\end{equation}

\paragraph{Conditional Expectations}
Let
\begin{math}
  \vec{X}=
  \begin{bmatrix}
    X_1\\
    X_2
  \end{bmatrix}
  \sim
  N(\vec{\mu}, \vec{\Sigma})
\end{math}, where
\begin{math}
\vec{\Sigma} = \left[
 \begin{array}{rr} \sigma_1^2 & \sigma_{12}\\ \sigma_{12} & \sigma_2^2
 \end{array}\right]
\end{math} and
\begin{math}
\vec{\mu}=\left[\begin{array}{r} \mu_1\\
 \mu_2\end{array}\right]
\end{math}:

\begin{equation}
  (X_1|X_2 = x_2)
  \sim 
  N\left(\mu_1 + \frac{\sigma_{12}}{\sigma_2^2}(x_2 - \mu_2),
    \sigma_1^2 - \frac{\sigma_{12}^2}{\sigma_2^2}\right)
\label{eq:tinglik normaaljaotus}
\end{equation}
(follows from \ref{eq:normal2D_exponent}) and
\begin{align}
  \E[X_1|X_2 < a] 
  & = 
  \mu_1 - \frac{\sigma_{12}}{\sigma_2}
  \lambda 
  \left( \frac{a - \mu_2}{\sigma_2} \right)
  \label{eq:normal2d_E[X1|X2<a]}
  \\
  \E[X_1|X_2 > a] 
  & = 
  \mu_1 + \frac{\sigma_{12}}{\sigma_2}
  \lambda \left( \frac{\mu_2 - a}{\sigma_2} \right)
  \\
  \E[X_{1}^{2}|X_{2} < a]
  &=
  \sigma_{1}^{2} - \frac{\sigma_{12}^{2}}{\sigma_{2}^{3}}
  a \lambda \left( \frac{a}{\sigma_{2}} \right)
  \\
  \E[X_{1}^{2}|X_{2} > a]
  &=
  \sigma_{1}^{2} + \frac{\sigma_{12}^{2}}{\sigma_{2}^{3}}
  a \lambda \left( -\frac{a}{\sigma_{2}} \right)
  \\
  \E[X_1^2|X_2 \in \mathcal{A}]
  &=
  \frac{\sigma_{12}^2}{\sigma_2^4}
  \E[X_2^2|X_2 \in \mathcal{A}] +
  \sigma_1^2 - \frac{\sigma_{12}^2}{\sigma_2^2}
  \\
  \var[ X_1|X_2 < a]
  & =
  \sigma_{1}^{2} 
  -
  \frac{\sigma_{12}^2}{\sigma_{2}^{3}} a 
  \lambda \left( \frac{a}{\sigma_{2}} \right) 
  - 
  \frac{\sigma_{12}^2}{\sigma_{2}^{2}}  
  \lambda^2 \left( \frac{a}{\sigma_{2}} \right)
  \label{eq:normal_var[X1|X2<a]}
  \\
  \var[ X_1|X_2 > a]
  & =
  \sigma_{1}^{2} 
  +
  \frac{\sigma_{12}^2}{\sigma_{2}^{3}} a 
  \lambda \left(-\frac{a}{\sigma_{2}} \right) 
  - 
  \frac{\sigma_{12}^2}{\sigma_{2}^{2}}  
  \lambda^2 \left(-\frac{a}{\sigma_{2}} \right)
  \label{eq:normal_var[X1|X2>a]}
\end{align}
Proof: write (\ref{eq:tinglik normaaljaotus}) $\Rightarrow X_1 =
\mu_1 + \frac{\sigma_{12}}{\sigma_2}(X_2 - \mu_2) + E$,
where $E$ and $X_2$ are independent (property of normal distribution).  Find $X_1|X_2
\in \mathcal{A} = \frac{\sigma_{12}}{\sigma_2^2} \E[X_2|X_2 \in
\mathcal{A}] + E$.


\paragraph{Multiplying normals}

\begin{equation}
\frac{1}{\sigma_1}\phi\left( \frac{x-ay}{\sigma_1} \right)
  \frac{1}{\sigma_2}\phi\left( \frac{y-b}{\sigma_2} \right) =
%
\frac{1}{\sigma_x}\phi\left( \frac{x-ab}{\sigma_x} \right)
  \frac{1}{\sigma_y}\phi\left( \frac{y-\mu_y}{\sigma_y} \right),
  \label{eq:normaaljaotuste_korrutis}
\end{equation}
\selectlanguage{english}
where
\begin{align*}
\sigma_x^{2}
&= 
\sigma_1^2 + \sigma_2^2 a^2 
&
        \sigma_y &= \frac{\sigma_1 \sigma_2}
          {\sqrt{\sigma_1^2 + \sigma_2^2 a^2}} \\
\mu_y &=
  \frac{\sigma_1^2 b + \sigma_2^2 a x}{\sigma_1^2 + \sigma_2^2 a^2}
\end{align*}

The same in multi-dimensional case:
\begin{multline}
  \frac{1}{\sigma_1}\phi\left( \frac{x_{1} - y}{\sigma_1} \right)
  \frac{1}{\sigma_1}\phi\left( \frac{x_{2} - y}{\sigma_1} \right)
  \dots
  \frac{1}{\sigma_1}\phi\left( \frac{x_{n} - y}{\sigma_1} \right)
  \frac{1}{\sigma_2}\phi\left( \frac{y}{\sigma_2} \right) 
  =
  \\
  =
  \prod_{i=1}^{n}
  \frac{1}{\sigma_1}\phi\left( \frac{x_{i} - y}{\sigma_1} \right)
  \frac{1}{\sigma_2}\phi\left( \frac{y}{\sigma_2} \right) 
  =
  \\
  =
  \frac{1}{\sigma_x}\phi\left( \frac{x_{1}}{\sigma_x} \right)
  \frac{1}{\sigma_x}\phi\left( \frac{x_{2}}{\sigma_x} \right)
  \dots
  \frac{1}{\sigma_x}\phi\left( \frac{x_{n}}{\sigma_x} \right)
  \frac{1}{\sigma_y}\phi\left( \frac{y-\mu_y}{\sigma_y} \right)
  =
  \\
  =
  \prod_{i=1}^{n}
  \frac{1}{\sigma_x}\phi\left( \frac{x_{i}}{\sigma_x} \right)
  \frac{1}{\sigma_y}\phi\left( \frac{y-\mu_y}{\sigma_y} \right)
  \label{eq:multivariate-normaaljaotuste-korrutis}
\end{multline}
where
\begin{align*}
  \sigma_x^{2}
  &= 
  \sigma_{1}^{2}
  \frac{\sigma_1^2 + n \sigma_2^2}
  {\sigma_1^2 + (n - 1) \sigma_2^2}
  &
  \sigma_y 
  &= 
  \frac{\sigma_1 \sigma_2}
  {\sqrt{\sigma_1^2 + n \sigma_2^2}} 
  \\
  \mu_y 
  &=
  \frac{\sigma_2^2 \sum_{i=1}^{n} x_{i}}{\sigma_1^2 + n \sigma_2^2}
\end{align*}


\subsubsection{Wishart distribution}
\label{sec:wishart-distribution}

Wishart distribution is a generalization of \hyperref[sec:gamma-distribution]{gamma distribution} to
positive definite matrices.  

Probability density:
\begin{equation}
  \label{eq:wishart}
  f(\mat{X}) = \frac{1}{Z_{Wi}} |\mat{X}|^{\frac{1}{2}(\nu - D - 1)}
               \me^{-\frac{1}{2} \Tr (\mat{X} \mat{S}^{-1})}
\end{equation}
where $\nu$ is degrees of freedom, $\mat{S}$ is scale matrix, and 
\begin{equation}
  \label{eq:wishart-normalizer}
  Z_{Wi} = 2^{\frac{1}{2}\nu D} \Gamma_{D} \left( \frac{\nu}{2} \right)
  |\mat{S}|^{\frac{\nu}{2}}.
\end{equation}
here $\Gamma_{d}(\cdot)$ is
\hyperref[sec:multivariate-gamma]{multivariate gamma} function.

Properties:
\begin{align}
  \label{eq:wishart-expectation}
  \E \mat{X} & = \nu \mat{S}
  \\
  \text{mode} &= (\nu - D - 1)\mat{S} \quad\text{for} \quad \nu > D + 1
\end{align}
If $\mat{X}$ is Wishart-distributed, then $\mat{X}^{-1}$ is
\hyperref[sec:inverse-wish-distr]{inverse-Wishart distributed}.
\index{distributions!inverse Wishart}

\subsection{Distribution Families}

\subsubsection{Exponential Family}
\label{sec:exponential-family}

Exponential family is a distribution whose density can be written as
\begin{equation}
  \label{eq:exponential_family_density}
  p(x|\vec{\eta}) = 
  h(x) \me^{\vec{\eta}` T(x) - A(\vec{\eta})} =
  \frac{h(x)}{Z(\vec{\eta})} \me^{\vec{\eta}` T(x)}
  ,
\end{equation}
where $\vec{\eta}$ is the canonical parameter, $T(x)$ sufficient statistic,
and $A(\vec{\eta}) = \log Z(\vec{\eta})$ is the 
cumulant function.  Instead of the canonical parameter, one
can use another parameter $\vec{\theta}$: $\vec{\eta} = \vec{\eta}(\vec{\theta})$:
\begin{equation}
  \label{eq:exponential_family_other_param}
  p(x|\vec{\theta}) = h(x) \me^{\vec{\eta}(\vec{\theta})` T(x) - A(\vec{\eta}(\vec{\theta}))}.
\end{equation}


\selectlanguage{estonian}
\subsubsection{Stabiilne pere}
Mittenegatiivsesse stabiilsesse perre kuuluvad jaotused, mille
momendifunktsioon on
\begin{equation}
M_x (s) = e^{-s^\alpha}, \qquad 0 < \alpha \le 1.
\end{equation}
Stabiilsel pere omadused:
\begin{enumerate}
\item kui juhusliku muutuja $X_i$ jaotusfunktsioon on $G_\alpha$ mis kuulub
  stabiilsesse perre, siis juhusliku muutuja
  \begin{equation}
    Y = n^{-\frac{1}{\alpha}} \sum_{i=1}^N X_i
  \end{equation}
  jaotusfunktsioon on kah $G_\alpha$.
\item Momendifunktsiooni tuletis
  \begin{equation}
    M_x'(s) = -\alpha s^{\alpha - 1} M(s)
  \end{equation}
\end{enumerate}


\subsection{Functions of Random Variables}
\label{sec:functions-of-random-variables}


Let $\vec{X} = (X_{1}, X_{2}, \dots, X_{K})$ be a vector of i.i.d
random variables with $\E X_{i} X_{j}
= \kronDelta_{ij} \sigma^{2}$, and $\mat{A}$ be a $K\times K$ matrix.  Then
\begin{equation}
  \E [\vec{X}^{\transpose} \, \mat{A} \, \vec{X}]
  =
  \Tr \mat{A} \cdot \E X^{2}
\end{equation}



\newpage
\selectlanguage{english}
\section{Estimators}
\label{sec:estimators}

\subsection{M-Estimators}
\label{sec:M-Estimators}

\subsubsection{Variance}
\label{sec:M-Estimators_variance}

Let an estimator solve
\begin{equation}
  H = \sum_{i} h_{i}(\vec{\hat{\theta}}) = 0.
\end{equation}
From Taylor approximation
\begin{equation}
  \sum_{i} h_{i}(\vec{\hat{\theta}}) 
  = 
  \sum_{i} h_{i}(\vec{\theta}_{0}) 
  +
  \pderiv{\vec{\theta}} 
  \sum_{i} h_{i}(\vec{\theta})\Big|_{\vec{\theta}_{0}}
  (\vec{\hat{\theta}} - \vec{\theta}_{0} )
  = 0
\end{equation}
from where
\begin{equation}
  \vec{\hat{\theta}} - \vec{\theta}_{0} 
  =
  -\left(
    \pderiv{\vec{\theta}} 
    \sum_{i} h_{i}(\vec{\theta})\Big|_{\vec{\theta}_{0}}
  \right)^{-1}    
  \sum_{i} h_{i}(\vec{\theta}_{0}).
\end{equation}
The estimate for variance is
\begin{equation}
  \label{eq:M-estimator_variance}
  \var \vec{\hat{\theta}} =
  \mat{\hat{A}}^{-1} \widehat{\var H} \mat{\hat{A}}^{-1}
\end{equation}
where
\begin{equation}
  \label{eq:M-estimators_A}
  \mat{\hat{A}} = 
  \pderiv{\vec{\theta}} 
  \sum_{i} h_{i}(\vec{\theta})\Big|_{\vec{\theta}_{0}}
\end{equation}
and $\widehat{\var H}$ is an estimator for $\var H$.


\subsection{Maximum likelihood}
\label{sec:maxlik}

\subsubsection{Definition}
\label{sec:ml_definition}

Let the random variables $X_1, X_2, \dots, X_n$ be \iid distributed
according to a distribution function $F(\cdot|\vec{\vartheta})$ and
corresponding density function $
f(\cdot|\vec{\vartheta})$.
Let $f(\cdot|\vec{\vartheta})$ be specified fully parametrically with
a finite unknown parameter vector $\vec{\vartheta}$.  The
\emph{log-likelihood} function of the observed values $x_{1}, x_{2}, \dots,
x_{n}$ is:
\begin{equation}
  \loglik(\vec{\vartheta}|x_1, x_2, \dots, x_n)
  =
  \frac{1}{n}
  \sum_{i=1}^n
  \log f(x_i|\vec{\vartheta}).
\end{equation}
The \emph{score} is defined as
\begin{equation}
  \label{eq:ml_score}
  g(\vec{\vartheta}|x_1, x_2, \dots, x_n)
  =
  \pderiv{\vec{\theta}} \loglik(\vec{\vartheta}|x_1, x_2, \dots, x_n)  
\end{equation}
The \emph{maximum likelihood} estimator of $\vec{\vartheta}$ is the
value of $\vec{\vartheta}$ which maximises the log-likelihood
function:
\begin{equation}
  \hat{\vec{\vartheta}}
  =
  \arg \max_{\vec{\vartheta}}
  \loglik(\vec{\vartheta}|x_1, x_2, \dots, x_n).
\end{equation}

\subsubsection{Information matrix}

Information matrix is defined as
\begin{equation}
  I(\vec{\vartheta})
  \equiv
  - \E \left[ 
    \frac{\partial^2 \loglik(\vartheta)}
    {\partial \vec{\vartheta} \partial \vec{\vartheta}'}
  \right]
  =
  \E \left[
    \pderiv[\loglik(\vec{\vartheta})]
    {\vec{\vartheta}}
    \pderiv[\loglik(\vec{\vartheta})]
    {\vec{\vartheta}'}
  \right].
\end{equation}
(information equality.)

\subsubsection{Relationship to Kullback-Leibler divergence}

ML estimator can be written as
\begin{equation}
  \hat{\vec{\vartheta}}
  =
  \arg \max_{\vec{\vartheta}}
  \int \log f(x|\vec{\vartheta}) \,\dif F_n(x),
\end{equation}
where $F_n(x)$ is the empirical distribution function:
\begin{equation}
  F_n(x) 
  =
  \frac{1}{n}
  \sum_{i=1}^n
  \indic(X_i \le x).
\end{equation}
Further, we may write the estimator as
\begin{equation}
  \hat{\vec{\vartheta}}
  =
  \arg \min_{\vec{\vartheta}}
  \left[
    \int \log f(x|\vec{\vartheta}_{0}) \,\dif F_n(x)
    -
    \int \log f(x|\vec{\vartheta}) \,\dif F_n(x)
  \right],
\end{equation}
where $f(\cdot|\vec{\vartheta}_{0})$ is the true density function of
$X$ that does not depend on $\vec{\vartheta}$.  Hence
\begin{equation}
  \hat{\vec{\vartheta}}
  =
  \arg \min_{\vec{\vartheta}}
  \int 
  \log \frac{f(x|\vec{\vartheta}_{0})}{f(x|\vec{\vartheta})} 
  \,\dif F_n(x)
  =
  \arg \min_{\vec{\vartheta}}
  KL(f|\vec{\vartheta}_0 || f |\vec{\vartheta}),
\end{equation}
the
\hyperref[sec:kullback-leibler-divergence]{Kulback-Leibler divergence} of
distributions $f|\vec{\vartheta}_0$ and $f |\vec{\vartheta}$.


\subsection{Generalized Method of Moments}
\label{sec:gmm}

The asymptotic variance of the estimator is
\begin{equation}
  V_{GMM} = \frac{1}{n} 
  \left[ \mat{\Gamma}' \, \mat{W} \mat{\Gamma} \right]^{-1}
\end{equation}
where $\mat{W}$ is the weighting matrix and 
\begin{equation}
  \mat{\Gamma} = \pderiv[\bar m(\vec{\theta})]{\vec{\theta}'}
\end{equation}

\paragraph{Optimal Weighting Matrix}

\begin{equation}
  W^{*} = \frac{1}{n} 
  \left\{\text{Asy.} \var \left[\frac{1}{n} \sum_{i} m_{i} \right] \right\}^{-1}
\end{equation}


\subsubsection{Optimal Weighting Matrices and Asymptotic Variances}
\label{sec:optimalW}

Assume we have i.i.d sample of random values $X_{i}$.
Let $\E X = \mu$, $\var X = \sigma^{2}$, $\E X^{2} = \mu_{2} = \mu^{2}
+ \sigma^{s}$, $\E X^{4} = \mu_{4}$.  

For the moment condition
\begin{equation}
  \E X - \mu = 0
\end{equation}
the optimal weighting matrix:
\begin{equation}
  W = 
  \frac{1}{n} \left(\frac{\mu_{2} - \mu^{2}}{n} \right)^{-1}
  =
  \frac{1}{\sigma^{2}}
  \quad\text{and}\quad
  \var \hat\mu = \frac{1}{n} \sigma^{2}
\end{equation}

For the moment condition
\begin{equation}
  \E X^{2} - \mu_{2} = 0
\end{equation}
the optimal weighting matrix:
\begin{equation}
  W = 
  \frac{1}{n} \left(\frac{\mu_{4} - \mu_{2}^{2}}{n} \right)^{-1}
  =
  \frac{1}{\mu_{4} - \mu_{2}^{2}}
  \quad\text{and}\quad
  \var \hat\mu = \frac{1}{n} \frac{\mu_{4} - \mu_{2}^{2}}{4\mu^{2}}
\end{equation}

For the moment condition
\begin{equation}
  \begin{pmatrix}
      \E X - \mu\\
      \E X^{2} - \mu_{2}\\
  \end{pmatrix}
  =
  \begin{pmatrix}
    0 \\ 0
  \end{pmatrix}
  \quad\text{or}\quad
  \bar m =
  \begin{pmatrix}
      \frac{1}{n} \sum_{i}X_{i} - \mu\\
      \frac{1}{n} \sum_{i}X_{i}^{2} - \mu_{2}\\
  \end{pmatrix}.
\end{equation}
Matrix
\begin{equation}
  \mat{\Gamma} = \pderiv[\bar m((\vec{\mu},
  \vec{\sigma}^{2})')]{(\vec{\mu}, \vec{\sigma}^{2})} =
  \begin{pmatrix}
    -1 & 0 \\ -2\mu & -1
  \end{pmatrix}
\end{equation}
we have 
the optimal weighting matrix:
\begin{equation}
  \mat{W}^{-1} =
  \begin{pmatrix}
    \mu_{2} - \mu^{2} & \mu_{3} - \mu \mu_{2} \\
    \mu_{3} - \mu \mu_{2} & \mu_{4} - \mu_{2}^{2}
  \end{pmatrix}
  =
  \begin{pmatrix}
    \sigma^{2} & \mu_{3} - \mu \mu_{2} \\
    \mu_{3} - \mu \mu_{2} & \mu_{4} - \mu_{2}^{2}
  \end{pmatrix}
\end{equation}
and the variance
\begin{equation}
  \mat{V} = \frac{1}{n}
  \begin{pmatrix}
    \sigma^{2} & 
    -2\,\mu\, \sigma^{2} - \mu_2 \mu + \mu_3 \\ 
    -2\,\mu\, \sigma^{2} - \mu_2 \mu + \mu_3 &
    -4\mu^{2} \sigma^{2} - 4\mu \mu_{3} +2 \mu^{2}\mu_{2} + \mu_4 - \mu_2^2
  \end{pmatrix}
\end{equation}
If $\mu_{3} = \mu = 0$, the
variance is
\begin{equation}
  \mat{V} = \frac{1}{n}
  \begin{pmatrix}
    \sigma^{2} &  0 \\
    0 & \mu_{4} - \sigma^{2}
  \end{pmatrix}
\end{equation}


\subsection{Entropy Distance}
\label{sec:entropy_distance}

\subsubsection{Entropy Distance}
\label{sec:lin1991_entropy_distance}

\citet{lin1991} defines \emph{entropy distance:}

\begin{equation}
  K(p_{1},p_{2})
  =
  \sum_{x} p_{1}(x)
  \log_{2}
  \frac{p_{1}(x)}
  {\frac{1}{2}[p_{1}(x) + p_{2}(x)]}
  =
  1 + \frac{1}{\log 2}
  \sum_{x} p_{1}(x)
  \log
  \frac{p_{1}(x)}
  {p_{1}(x) + p_{2}(x)}
\end{equation}
Properties: $K(p_{1}, p_{2}) = 0$ if and only if $p_{1} \equiv
p_{2}$.  Otherwise, $K > 0$.

\newpage
\section{Stochastic Processes}

\subsection{Autoregressive (AR) Processes}

\paragraph{AR(1) process}
\selectlanguage{estonian}

Juhuslik muutuja $U$ järgib AR(1) protsessi kui $U$ käesoleva perioodi
realisatsioon on seotud eelmise perioodi omaga
\begin{equation}
  u_t = \varrho u_{t-1} + \varepsilon_t
\end{equation}
ning $\varepsilon_t$ väärtused eri ajaperioodidel on sõltumatud.  Et
protsess oleks stabiilne peab $\varrho$ väärtus jääma vahemikku
$(-1, 1)$. 



\paragraph{AR(2) protsess}

Juhuslik muutuja $U$ järgib AR(2) protsessi kui $U$ käesoleva perioodi
realisatsioon on seotud kahe eelmise perioodi omaga
\begin{equation}
  u_t = \varrho_1 u_{t-1} + \varrho_2 u_{t-2} + \varepsilon_t
\end{equation}
ning $\varepsilon_t$ väärtused eri ajaperioodidel on sõltumatud.


\subsection{Hulkumine}

Definitsioon: hulkumine (\textit{random walk}) on statistiline
protsess
\begin{equation}
z_{t+1} = z_t + \varepsilon_{t+1}.
\end{equation}


\subsubsection{Hulkumine vastu barjääri}
Olgu $z_0=0$ ja $\varepsilon \sim N(0,1)$ \iid protsess.
Siis $z_2$ jaotus tingimusel et $z_1$ es ületa barjääri $\alpha$ on
\begin{equation}
f(z_2|z_1<\alpha) = 
  \frac{1}{\sqrt{2}}
  \frac{\Phi\left(\sqrt{2}\alpha - 
                  \displaystyle\frac{1}{\sqrt{2}}z_2\right)}
    {\Phi(\alpha)}
  \phi \left( \frac{z_2}{\sqrt{2}} \right)
\end{equation}
Tõestus: kirjuta $\phi(x)$ lahti ja integreeri.


\selectlanguage{english}
\clearpage
\section{Dynamic Models}
\label{sec:dynamic-models}

\subsection{Contagion in Random Network}
\label{sec:contagion-random-network}

Look at population of size $N$ where $n$ are infected.
The individuals are meeting at random with Poisson rate $\lambda$.  $p = n/N$
is the probability a random individual in the population is infected.
The
dynamics is governed by
\begin{equation}
  \label{eq:contagion-random-network-process}
  \dif n = \lambda n (1 - p) \,\dif t
\end{equation}
$n$ infected persons meet someone at rate $\lambda$.  With probability
$1-p$ the one they meet was uninfected
and will be infected.

Divide both sides by $N$ to receive
\begin{equation}
  \label{eq:contagion-random-network-probability}
  \dif p = \lambda p (1 - p) \,\dif t
  \quad\text{or}\quad
  \frac{\dif p}{p(1-p)} = \lambda\,\dif t
\end{equation}
Assume initial state at $t = 0$ is $p_{0}$ infection probability.  The
solution is
\begin{equation}
  \label{eq:contagion-random-network-solution}
  p(t) =
  \frac{p_{0}}{p_{0} + (1 - p_{0}) \me^{-\lambda t}}.
\end{equation}
The initial exponential growth with rate $\lambda$ flattens out later:
\begin{figure}[ht]
<<contagionPlot>>=
par(mar=c(3,3,0,0)+0.1,
    mgp=c(2,1,0))
p0 <- 1e-5
lambda <- 0.4
f <- function(t) p0/(p0 + (1 - p0)*exp(-lambda*t))
curve(f, 0, 40, log="y",
      xlab="time", ylab="p(t)")
@ 
\end{figure}



\newpage
\section{Statistical Models}

\subsection{Estimating distribution parameters}
\label{sec:estimating-distribution-parameters}

\subsubsection{Normal distribution}
\label{sec:estimating-normal-distribution}

Take sample $x_{i}, i=0\dots N$.  Estimate normal distribution
parameters $\mu$, $\sigma$.  

Log likelihood:
\begin{equation}
  \label{eq:normal-log-likelihood}
  \loglik(\mu, \sigma) =
  -\frac{N}{2} \log 2\pi
  - N\log \sigma
  - \frac{1}{2} \frac{\sum_{i}(x_{i} - \mu)^{2}}{\sigma^{2}}
\end{equation}
Score:
\begin{align}
  \label{eq:normal-score-likelihood}
  \pderiv[\loglik(\mu, \sigma)]{\mu} &=
                                       \frac{\sum_{i}(x_{i} - \mu)}{\sigma^{2}}
  \\[1.2ex]
  \pderiv[\loglik(\mu, \sigma)]{\sigma} &=
                                          -\frac{N}{\sigma} +
                                          \frac{\sum_{i}(x-\mu)^{2}}{\sigma^{3}}
\end{align}
Hessian:
\begin{align}
  \label{eq:normal-hessian-likelihood}
  \begin{pmatrix}
    -\displaystyle\frac{N}{\sigma^{2}} & 
    -\displaystyle\frac{2\sum_{i}(x_{i} - \mu)}{\sigma^{3}}
    \\[1.2ex]
    -\displaystyle\frac{2\sum_{i}(x_{i} - \mu)}{\sigma^{3}} &
    \displaystyle\frac{N}{\sigma^{2}} - \frac{3\sum_{i}(x_{i} - \mu)^{2}}{\sigma^{4}}
  \end{pmatrix}.
\end{align}
ML estimator:
\begin{equation}
  \label{eq:normal-ml-estimator}
  \hat \mu = \bar x
  \qquad
  \hat \sigma = \sqrt{\frac{\sum_{i}(x - \bar x)^{2}}{N}}
\end{equation}

When calculating the Hessian at ML solution
\begin{align}
  \label{eq:normal-hessian-likelihood}
  H^{*} =
  \begin{pmatrix}
    -\displaystyle\frac{N}{\hat\sigma^{2}} & 0
    \\[1.2ex]
    0 & -2\displaystyle\frac{N}{\hat\sigma^{2}}
  \end{pmatrix}.
\end{align}



\subsection{Linear Regression}
\label{sec:linear-regression}
\index{linear regression|textbf}
Loss function (MSE):
\begin{equation}
  \label{eq:ols-loss-function}
  L(\vec{\beta}) =
  \frac{1}{N}
  (\vec{y} - \mat{X}\,\vec{\beta})^{\transpose} \;
  (\vec{y} - \mat{X}\,\vec{\beta})
\end{equation}
Its gradient:
\index{gradient}
\begin{align}
  \label{eq:ols-gradient-matrix}
  \pderiv{\vec{\beta}}L(\vec{\beta})
  &=
  -\frac{2}{N}
    \mat{X}^{\transpose}(\vec{y} - \mat{X}\,\vec{\beta})
    \qquad\text{matrix form}
  \\
  &=
  -\frac{2}{N} \sum_{i=1}^{N}
    (\vec{y}_{i} - \vec{x}_{i}\,\vec{\beta}) \cdot \vec{x}_{i}
    \qquad\text{vector form}
\end{align}
Predictions:
\begin{equation}
  \label{eq:linear-regression-predictions}
  \hat{\vec{y}} = \mat{P} \, \vec{y} = 
  \mat{X} \, 
  (\mat{X}^{\transpose}\,\mat{X})^{-1} \,
  \mat{X}^{\transpose} \, \vec{y}
\end{equation}
where $\mat{P}=   \mat{X} \; (\mat{X}^{\transpose}\; \mat{X})^{-1} \;
\mat{X}^{\transpose}$ is the \emph{projection matrix}.
\index{linear regression!projection matrix|textbf}
\begin{align}
  \label{eq:linear-regression-residuals}
  \text{Residuals}\quad
  \vec{e} &= \mat{M} \, \vec{y} = 
                  (\mat{I} - \mat{P}) \, \vec{y}
  \\
  \label{eq:linear-regression-mse}
  \mathit{MSE} &= \frac{1}{N} \,
                  \vec{y}^{\transpose}\, \mat{M}^{\transpose}\mat{M} \, \vec{y} 
                  = 
                  \frac{1}{N} \,
                  \vec{y}^{\transpose}\, \mat{M}^{2} \, \vec{y}
\end{align}
\index{MSE}
Gradient in matrix

\subsection{Tobit-2 model}

Definition:
\begin{align}
  y_{1i}^* &= \vec{z}_i' \vec{\gamma} + u_{1i}
  \\
  y_{2i}^* &= \vec{x}_i' \vec{\beta} + u_{2i}
  \\
  y_{1i} &=
  \begin{cases}
    1, \quad \text{if} \quad y_{1i}^* > 0\\
    0, \quad \phantom{\text{if}} y_{1i}^* \le 0.
  \end{cases}
  \\
  y_{2i} &=
  \begin{cases}
    y_{2i}^*, \quad \text{if} \quad y_{1i}^* > 0\\
    0, \quad \phantom{\text{if}} y_{1i}^* \le 0.
  \end{cases}
\end{align}
Assume
\begin{equation}
  \begin{pmatrix}
    U_{1}\\
    U_{2}
  \end{pmatrix}
 \sim 
 N \left( 
   \begin{pmatrix}
     0 \\ 0
   \end{pmatrix},
  \begin{pmatrix}
  1             & \varrho \\
  \varrho       & \sigma^2
  \end{pmatrix}
\right).
\end{equation}

The Heckman two-step estimator in this case is as follows:
$\vec{\gamma}$ can be consistently estimated with probit model.
Further we may write:
\begin{align}
  \E [Y_2|Y_1 > 0, \vec{x}, \vec{z}] &=
  \vec{x}' \beta + \E [U_2|U_1 > -\vec{z}' \gamma]
  = \vec{x}' \beta + \varrho\sigma\lambda(-\vec{z}' \gamma)
  \\
  \var [Y_2|Y_1 > 0, \vec{x}, \vec{z}] &=
  \E [U_2|U_1 > -\vec{z}' \gamma]
  = \sigma^2 + \varrho^2 \sigma^2
  [-\vec{z}' \gamma \lambda(\vec{z}' \gamma)
  - \lambda^2(\vec{z}' \gamma)]
\end{align}
where $\lambda(x) = \dnorm(x) / \pnorm(x)$; $\pnorm(\cdot)$ and
$\dnorm(\cdot)$ are the normal cumulative distribution function and
density function respectively.  $\varrho$ and $\sigma$ can be
estimated regressing $y_{2i}$ on $\vec{x}_i$ and $\lambda(-\vec{z}'
\gamma)$.  From the coefficient of the latter, $\beta_\lambda$ and the
residual variance $s^2$, one can isolate $\varrho$ and $\sigma$:
\begin{align}
  \hat\sigma^2 &=
  s^2 + \beta_\lambda^2 [\lambda^2(\vec{z}' \gamma) - 
  \vec{z}' \gamma \lambda(\vec{z}' \gamma)]
  \\
  \hat\varrho &=
  \frac{\beta_\lambda}{\hat\sigma}.
\end{align}
Note that $\hat\varrho$ need not to be in $[-1,1]$.

Denote:
\begin{align}
  r &=
  \sqrt{1 - \varrho^2}
  \\
  u_{2i} &= y_{2i} - \vec{x}_i'\vec{\beta}
  \\
  B_i &= 
  \frac{\vec{z}_i' \vec{\gamma} +
    \displaystyle \frac{\varrho}{\sigma} u_{2i}} {r}
  \\
  C(B) &= -\frac{\pnorm(B) \dnorm(B) B + \dnorm(B)^2}{\pnorm(B)^2}
\end{align}
The contribution of observation $i$ to the log-likelihood:
\begin{align}
  \loglik & = 
  \sum_{i:y_{1i} \le 0} 
  \log \pnorm(-\vec{z}_i' \vec{\gamma}) +
  \\
  & +
  \sum_{i:y_{1i} > 0} \left[
    \log \pnorm (B_i) 
    -\frac{1}{2} \log 2\pi - \log \sigma -
    \frac{1}{2} \frac{u_{2i}^2}{\sigma^2} \right].
\end{align}
The gradient of the log-likelihood is:
\begin{align}
                                % dl/dgamma
  \frac{\partial \loglik}{\partial \vec{\gamma}} 
  & = 
  \sum_{i:y_{1i} \le 0} 
  -\lambda(\vec{z}_i' \vec{\gamma}) \vec{z}_i
  +
  \sum_{i:y_{1i} > 0} 
  \lambda(B_i) \frac{\vec{z}_i}{r} \\
                                % dl/dbeta
  \frac{\partial \loglik}{\partial \vec{\beta}} 
  & = 
  \sum_{i:y_{1i} > 0} \left[
    \frac{u_{2i}}{ \sigma^2} -
    \lambda(B_i)
    \frac{\varrho}{\sigma} \frac{1}{r} 
  \right]
  \vec{x}_i
  \\
% dl/dsigma
  \frac{\partial l}{\partial \sigma} 
  & = 
  \sum_{i:y_{1i} > 0} 
  \left[
    \frac{ u_{2i}^2}{\sigma^3} -
    \frac{1}{\sigma} -
    \lambda(B_i) \frac{\varrho}{\sigma^2} 
    \frac{u_{2i}}{r} 
  \right]
  \\
                                % dl/dr
  \frac{\partial \loglik}{\partial \varrho} 
  & = 
  \sum_{i:y_{1i} > 0} 
  \lambda(B_i)
  \frac{ \displaystyle \frac{1}{\sigma}
    u_{2i} + \varrho \vec{z}_i' \vec{\gamma}}
  {r^3}.
\end{align}
Hessian components are
\begin{align}
                                % d2l/ dg dg
  \frac{ \partial^2 \loglik}{\partial\vec{\gamma}\vec{\gamma}'}
  &=
  -\sum_{i:y_{1i} = 0} 
  C(-\vec{z}_i'\vec{\gamma})
  \vec{z}_i \vec{z}_i'
  +
  \sum_{i:y_{1i} = 1} \frac{C(B)}{r}
  \vec{z}_i \vec{z}_i'
  \\
% d2l/ dg db
  \frac{ \partial^2 \loglik}
  {\partial\vec{\gamma} \partial\vec{\beta}'} 
  &=
  - \sum_{i:y_{1i} = 1} 
  C(B) \frac{1}{\sigma} \frac{\varrho}{r}
  \vec{z}_i\vec{x}_i' 
  \\
% d2l/ dg ds3
  \frac{ \partial^2 \loglik}{\partial\vec{\gamma} \partial \sigma} 
  &=
  - \sum_{i:y_{1i} = 1}
  C(B)
  \frac{\varrho u_2}{\sigma^2 r^2}
  \vec{z}_i
  \\
                                % d2l/ dg dr
  \frac{ \partial^2 \loglik}{\partial\vec{\gamma} \partial\varrho} 
  &=
  \sum_{i:y_{1i} = 1} \left[
    C(B) \frac
      {\displaystyle \frac{u2}{\sigma} +
        \varrho \vec{z}_i'\vec{\gamma}}
      {r^4}
    + \lambda(B) \frac{\varrho}{r^3}
    \right] 
    \vec{z}_i 
    \\
                                % d2l / db db
    \frac{ \partial^2 \loglik}
    {\partial\vec{\beta} \partial\vec{\beta}'} 
    &=
    \sum_{i:y_{1i} = 1} 
    \frac{1}{\sigma^2} \left[
      \frac{\varrho^2}{r^2} C(B) - 1 \right]
    \vec{x}_i\vec{x}_i' 
    \\
                                % d2l / db ds
    \frac{ \partial^2 \loglik}
    {\partial\vec{\beta} \partial\sigma} 
    &=
    \sum_{i:y_{1i} = 1} \left[
      C(B) \frac{\varrho^2}{\sigma^3}
      \frac{u_2}{r^2}
      + \frac{\varrho}{\sigma^2}
      \frac{\lambda(B)}{r}
      - 2\frac{u_2}{\sigma^3}
    \right] 
    \vec{x}_i \\
                                % d2l / db dr
    \frac{ \partial^2 \loglik}
    {\partial\vec{\beta} \partial\varrho} 
    &=
    \sum_{i:y_{1i} = 1} \left[
      -C(B) \frac
      {\displaystyle \frac{u_2}{\sigma} +
        \varrho\vec{z}_i'\vec{\gamma}}{r^4}
      \frac{\varrho}{\sigma}
      - \frac{\lambda(B)}{\sigma}
      \frac{1}{r^3}
    \right] 
    \vec{x}_i
    \\
                                % d2l/ ds ds
    \frac{ \partial^2 \loglik}
    {\partial \sigma_3^2} 
    &=
    \sum_{i:y_{1i} = 1} \left[
      \frac{1}{\sigma^2}
      - 3 \frac{ u_2^2}{\sigma^4}
      + 2 \lambda( B)
      \frac{u2}{r}
      \frac{\varrho}{\sigma^3}
      + \frac{\varrho^2}{\sigma^4}
      \frac{ u_2^2}{r^2}
      C(B) 
    \right]
    \\
                                % d2l/ ds_3 dr_3
    \frac{ \partial^2 \loglik}
    {\partial \sigma \partial\varrho} 
    &=
    -\frac{1}{r^3}
    \sum_{i:y_{1i} = 1}  
    \frac{u_2}{\sigma^2} \left[
      C(B) \frac
      {\varrho \left(
          \displaystyle \frac{u_2}{\sigma} +
          \varrho \vec{z}_i'\vec{\gamma} \right)}
      {r}
      + \lambda( B)
    \right]
    \\
                                % d2l/dr dr
    \frac{ \partial^2 \loglik}{\partial \varrho^2} 
    &=
    \sum_{i:y_{1i} = 1}
    \left[
      C(B) \left(
        \frac
        {\displaystyle \frac{u_2}{\sigma}
          + \varrho \vec{z}_i'\vec{\gamma}}
        {r^3}
      \right)^2 + 
      \lambda( B)
      \frac
      {\vec{z}_i'\vec{\gamma}( 1 + 2 \varrho^2) +
        3 \varrho \displaystyle \frac{u_2}{\sigma}}
      {r^5}
    \right]
\end{align}


\subsection{Tobit-2 Model with Binary Outcome}
\label{sec:tobit2B}

The underlying latent model:
\begin{align}
  y_{i}^{S*} &= {\vec{\beta}^{S}}'\vec{x}_i^{S} + \epsilon_{i}^{S}
  \\
  y_{i}^{O*} &= {\vec{\beta}^{O}}'\vec{x}_i^{O} + \epsilon_{i}^{0}
  \\
  y_{i}^{S} &=
  \begin{cases}
    1, \quad \text{if} \quad y_{i}^{S*} > 0\\
    0, \quad \phantom{\text{if}} y_{i}^{S*} \le 0.
  \end{cases}
  \\
  y_{i}^{O} &=
  \begin{cases}
    \text{undetermined}, &\text{if} \quad y_{i}^{S} = 0
       \qquad\text{(case 1)}\\
    0, &\text{if} \quad y_{i}^{O*} \le 0 
       \quad \text{and} \quad y_{i}^{S} = 1
       \quad\text{(case 2)}\\
    1, &\text{if} \quad y_{i}^{O*} > 0 
       \quad \text{and} \quad y_{i}^{S} = 1
       \quad\text{(case 3)}
  \end{cases}
  \label{eq:tobit2Bobservables}
\end{align}
Assume
\begin{equation}
  \begin{pmatrix}
    \epsilon_{1}\\
    \epsilon_{2}
  \end{pmatrix}
 \sim 
 N \left( 
   \begin{pmatrix}
     0 \\ 0
   \end{pmatrix},
  \begin{pmatrix}
  1             & \varrho \\
  \varrho       & 1
  \end{pmatrix}
\right).
\end{equation}

The log-likelihood function contains 3 components, corresponding to
the cases in~\eqref{eq:tobit2Bobservables}:
\begin{align}
  \label{eq:tobit2Bloglik}
  \loglik =
  &\sum_{i \in \text{case 1}}
  \log \Phi \left( 
    -{\vec{\beta}^{S}}'\vec{x}_i^{S}
  \right)\\
  + 
  &\sum_{i \in \text{case 2}} \log\left[
    1 - \Phi \left( 
      -{\vec{\beta}^{S}}'\vec{x}_i^{S}
    \right)
    - \bar \Phi_{2} \left(
      \begin{pmatrix}
        -{\vec{\beta}^{S}}'\vec{x}_i^{S}\\
        -{\vec{\beta}^{O}}'\vec{x}_i^{O}
      \end{pmatrix}
      ,
      \begin{pmatrix}
        1             & \varrho \\
        \varrho       & 1
      \end{pmatrix}
    \right)
  \right]\\
  + 
  &\sum_{i \in \text{case 3}}
  \log\bar \Phi_{2} \left(
      \begin{pmatrix}
        -{\vec{\beta}^{S}}'\vec{x}_i^{S}\\
        -{\vec{\beta}^{O}}'\vec{x}_i^{O}
      \end{pmatrix}
      ,
      \begin{pmatrix}
        1             & \varrho \\
        \varrho       & 1
      \end{pmatrix}
    \right),
\end{align}
where $\bar \Phi_{2}(\cdot,\cdot)$ is the upper tail probability of
2-dimensional normal distribution.

Denote by $\likelihood_{i}$ the corresponding individual likelihood
value.  The score vector:
\begin{align*}
  \pderiv{\vec{\beta}^{S}}\loglik =
  &\sum_{i \in \text{case 1}}
  \frac{1}{\likelihood_{i}}
  \phi \left( 
    -{\vec{\beta}^{S}}'\vec{x}_i^{S}
  \right)
  \vec{x}_{i}^{S}
  \\ + 
  &\sum_{i \in \text{case 2}} 
  \frac{1}{\likelihood_{i}}
    \phi \left( 
      {\vec{\beta}^{S}}'\vec{x}_i^{S}
    \right)
    \bar \Phi \left(
      \frac{
        {\vec{\beta}^{O}}'\vec{x}_i^{O}
        - \varrho{\vec{\beta}^{S}}'\vec{x}_i^{S}
      }
      {\sqrt{1 - \varrho^{2}}}
    \right)
    \vec{x}_{i}^{S}
    \\ + 
  &\sum_{i \in \text{case 3}} 
  \frac{1}{\likelihood_{i}}
    \phi \left( 
      {\vec{\beta}^{S}}'\vec{x}_i^{S}
    \right)
    \Phi \left(
      \frac{
        {\vec{\beta}^{O}}'\vec{x}_i^{O}
        - \varrho{\vec{\beta}^{S}}'\vec{x}_i^{S}
      }
      {\sqrt{1 - \varrho^{2}}}
    \right)
    \vec{x}_{i}^{S}
    \\
    % d/d betaO
    \pderiv{\vec{\beta}^{O}}\loglik =
    &\sum_{i \in \text{case 2}} 
    \frac{1}{\likelihood_{i}}
    \phi \left( 
      {\vec{\beta}^{O}}'\vec{x}_i^{O}
    \right)
    \bar \Phi \left(
      \frac{
        {\vec{\beta}^{S}}'\vec{x}_i^{S}
        - \varrho{\vec{\beta}^{O}}'\vec{x}_i^{O}
      }
      {\sqrt{1 - \varrho^{2}}}
    \right)
    \vec{x}_{i}^{O}
    \\ + 
    &\sum_{i \in \text{case 3}} 
    \frac{1}{\likelihood_{i}}
    \phi \left( 
      {\vec{\beta}^{O}}'\vec{x}_i^{O}
    \right)
    \Phi \left(
      \frac{
        {\vec{\beta}^{S}}'\vec{x}_i^{S}
        - \varrho{\vec{\beta}^{O}}'\vec{x}_i^{O}
      }
      {\sqrt{1 - \varrho^{2}}}
    \right)
    \vec{x}_{i}^{O}
    \\
    % d/d rho
    \pderiv{\varrho}\loglik =
    &-\sum_{i \in \text{case 2}} 
    \phi_{2} \left(
      \begin{pmatrix}
        -{\vec{\beta}^{S}}'\vec{x}_i^{S}\\
        -{\vec{\beta}^{O}}'\vec{x}_i^{O}
      \end{pmatrix}
      ,
      \begin{pmatrix}
        1             & \varrho \\
        \varrho       & 1
      \end{pmatrix}
    \right)
    + \sum_{i \in \text{case 3}} 
    \phi_{2} \left(
      \begin{pmatrix}
        -{\vec{\beta}^{S}}'\vec{x}_i^{S}\\
        -{\vec{\beta}^{O}}'\vec{x}_i^{O}
      \end{pmatrix}
      ,
      \begin{pmatrix}
        1             & \varrho \\
        \varrho       & 1
      \end{pmatrix}
    \right),
\end{align*}
where $\phi_{2}(\cdot,\cdot)$ is 2-dimensional normal density.



\clearpage

\subsection{Tobit-5 Model}

\selectlanguage{estonian}
Definitsioon (lühiduse mõttes on indeks $i$ ära jäetud):
\begin{eqnarray}
y_1^* &=& \vec{Z}'\vec{\gamma} + u_1\\
y_2^* &=& \vec{X}'\vec{\beta_2} + u_2\\
y_3^* &=& \vec{X}'\vec{\beta_3} + u_3\\
y_2 &=& \left\{
 \begin{array}{c@{\quad}l@{\quad}l}
 y_2^*  & \mathrm{kui}  & y_1^* \le 0\\
 0      &               & y_1^* > 0
 \end{array} \right.\\
y_3 &=& \left\{
 \begin{array}{c@{\quad}l@{\quad}l}
 y_3^*  & \mathrm{kui}  & y_1^* > 0\\
 0      &               & y_1^* \le 0
 \end{array}\right.
\end{eqnarray}
Eeldatakse et jääkliikmete jaotus on niisugune:
\begin{equation} \left(
 \begin{array}{c}
 u_1\\
 u_2\\
 u_3
 \end{array} \right) \sim N \left(
 \left( \begin{array}{c}
 0 \\
 0 \\
 0
 \end{array} \right),
 \left( \begin{array}{ccc}
 1                      & \varrho_2\sigma_2     & \varrho_3\sigma_3\\
 \varrho_2\sigma_2      & \sigma^2_2            & \sigma_{23}\\
 \varrho_3\sigma_3      & \sigma_{23}           & \sigma^2_3
 \end{array} \right) \right).
\end{equation}


\subsubsection{Heckmani kahesammuline hinnang}

$\hat\gamma$ leitakse probiti abil. Edasi võib kirjutada
\begin{eqnarray}
y_2 &=& \vec{X}'\vec{\beta_2} - \varrho_2\sigma_2 \lambda(
  -\vec{Z}'\vec{\gamma}) + e_2 \nonumber\\
y_3 &=& \vec{X}'\vec{\beta_3} + \varrho_3\sigma_3 \lambda(
  \vec{Z}'\vec{\gamma}) + e_3
\label{eq:tobit5 H2s}
\end{eqnarray}
Kusjuures
\begin{eqnarray}
\sigma_{e2} &=& \sigma_2^2 \left\{
  1 - \varrho_2^2 \left[
    \lambda^2( -\vec{Z}'\vec{\gamma}) -
    \vec{Z}'\vec{\gamma} \lambda( -\vec{Z}'\vec{\gamma}) \right]
  \right\} \nonumber\\
\sigma_{e3} &=& \sigma_3^2 \left\{
  1 - \varrho_3^2 \left[
    \lambda^2( \vec{Z}'\vec{\gamma}) +
    \vec{Z}'\vec{\gamma} \lambda( -\vec{Z}'\vec{\gamma}) \right] \right\}
\end{eqnarray}
Kui lähendada seost~(\ref{eq:tobit5 H2s}) OLS-ga, siis saab $\lambda$
koefitsendi ja dispersiooni hinnangu abil leida $\hat\varrho$ ja
$\hat\sigma$. Märkus: $\hat\varrho$ ei pruugi olla -1 ja 1 vahel.


\subsubsection{Maksimum-laiklikhuud hinnang}

Mudeli log-laiklihuud on:
\begin{eqnarray}
l &=& -\frac{N}{2}\log 2\pi + \nonumber\\
&+& \sum_{i \in \text{case 2}} \left\{
  -\log\sigma_2
  -\frac{1}{2}\left( \frac{u_2}{\sigma_2} \right)^2
  +\log \pnorm \left[ -\frac
    {\vec{Z}'\vec{\gamma} + \displaystyle \frac{\varrho_2}{\sigma_2}
      \left( y_2 - \vec{X}_i'\vec{\beta}_2 \right)}
    {\sqrt{ 1 - \varrho_2^2}} \right] \right\} \nonumber\\
&+& \sum_{i \in \text{case 3}} \left\{
  -\log\sigma_3
  -\frac{1}{2}\left( \frac{y_3 - \vec{X\beta}_3}{\sigma_3} \right)^2
  +\log \pnorm \left[ \frac
    {\vec{Z'}\vec{\gamma} + \displaystyle \frac{\varrho_3}{\sigma_3}
      \left( y_3 - \vec{X}_i'\vec{\beta}_3 \right)}
    {\sqrt{ 1 - \varrho_3^2}} \right] \right\}. \nonumber\\
&&
\end{eqnarray}
Valikud 2 ja 3 erinevad ainult avaldise märgi poolest funktsiooni
$\pnorm$ sees. Tuletised on:
\begin{eqnarray}
%dl/dg
\frac{\partial l}{\partial \gamma} & = &
  -\sum_2 \frac{\dnorm(B_2)}{\pnorm(B_2)}
    \frac{\vec{Z}}{\sqrt{1-\varrho_2^2}}
  +\sum_3 \frac{\dnorm(B_3)}{\pnorm(B_3)}
    \frac{\vec{Z}}{\sqrt{1-\varrho_3^2}} \\
% dl/db2
\frac{\partial l}{\partial \vec{\beta}_2} & = &
  \sum_2 \left[
    \frac{\dnorm(B_2)}{\pnorm(B_2)} \left(
      \frac{\varrho_2}{\sigma_2}
      \frac{\vec{X}}{\sqrt{1-\varrho_2^2}} \right)
    +\frac{u_2}{\sigma_2^2}\vec{X}
    \right] \\
% dl/dsigma2
\frac{\partial l}{\partial \sigma_2} & = &
  \sum_2 \left[
    -\frac{1}{\sigma_2}
    +\frac{ \left( y_2 - \vec{X}'\vec{\beta}_2 \right)^2}
      {\sigma_2^3}
    +\frac{\dnorm(B_2)}{\pnorm(B_2)}
    \frac{\varrho_2}{\sigma_2^2}
    \frac{y_2 - \vec{X}'\vec{\beta}_2}{\sqrt{1-\varrho_2^2}}
    \right] \\
% dl/droo2
\frac{\partial l}{\partial \varrho_2} & = &
  -\sum_2
    \frac{\dnorm(B_2)}{\pnorm(B_2)}
    \frac{ \displaystyle \frac{1}{\sigma_2}
        ( y_2 - \vec{X}'\vec{\beta}_2) + \varrho_2 \vec{Z}'\vec{\gamma}}
      {( 1 - \varrho_2^2)^{\frac{3}{2}}}\\
% dl/db_3
\frac{\partial l}{\partial \vec{\beta}_3} & = &
  \sum_3 \left[
    -\frac{\dnorm(B_3)}{\pnorm(B_3)} \left(
      \frac{\varrho_3}{\sigma_3}
      \frac{\vec{X}}{\sqrt{1-\varrho_3^2}} \right)
    +\frac{u_3}{\sigma_3^2}\vec{X}
    \right] \\
% dl/dsigma_3
\frac{\partial l}{\partial \sigma_3} & = &
  \sum_3 \left[
    -\frac{1}{\sigma_3}
    +\frac{ \left( y_3 - \vec{X}'\vec{\beta}_3 \right)^2}
      {\sigma_3^3}
    -\frac{\dnorm(B_3)}{\pnorm(B_3)}
    \frac{\varrho_3}{\sigma_3^2}
    \frac{y_3 - \vec{X}'\vec{\beta}_3}{\sqrt{1-\varrho_3^2}}
    \right] \\
% dl/droo_3
\frac{\partial l}{\partial \varrho_3} & = &
  \sum_3
    \frac{\dnorm(B_3)}{\pnorm(B_3)}
    \frac{ \displaystyle \frac{1}{\sigma_3}
        ( y_3 - \vec{X}'\vec{\beta}_3) + \varrho_3 \vec{Z}'\vec{\gamma}}
      {( 1 - \varrho_3^2)^{\frac{3}{2}}}
\end{eqnarray}
Teised tuletised:
\begin{eqnarray}
% d2l/ dg dg
\frac{ \partial^2 l}{\partial\vec{\gamma}^2} &=&
  \sum_2 \frac{C(B_2)}{1-\varrho_2^2}\vec{z}_i \vec{z}_i'
  + \sum_3 \frac{C(B_3)}{1-\varrho_3^2}\vec{z}_i \vec{z}_i'\\
% d2l/ dg db2
\frac{ \partial^2 l}{\partial\vec{\gamma} \partial\vec{\beta}_2'} &=&
  - \sum_2 C(B_2) \frac{1}{\sigma_2} \frac{\varrho_2}{1-\varrho_2^2}
  \vec{Z}\vec{X}' \\
% d2l/ dg ds2
\frac{ \partial^2 l}{\partial\vec{\gamma} \partial \sigma_2} &=&
  - \sum_2
    \frac{\varrho_2 u_2}{\sigma_2^2 ( 1- \varrho_2^2)}
    C(B_2) \vec{Z} \\
% d2l/ dg dr2
\frac{ \partial^2 l}{\partial\vec{\gamma} \partial \varrho_2} &=&
  \sum_2 \left[
    C(B_2) \frac
      {\displaystyle \frac{u_2}{\sigma_2}
        \varrho_2 \vec{Z}'\vec{\gamma}}
      {( 1 - \varrho_2^2)^2}
    - \lambda(B_2) \frac{\varrho_2}
      {(1 - \varrho_2^2)^\frac{3}{2}}
    \right] \vec{Z} \\
% d2l/ dg db3
\frac{ \partial^2 l}{\partial\vec{\gamma} \partial\vec{\beta}_3'} &=&
  - \sum_3 C(B_3) \frac{1}{\sigma_3} \frac{\varrho_3}{1-\varrho_3^2}
  \vec{Z}\vec{X}' \\
% d2l/ dg ds3
\frac{ \partial^2 l}{\partial\vec{\gamma} \partial \sigma_3} &=&
  - \sum_3
    \frac{\varrho_3 u_3}{\sigma_3^2 ( 1- \varrho_3^2)}
    C(B_3) \vec{Z} \\
% d2l/ dg dr_3
\frac{ \partial^2 l}{\partial\vec{\gamma} \partial \varrho_3} &=&
  \sum_3 \left[
    C(B_3) \frac
      {\displaystyle \frac{u_3}{\sigma_3}
        \varrho_3 \vec{Z}'\vec{\gamma}}
      {( 1 - \varrho_3^2)^2}
    + \lambda(B_3) \frac{\varrho_3}
      {(1 - \varrho_3^2)^\frac{3}{2}}
    \right] \vec{Z} \\
% d2l / db_2 db_2
\frac{ \partial^2 l}{\partial\vec{\beta}_2 \partial\vec{\beta}_2'} &=&
  \sum_2 \frac{1}{\sigma_2^2} \left[
  \frac{\varrho_2^2}{1 - \varrho_2^2}C(B_2) - 1 \right]
  \vec{X}\vec{X}' \\
% d2l / db2 ds2
\frac{ \partial^2 l}{\partial\vec{\beta}_2 \partial\sigma_2} &=&
  \sum_2 \left[
    C(B_2) \frac{u_2}{\sigma_2^3}
      \frac{\varrho_2^2}{1 - \varrho_2^2}
    - \frac{\lambda(B_2)}{\sigma_2^2}
      \frac{\varrho_2}{\sqrt{1 - \varrho_2^2}}
    - 2\frac{u_2}{\sigma_2^3}
    \right] \vec{X} \\
% d2l / db_2 dr_2
\frac{ \partial^2 l}{\partial\vec{\beta}_2 \partial\varrho_2} &=&
  \sum_2 \left[
    -C(B_2) \frac
      {\displaystyle \frac{u_2}{\sigma_2} +
        \varrho_2\vec{Z}'\vec{\gamma}}
      {(1 - \varrho_2^2)^2}
      \frac{\varrho_2}{\sigma_2}
    + \frac{\lambda(B_2)}{\sigma_2}
      \frac{1}{(1 - \varrho_2^2)^\frac{3}{2}}
    \right] \vec{X}\\
% d2l / db2 db3
\frac{ \partial^2 l}{\partial\vec{\beta}_2 \partial\vec{\beta}_3}
  &=& 0\\
% d2l / db2 ds3
\frac{ \partial^2 l}{\partial\vec{\beta}_2 \partial\sigma_3}
  &=& 0 \\
% d2l / db2 dr3
\frac{ \partial^2 l}{\partial\vec{\beta}_2 \partial\varrho_3}
  &=& 0 \\
% d2l/ ds_2 ds_2
\frac{ \partial^2 l}{\partial \sigma_2^2} &=&
  \sum_2 \left[
    \frac{1}{\sigma_2^2}
    - 3 \frac{ u_2^2}{\sigma_2^4}
    + \frac{u_2}{\sigma_2^4}
      \frac{\varrho_2^2}{1 - \varrho_2^2}
      C(B_2) \right] - \nonumber\\
&-&   2 \sum_2
      \lambda( B_2)
      \frac{u_2}{\sigma_2^3}
      \frac{\varrho_2}{\sqrt{1- \varrho_2^2}} \\
% d2l/ ds2 dr2
\frac{ \partial^2 l}{\partial \sigma_2 \partial\varrho_2} &=&
  \frac{1}{(1 - \varrho_2^2)^\frac{3}{2}}
  \sum_2  \frac{u_2}{\sigma_2^2} \left[
    -C(B_2) \frac
      {\varrho_2 \left(
      \displaystyle \frac{u_2}{\sigma_2} +
        \varrho_2 \vec{Z}'\vec{\gamma} \right)}
      {\sqrt{1-\varrho_2^2}}
    + \lambda( B_2)
    \right]\\
% d2l / ds2 db3
\frac{ \partial^2 l}{\partial\sigma_2 \partial\vec{\beta}_3}
  &=& 0\\
% d2l / ds2 ds3
\frac{ \partial^2 l}{\partial\sigma_2 \partial\sigma_3}
  &=& 0 \\
% d2l / ds2 dr3
\frac{ \partial^2 l}{\partial\sigma_2 \partial\varrho_3}
  &=& 0 \\
% d2l/dr_2 dr_2
\frac{ \partial^2 l}{\partial \varrho_2^2} &=&
  \sum_2
    C(B_2) \left[ \frac
      {\displaystyle \frac{u_2}{\sigma_2}
        + \varrho_2 \vec{Z}'\vec{\gamma}}
      {( 1 - \varrho_2^2)^\frac{3}{2}}
      \right]^2 - \nonumber\\
&&  - \sum_2 \frac{\dnorm(B_2)}{\pnorm(B_2)}
      \frac
        {\vec{Z}'\vec{\gamma}( 1 + 2 \varrho_2^2) +
          3 \varrho_2 \displaystyle \frac{u_2}{\sigma_2}}
        {(1-\varrho_2^2)^\frac{5}{2}}\\
% d2l / dr2 db3
\frac{ \partial^2 l}{\partial\varrho_2 \partial\vec{\beta}_3}
  &=& 0\\
% d2l / dr2 ds3
\frac{ \partial^2 l}{\partial\varrho_2 \partial\sigma_3}
  &=& 0 \\
% d2l / dr2 dr3
\frac{ \partial^2 l}{\partial\varrho_2 \partial\varrho_3}
  &=& 0 \\
% d2l / db_3 db_3
\frac{ \partial^2 l}{\partial\vec{\beta}_3 \partial\vec{\beta}_3'} &=&
  \sum_3 \frac{1}{\sigma_3^2} \left[
  \frac{\varrho_3^2}{1 - \varrho_3^2}C(B_3) - 1 \right]
  \vec{X}\vec{X}' \\
% d2l / db_3 ds_3
\frac{ \partial^2 l}{\partial\vec{\beta}_3 \partial\sigma_3} &=&
  \sum_3 \left[
    C(B_3) \frac{\varrho_3^2}{\sigma_3^3}
      \frac{u_3}{1 - \varrho_3^2}
    + \frac{\varrho_3}{\sigma_3^2}
      \frac{\lambda(B_3)}{\sqrt{1 - \varrho_3^2}}
    - 2\frac{u_3}{\sigma_3^3}
    \right] \vec{X} \\
% d2l / db_3 dr_3
\frac{ \partial^2 l}{\partial\vec{\beta}_3 \partial\varrho_3} &=&
  \sum_3 \left[
    -C(B_3) \frac
      {\displaystyle \frac{u_3}{\sigma_3} +
        \varrho_3\vec{Z}'\vec{\gamma}}
      {(1 - \varrho_3^2)^2}
      \frac{\varrho_3}{\sigma_3}
    - \frac{\lambda(B_3)}{\sigma_3}
      \frac{1}{(1 - \varrho_3^2)^\frac{3}{2}}
    \right] \vec{X}\\
% d2l/ ds_3 ds_3
\frac{ \partial^2 l}{\partial \sigma_3^2} &=&
  \sum_3 \left[
    \frac{1}{\sigma_3^2}
    - 3 \frac{ u_3^2}{\sigma_3^4}
    + 2 \lambda( B_3)
      \frac{y_3 - \vec{X}'\vec{\beta_3}}{\sqrt{1- \varrho_3^2}}
      \frac{\varrho_3}{\sigma_3^3}
    \right] + \nonumber\\
&&  + \sum_3
      \frac{\varrho_3^2}{\sigma_3^4}
      \frac{ u_3^2}{1 - \varrho_3^2}
      C(B_3) \\
% d2l/ ds_3 dr_3
\frac{ \partial^2 l}{\partial \sigma_3 \partial\varrho_3} &=&
  -\frac{1}{(1 - \varrho_3^2)^\frac{3}{2}}
  \sum_3  \frac{u_3}{\sigma_3^2} \left[
    C(B_3) \frac
      {\varrho_3 \left(
      \displaystyle \frac{u_3}{\sigma_3} +
        \varrho_3 \vec{Z}'\vec{\gamma} \right)}
      {\sqrt{1-\varrho_3^2}}
    + \lambda( B_3)
    \right]\\
% d2l/dr_3 dr_3
\frac{ \partial^2 l}{\partial \varrho_3^2} &=&
  \sum_3
    C(B_3) \left[ \frac
      {\displaystyle \frac{1}{\sigma_3}u_3
        + \varrho_3 \vec{Z}'\vec{\gamma}}
      {( 1 - \varrho_3^2)^\frac{3}{2}}
      \right]^2 + \nonumber\\
&&  + \sum_3 \lambda( B_3)
      \frac
        {\vec{Z}'\vec{\gamma}( 1 + 2 \varrho_3^2) +
          3 \varrho_3 \displaystyle \frac{1}{\sigma_3} u_3}
        {(1-\varrho_3^2)^\frac{7}{2}}
%
\end{eqnarray}
Siin on tähistatud
\begin{eqnarray}
B_2 &=& -\frac
    {\vec{Z}'\vec{\gamma} + \displaystyle \frac{\varrho_2}{\sigma_2}
      \left( y_2 - \vec{X}'\vec{\beta_2} \right)}
    {\sqrt{ 1 - \varrho_2^2}} \\
% B_3
B_3 &=& \frac
    {\vec{Z}'\vec{\gamma} + \displaystyle \frac{\varrho_3}{\sigma_3}
      \left( y_3 - \vec{X}'\vec{\beta_3} \right)}
    {\sqrt{ 1 - \varrho_3^2}}\\
% lambda
\lambda( B) &=& \frac{\dnorm(B)}{\pnorm(B)}\\
% u_2
u_2 &=& y_2 - \vec{X}'\vec{\beta_2}\\
u_3 &=& y_3 - \vec{X}'\vec{\beta_3}\\
% C
C(B) &=& -\frac{\pnorm(B) \dnorm(B) B + \dnorm(B)^2}{\pnorm(B)^2}
\end{eqnarray}


\newpage
\subsection{Kestusmudelid}

Tähistused:
\begin{description}
\item[$\tau$] kestus, algseisundis viibitud aeg
\item[$t$] kalendriaeg
\end{description}


\subsubsection{Kaplan-Meieri hinnang}

\paragraph{KM hinnang diskreetses ajas}
Olgu perioodil $j$ $r_j$ inimest ``riski hulgas'', s.t. $r_j$ inimest
võiksid põhimõtteliselt seisundist lahkuda.  Lahkugu tegelikult $n_j$
inimest, $r_j - n_j$ jäävad edasi algseisundisse.  KM hinnang
hasardile on seega
\begin{align}
  \label{eq:KM_hinnang}
  \hat h &= \frac{n_j}{r_j}\\
  \widehat{\var\hat h_j} &= \frac{\hat h_j(1 - \hat h_j)}{r_j}.
\end{align}
Kui periood $j$ on $k$ kuu pikkune, siis (keskmise) ühe kuu
spetsiifilise hasardi saab
\begin{align}
  \hat\vartheta_j &= 1 - (1 - \hat h_j)^{1/k}\\
                                %
  \widehat{\var\hat\vartheta_j} &=
  \frac{\var\hat h_j}
  {\left[ k(1 - \hat h_j)^{1 - 1/k} \right]^2}
\end{align}

\paragraph{KM hinnang pidevas ajas}
Lahkugu aja $t$ jooksul $r$ algseisundis olnud inimesest $n$.
Keskmine hasart ajaühikus on
\begin{align}
  \hat\vartheta &= -\frac{1}{t} \log(1 - n/r)\\
                                %
  \widehat{\var\hat\vartheta} &=
  \frac{1}{t^2} 
  \frac{n/r}{r - n}.
\end{align}




\subsubsection{Multiplikatiivne mittevaadeldav heterogeensus}

Eeldame et hasart avaldub
\begin{equation}
  \vartheta(\tau|x, v) = \lambda(\tau|x) v
\end{equation}
kus $v$ on mingi kindla jaotusega mittevaadeldav juhuslik suurus.
Nüüd $v$ keskväärtus algseisundisse jääjatel sõltub ajast:
\begin{equation}
  \E (v|T \ge \tau) =
  - \frac{\laplace'[z(\tau|x)]}{\laplace[z(\tau|x)]}
\end{equation}
kus on integreeritud hasart $v$-d arvestamata:
\begin{equation}
  z(\tau|x) = \int_0^\tau \lambda(s|x) \,\dif s
\end{equation}
Kui $v$ on algseisundisse sissevoolus ühikdispersiooniga gammajaotus
parameetriga $\alpha$, siis
\begin{equation}
  \E (v|T \ge \tau) =
  \frac{\alpha}{z(\tau|x) + \alpha^{1/2}}.
\end{equation}



\subsubsection[Tükati konstantne hasart]{Tükati konstantne põhihasart ja diskreetne
mittevaadeldav heterogeensus ning pidev aeg}
\label{sec:MPH_pcw}

\paragraph{Sõltumatud vaatlused}
Eeldatakse, et hasart on konstantne iga $M$ ajavahemiku sees,
erinevatel ajavahemikel võib ta aga olla erinev.  Olgu hasart
kirjeldatud vektoriga $\vec{\lambda}$, kusjuures ajavahemiku $j$
põhihasart olgu $e^{\lambda_j}$.  Mittevaadeldav heterogeensus on
diskreetse jaotusega:
\begin{equation}
v = \begin{cases}
  \begin{array}{ll}
  v_h \equiv 1,\quad &\mbox{tõenäosusega}\quad p_h,\\
  v_l,               &\mbox{tõenäosusega}\quad p_l = 1 - p_h.
  \end{array}
\end{cases}
\end{equation}
Sobiv on parametriseerida
\begin{equation}
  \begin{array}{lcl}
    v_1 & = & \me^{\tilde v_1}\\
    \ldots\\
    v_K & = & 1
  \end{array} 
  \qquad \mbox{ja} \qquad
  \begin{array}{lcl}
    p_1 & = & \Lambda(\tilde p_1)\\
    \ldots\\
    p_K & = & 1 - \sum^{K-1} p_k,
  \end{array}
\end{equation}
Kus $\Lambda(\cdot)$ on logistile jaotusfunktsioon ja $v_k \in \Re$
ning $p_k \in \Re$.

Olgu $m_i$ ajaperiood, mille jooksul inimene lahkub uuritavast
seisundist ja tsenseerimist kirjeldagu $\delta_i = 0$ kui vaatlus on
tsenseeritud ja $1$ kui tsenseerimata ning $\mu_i = \me^{\vec{\gamma}
\vec{x}_i'}$ olgu hasardi inimesest sõltuv osa.  $T_{ij}$ olgu
teadaolev (võimalik et tsenseeritud) aeg, mis inimene $i$ veetis
uuritavas seisundis ajaperioodi $j$ jooksul.  Vektor $\vec{T}_i$ on
$M$-vektor, mille komponendid on $T_{ij}$ ning vector $\vec{d}_i$ on
vektor, mille $j$-s komponent on 1, kui inimene lahkus uuritavast
seisundist ajaperioodil $j$.  Muud komponendid on nullid.

Sel juhul inimese $i$ laiklihuud avaldub:
\begin{multline}
\likelihood_i = p_l \mathcal \likelihood_{li} + p_h \mathcal \likelihood_{hi} = 
%
p_l \left( v_l \mu_i \me^{\lambda_{m_i}} \right)^{\delta_i}
\me^{-z_{li}} +
p_h \left( \mu_i \me^{\lambda_{m_i}} \right)^{\delta_i}
\me^{-z_{hi}},
\end{multline}
kus
\begin{equation}
z_{li} = v_l \mu_i \sum_{j=1}^{M-1} \me^{\lambda_j} T_{ij}
\qquad\mbox{ja}\qquad
z_{hi} = \mu_i \sum_{j=1}^{M-1} \me^{\lambda_j} T_{ij}
\end{equation}
on integreeritud hasart.
Log-laiklihuudi gradient avaldub:
\begin{eqnarray}
% d l / d v_l
\frac{\partial \loglik_i}{\partial v_l} & = &
\frac{p_l \likelihood_{li}}{\likelihood} \left[
  \frac{\delta_i}{v_l} - z_{hi}
  \right] \\
% d l / d p_l
\frac{\partial \loglik_i}{\partial p_l} & = &
\frac{1}{\likelihood_i} \left[ \likelihood_{li} - \likelihood_{hi} \right]\\
% d l / d lambda
\frac{\partial \loglik_i}{\partial \vec{\lambda}} & = &
p_l \frac{\likelihood_{li}}{\likelihood_i} \left(
  \delta_i \vec{d}_i - v_l \mu_i \vec{T}_i \right) +
p_h \frac{\likelihood_{hi}}{\likelihood_i} \left(
  \delta_i \vec{d}_i - \mu_i \vec{T}_i \right)\\
% d l / d gamma
\frac{\partial \loglik_i}{\partial \vec{\gamma}} & = &
\frac{\vec{x_i}}{\likelihood_i} \left[
  p_l \likelihood_l \left( \delta_i - z_{li} \right) +
  p_h \likelihood_h \left( \delta_i - z_{hi} \right) \right]\\
\end{eqnarray}
ja hessi maatriks:
\begin{eqnarray}
% d2 l / d v_l^2
\frac{\partial^2 \loglik_i}{\partial v_l^2} &=&
\frac{\likelihood_{li}}{\likelihood_i} \left[
  \left( \frac{\delta_i}{v_l} - z_{hi} \right)^2
  \left( 1 - p_l \frac{\likelihood_{li}}{\likelihood_i} \right)
  - \frac{\delta_i}{v_l^2} \right]\\
% d2 l / d p_l^2
\frac{\partial^2 \loglik_i}{\partial p_l^2} &=&
- \left( \frac{\likelihood_{li}}{\likelihood_i} - \frac{\likelihood_{hi}}{\likelihood_i} \right)^2\\
% d2 l / d v_l d p_l
\frac{\partial^2 \loglik_i}{\partial v_l \partial p_l} &=&
\frac{\likelihood_{li}}{\likelihood_i}
  \left( \frac{\delta_i}{v_l} - z \right)
  \left( 1 - p_l \frac{\likelihood_{li} - \likelihood_{hi}}{\likelihood_i} \right)\\
% d2 l / d v_l^2
\frac{\partial^2 \loglik_i}{\partial\vec{\lambda} \partial\vec{\lambda}'} &=&
\frac{p_l}{\likelihood_i} \left[
  \frac{\partial \likelihood_{li}}{\partial \lambda}
  \left( \delta_i \vec{d}_i - v_l \mu_i \vec{T}_i \right) -
  \likelihood_{li} \diag
    \left( v_l \mu_i \me^{\vec{\lambda}} \compprod \vec{T}_i \right)
  \right] +\notag\\
&+&
\frac{p_h}{\likelihood_i} \left[
  \frac{\partial \likelihood_{hi}}{\partial \lambda}
  \left( \delta_i \vec{d}_i - \mu_i \vec{T}_i \right) -
  \likelihood_{hi} \diag
    \left( \mu_i \me^{\vec{\lambda}} \compprod \vec{T}_i \right)
  \right] -\notag\\
&-&
\frac{1}{\likelihood_i^2} 
  \left( \frac{\partial \likelihood_i}{\partial \vec\lambda} \right)^2\\
% d2 l / d gamma^2
\frac{\partial^2 \loglik_i}{\partial \vec{\gamma} \partial \vec{\gamma}'} &=&
p_l \frac{\likelihood_{li}}{\likelihood_i}
  \left[ \left( \delta_i - z_{li} \right)^2 - z_{li} \right]
  \vec{x}_i \vec{x}_i' +
p_h \frac{\likelihood_{hi}}{\likelihood_i}
  \left[ \left( \delta_i - z_{hi} \right)^2 - z_{hi} \right] 
  \vec{x}_i \vec{x}_i' - \notag\\
&-&
\frac{\partial \likelihood_i}{\partial \vec{\gamma}}
  \frac{\partial \likelihood_i}{\partial \vec{\gamma}'}\\
% d2 l / d lambda d gamma
\frac{\partial^2 \loglik_i}{\partial \vec{\lambda} \vec{\gamma}'} &=&
\left[ \left( \delta_i - z_{li} \right)
  \left( \delta_i \vec{d}_i - z_{li} \right) - z_{li} \right]
  \frac{\likelihood_{li}}{\likelihood_i} \vec{x}_i' p_l +
\notag\\
&+&
\left[ \left( \delta_i - z_{hi} \right)
  \left( \delta_i \vec{d}_i - z_{hi} \right) - z_{hi} \right]
  \frac{\likelihood_{hi}}{\likelihood_i} \vec{x}_i' p_h -
\frac{1}{\likelihood_i}
  \frac{\partial \likelihood_i}{\partial \vec{\lambda}}
  \frac{\partial \likelihood_i}{\partial \vec{\gamma}'}\\
% d2 l / d lambda d v_l
\frac{\partial^2 \loglik_i}{\partial \vec{\lambda} \partial v_l} &=&
p_l \frac{\likelihood_{li}}{\likelihood_i}
  \left( \frac{\delta_i}{v_l} - z_{hi} \right)
  \left(
    \delta_i \vec{d}_i - 
      v_l \mu_i \me^{\vec{\lambda}} \compprod \vec{T}_i
    - \frac{1}{\likelihood_i} \frac{\partial \likelihood_i}{\partial \vec{\lambda}}
    \right) -
\notag\\
&-&
p_l \frac{\likelihood_{li}}{\likelihood_i} \mu_i 
  \me^{\vec{\lambda}} \compprod \vec{T}_i\\
% d2 l / d lambda d p_l
\frac{\partial^2 \loglik_i}{\partial \vec{\lambda} \partial p_l} &=&
\frac{\likelihood_{li}}{\likelihood_i} \left(
  \delta_i d_i - v_l \mu_i \me^{\vec{\lambda}} \compprod \vec{T}_i
  \right) -
\frac{\likelihood_{hi}}{\likelihood_i} \left(
  \delta_i d_i - \mu_i \me^{\vec{\lambda}} \compprod \vec{T}_i
  \right) -
\notag\\
&-&
\frac{\likelihood_{li} - \likelihood_{hi}}{\likelihood_i}
  \frac{1}{\likelihood_i} \frac{\partial \likelihood_i}{\partial \vec{\lambda}}\\
% d2 l / d gamma d v_l
\frac{\partial^2 \loglik_i}{\partial \vec{\gamma} \partial v_l} &=&
p_l \frac{\likelihood_{li}}{\likelihood_i}
  \left( \frac{\delta_i}{v_l} - z_{hi} \right)
  \left( \delta_i - z_{li} \right) \vec{x}_i -
\notag\\
&-&
p_l \frac{\likelihood_{li}}{\likelihood_i}
  \left( \frac{\delta_i}{v_l} - z_{hi} \right)
  \frac{1}{\likelihood_i} \frac{\partial \likelihood_i}{\partial \vec{\gamma}}
  \vec{x}_i -
p_l \frac{\likelihood_{li}}{\likelihood_i} z_i \vec{x}_i\\
% d2 l / d gamma d p_l
\frac{\partial^2 \loglik_i}{\partial \vec{\gamma} \partial p_l} &=&
\left[
  \frac{\likelihood_{li}}{\likelihood_i}
  \left( \delta_i - z_{li} \right) -
  \frac{\likelihood_{hi}}{\likelihood_i}
  \left( \delta_i - z_{hi} \right)
  \right] \vec{x}_i -
\frac{\likelihood_{li} - \likelihood_{hi}}{\likelihood_i}
  \frac{1}{\likelihood_i} \frac{\partial \likelihood_i}{\partial \vec{\gamma}}
\end{eqnarray}
Eelnevas tähendab $\{\me^{\vec{\lambda}}\}_i = \me^{\lambda_i}$,
$\compprod$ vektorite elementide kaupa korrutamist ($\{\vec{a}
\compprod \vec{b} \}_i = a_i b_i$) ning $\diag\,\vec{a}$ on maatriks,
mille peadiagonaalil on vektor $\vec{a}$ ja mujal nullid.



\newpage
\paragraph{Indiviidi-spetsiif{}iline heterogeensus}
Eeldame nii nagu eespool et hasart on konstantne iga $M$ ajavahemiku
sees, erinevatel ajavahemikel võib ta aga olla erinev.  Avaldugu
hasart
\begin{equation}
  \theta(t|\vec{x},v) = v \me^{\lambda(t)} 
  \me^{\vec{\gamma}
\vec{x}_i'}.
\end{equation}
Mittevaadeldav heterogeensus olgu
diskreetse jaotusega $v \in \{v_1, v_2, \ldots, v_K\}$ ja tõenäosusega
vastavalt $p_1, p_2, \ldots, p_K$.  Olgu inimese mittevaadeldav tunnus
$v$ ajas muutumatu, vaadeldav tunnus aga võib muutuda.  Inimese $i$
spelli $j$ osa laiklihuudi funktsioonis on siis:
\begin{equation}
  \likelihood_{ij}(\cdot|v) = v \theta(t_{ij}|\vec{x}_{ij})^{\delta_{ij}} 
  S(t_{ij}|\vec{x}_{ij},v).
\end{equation}
Siin $t_{ij}$ on spelli vaadeldud kestus, $\delta_{ij}$ on
mitte-tsenseerituse indikaator ja $\vec{x}_{ij}$ on inimese $i$
vaadeldavad isikutunnused spelli $j$ ajal.  $\likelihood_{ij}(\cdot|v)$ on
analoogne indiviidi-spetsiifilise laiklihuudiga sõltumatute vaatluste
juhul. 

Olgu inimese $i$ kohta $N_i$ vaadeldud spelli.  Inimese $i$ osa
laiklihuudis avaldub siis
\begin{equation}
  \likelihood_i(\cdot|v) = \prod_{j=1}^{N_i} \likelihood_{ij}(\cdot|v).
\end{equation}

Vaadeldav laiklihuud avaldub:
\begin{equation}
  \likelihood_i(\cdot) = \sum_{k=1}^K p_k \likelihood_i(\cdot|v_k).
\end{equation}

Log-laiklihuudi gradient avaldub:
\begin{eqnarray}
                                % d l / d v_l
  \frac{\partial \loglik_i}{\partial v_k} & = &
  \frac{p_k}{\likelihood_i(\cdot)}
  \sum_j \frac{\likelihood_i(\cdot|v_k)}{\likelihood_{ij}(\cdot|v_k)}
  \frac{\partial \likelihood_{ij}(\cdot|v_k)}{\partial v_k}\\
                                % d l / d p_l
  \frac{\partial \loglik_i}{\partial p_l} & = &
  \frac{\likelihood_i(\cdot|v_k)}{\likelihood_i(\cdot)}\\
                                % d l / d lambda
  \frac{\partial \loglik_i}{\partial \vec{\lambda}} & = &
  \frac{1}{\likelihood_i(\cdot)} 
  \sum_k p_k
  \sum_j \frac{\likelihood_i(\cdot|v_k)}{\likelihood_{ij}(\cdot|v_k)}
  \frac{\partial \likelihood_{ij}(\cdot|v_k)}{\partial \vec{\lambda}}\\
                                % d l / d gamma
  \frac{\partial \loglik_i}{\partial \vec{\gamma}} & = &
  \frac{1}{\likelihood_i(\cdot)} 
  \sum_k p_k
  \sum_j \frac{\likelihood_i(\cdot|v_k)}{\likelihood_{ij}(\cdot|v_k)}
  \frac{\partial \likelihood_{ij}(\cdot|v_k)}{\partial \vec{\gamma}}.
\end{eqnarray}
$\likelihood_{ij}(\cdot|v)$ tuletised on analoogilised nagu sõltumatute
vaatluste korral.  Lisaks tuleb $p$ järgi gradiendi võtmisel
arvestada, et $\sum p_k = 1$.




\clearpage



\subsubsection{Intervallandmed}
\label{sec:intervallandmed}

Intervallandmetega on tegemist siis kui on vaadeldav ainult fakt et
s"undmus (seisundite vahetamine, tsenseerimine) toimus mingis
kestuse(aja)vahemikus (n"aiteks kuu v~oi n"adala jooksul).  Intervallmudel
sobib ka siis kui ei soovi hasarti t"apselt spetsifitseerida.

Mudel: olgu kestus jagatud $T+1$ vahemikuks: $[0, t_1), [t_1, t_2),
\dots, [t_{T-1}, t_T), [t_T, \infty)$.  Iga indiviidi $i$ kohta olgu
vaadeldav et millises vahemikus ta algsest seisundist lahkus, v~oi et
millises vahemikust vaatlus on tsenseeritud.  Eeldame MPH mudelit nagu
\ref{sec:MPH_pcw}.~{}osas:
\begin{equation}
  \label{eq:MPH}
  \vartheta(\tau|\vec{x}, v) = \lambda(\tau) \me^{\vec{\beta}'\vec{x}} v.
\end{equation}
T~oen"aosus, et isik j"a"ab kogu intervalli $n$ jooksul algseisundisse
avaldub 
\begin{equation}
  S_n(\vec{x}, v) = \exp(-v z_n(\vec{x})) = 
  \exp(-v \me^{\vec{\beta}'\vec{x}} 
  \int_{\tau_{n-1}}^{\tau_n} \lambda(s) \,\dif s).
\end{equation}
N"u"ud v~oib defineerida $\tilde\lambda_n$:
\begin{equation}
  \me^{\tilde\lambda_n}(t_n - t_{n-1})
  \equiv \int_{\tau_{n-1}}^{\tau_n} \lambda(s) \,\dif s,
\end{equation}
kus $\me^{\tilde\lambda_s}$ on keskmine p~ohihasart vahemikus $s$ ja
$\tilde\lambda$ on lihtsalt mudeli parameeter.  Seega p~ohihasart on
spetsifitseeritud mitteparameetriliselt.  Vaatluse laiklihuud
fikseeritud $v$ korral avaldub
n"u"ud
\begin{equation}
  \likelihood(n|\vec{x}, v) =
  \left(
    1 - \me^{-v z_n(\vec{x})}
  \right)^\delta
  \prod_{m=1}^{n-1} \me^{-v z_m(\vec{x})},
\end{equation}
kus $\delta=1$ t"ahendab et vaatlus pole tsenseeritud.  Kogu vaatluse
log-laiklihuud on 
\begin{equation}
  \loglik(n|\vec{x}) = 
  \log\left(
    \sum_{k=1}^K p_k \likelihood(n|\vec{x}, v_k) \right).
\end{equation}

$v$-spetsiifilise laiklihuudi gradient avaldub
\begin{align}
  \pderiv{\vec{\beta}} \likelihood(n|\vec{x}, v) 
  &=
  v E_g(n|\vec{x}, v) \pderiv{\vec{\beta}} z_n(\vec{x}) -
  v S_g(n|\vec{x}, v) \sum_{m=1}^{n-1} \pderiv{\vec{\beta}} z_m(\vec{x})\\
                                %
  \pderiv{\tilde\lambda_s} \likelihood(n|\vec{x}, v) 
  &=
  v E_g(n|\vec{x}, v) \pderiv{\tilde\lambda_s} z_n(\vec{x}) -
  v S_g(n|\vec{x}, v) \sum_{m=1}^{n-1} \pderiv{\tilde\lambda_s} z_m(\vec{x})\\
                                %
  \pderiv{v} \likelihood(n|\vec{x}, v) 
  &=
  E_g(n|\vec{x}, v) z_n(\vec{x}) -
  S_g(n|\vec{x}, v) \sum_{m=1}^{n-1} z_m(\vec{x})
\end{align}
kus
\begin{align}
  E_g(n|\vec{x}, v) 
  &=
  \delta \me^{-v z_n(\vec{x})} \prod_{m=1}^{n-1} \me^{-v z_m(\vec{x})}\\
                                %
  S_g(n|\vec{x}, v)
  &= 
  \likelihood(n|\vec{x}, v) =
  \left(
    1 - \me^{-v z_n(\vec{x})}
  \right)^\delta
  \prod_{m=1}^{n-1} \me^{-v z_m(\vec{x})}
\end{align}
on gradiendi seisundist lahkumise ja seisundis kestmise spetsiifilised
osad ja $z$ gradient avaldub
\begin{align}
  \pderiv{\vec{\beta}} z_n(\vec{x})
  &=
  z_n(\vec{x}) \vec{x}_n\\
                                %
  \pderiv{\tilde\lambda_s} z_n(\vec{x})
  &=
  z_n(\vec{x}) \indic(s = n).
\end{align}
Kogulaiklihuudi gradient on
\begin{align}
  \pderiv{\vec{\beta}} \loglik(n|\vec{x}) 
  &=
  \frac{1}{\likelihood(n|\vec{x})}
  \left(
    \sum_{k=1}^K p_k \pderiv{\vec{\beta}} \likelihood(n|\vec{x}, v)
  \right)\\
                                %
  \pderiv{\tilde\lambda_s} \loglik(n|\vec{x}) 
  &=
  \frac{1}{\likelihood(n|\vec{x})}
  \left(
    \sum_{k=1}^K p_k \pderiv{\tilde\lambda_s} \likelihood(n|\vec{x}, v)
  \right)\\
                                %
  \pderiv{v_k} \loglik(n|\vec{x}) 
  &=
  \frac{1}{\likelihood(n|\vec{x})}
  p_k \pderiv{v_k} \likelihood(n|\vec{x}, v)\\
                                %
  \pderiv{p_k} \loglik(n|\vec{x}) 
  &=
  \frac{\likelihood(n|\vec{x}, v_k)}{\likelihood(n|\vec{x})}
\end{align}




\clearpage
\paragraph{Mitu lõppseisundit}

Mitme lõppseisundiga mudelid esitatakse sageli \emph{kompiiting risk}
kujul.  Too tähendab et kujutatakse ette et kõikidesse
lõppseisunditesse viivad sõltumatud Markovi protsessid, realiseerub
too mille aeg on kõige lühem.  Vajadusel võib tsenseerimist kujutada
ühena lõppseisunditest.

Kui kõik muutujad on vaadeldavad, ei erine mitme lõppseisundiga juhtub
olukorrast kui modelleerida üksikuid lõppseisundeid sõltumatult,
teised seisundid oleksid siis nagu tsenseeritud.  Kui mudelis on
mittevaadeldav heterogeensus, on pilt ainult veidi keerulisem.

Olgu $M$ võimalikku lõppseisundit ja $m \in \{1,\dots,M\}$ olgu
lõppseisundi näitaja.  Olgu seisundisse $m$ ülemineku hasart,
sõltuvalt vaadeldavatest ja mittevaadeldavatest parameetritest,
$\vartheta^m(\tau|\vec{x}, v^m)$.  Mittevaadeldav heterogeensus olgu
$M$-mõõtmelise diskreetse jaotusega: $v^m \in \{v_1^m, \dots,
v_{K^m}^m$ kusjuures $\vec{v} = (v_{k^1}^1, \dots, v_{k^M}^M)$ esineb
tõenäosusega $p_{k^1 \dots k^M}$.  Eeldame et erinevate spellide
jooksul on $\vec{v}$ konstantne, $\vec{x}$ aga võib muutuda.

Inimese $i$ spelli $j$, osa laiklihuudi funktsioonis siirde $m$ järgi
on nüüd:
\begin{equation}
  \likelihood_{ij}^m(\cdot|v^m) = 
  \left[ \vartheta^m(\tau_{ij}|\vec{x}_{ij}) \right]^{\delta_{ij}^m} 
  S^m(\tau_{ij}|\vec{x}_{ij},v^m).
\end{equation}
Siirdega seotud liige $\left[ \vartheta^m(\tau_{ij}|\vec{x}_{ij})
\right]^{\delta_{ij}^m}$ ja püsimisega seotud liige
$S^m(\tau_{ij}|\vec{x}_{ij},v^m)$ avalduvad nii nagu ühe spelli ja ühe
lõppseisundi puhul (vaata näiteks~\ref{sec:MPH_pcw}
või~\ref{sec:intervallandmed}).  Siin $\tau_{ij}$ on spelli vaadeldud
kestus, $\delta_{ij}^m$ on mitte-tsenseerituse indikaator $m$-seisundi
mõttes ($=1$ kui läks seisundisse $m$ ja 0 kui es lähe) ja
$\vec{x}_{ij}$ on inimese $i$ vaadeldavad isikutunnused spelli $j$
ajal.  Spelli kogulaiklihuud on:
\begin{equation}
  \likelihood_{ij}(\cdot|\vec{x}_{ij},\vec{v}) = 
  \prod_{m=1}^M
  \likelihood_{ij}^m(\cdot|\vec{x}_{ij}, v^m).
\end{equation}
Kompiiting risk mudel eeldab et siirded on sõltumatud (kui kontrollida
$\vec{x}$ ja $\vec{v}$ suhtes), seega siis laiklihuudi korrutis
siirete kaupa.  

Olgu inimese $i$ kohta $N_i$ vaadeldud spelli.  Inimese $i$ osa
laiklihuudis avaldub siis
\begin{equation}
  \likelihood_i(\cdot|\vec{x}, \vec{v}) = 
  \prod_{j=1}^{N_i} \likelihood_{ij}(\cdot|\vec{x}, \vec{v}).
\end{equation}
ja vaadeldav laiklihuud avaldub:
\begin{equation}
  \likelihood_i(\cdot|\vec{x}) = 
  \sum_{k^1=1}^{K^1} \dots \sum_{k^M=1}^{K^M} 
  p_{k^1 \dots k^M} 
  \likelihood_i(\cdot|\vec{x}, \vec{v}).
\end{equation}

Laiklihuudi gradient avaldub:
\begin{align}
                                % d l / d lambda
  \frac{\partial \likelihood_i(\cdot|\vec{x})}
  {\partial \vec{\lambda}^m} & = 
  \sum_{k^1=1}^{K^1} \dots \sum_{k^M=1}^{K^M} 
  p_{k^1 \dots k^M} 
  \likelihood_i(\cdot|\vec{x}, \vec{v})
  \sum_{j=1}^{N_i} 
  \left[ 
    \frac{1}{\likelihood_{ij}^m(\cdot|\vec{x} v_{k^m}^m)} 
    \frac{\partial \likelihood_{ij}^m(\cdot|\vec{x} v_{k^m}^m)}
    {\partial \vec{\lambda}^m} 
  \right]
    \\
                                % d l / d gamma
  \frac{\partial \likelihood_i(\cdot|\vec{x})}
  {\partial \vec{\gamma}^m} & = 
  \sum_{k^1=1}^{K^1} \dots \sum_{k^M=1}^{K^M} 
  p_{k^1 \dots k^M} 
  \likelihood_i(\cdot|\vec{x}, \vec{v})
  \sum_{j=1}^{N_i} 
  \left[ 
    \frac{1}{\likelihood_{ij}^m(\cdot|\vec{x} v_{k^m}^m)} 
    \frac{\partial \likelihood_{ij}^m(\cdot|\vec{x} v_{k^m}^m)}
    {\partial \vec{\gamma}^m} 
  \right]
    \\
                                % d l / d v_l
  \frac{\partial \likelihood_i(\cdot|\vec{x})}
  {\partial v_k^m} & = 
  \sum_{k^1=1}^{K^1} \dots \sum_{k^{m-1}=1}^{K^{m-1}} 
  \sum_{k^{m+1}=1}^{K^{m+1}} \dots \sum_{k^M=1}^{K^M} 
  p_{k^1 \dots k^{m-1} k k^{m+1} \dots k^M} 
  \cdot
  \notag\\
  & \cdot
  \likelihood_i(\cdot|\vec{x}, \vec{v})
  \sum_{j=1}^{N_i} 
  \left[ 
    \frac{1}{\likelihood_{ij}^m(\cdot|\vec{x} v_{k}^m)} 
    \frac{\partial \likelihood_{ij}^m(\cdot|\vec{x} v_k^m)}
    {\partial \vec{\gamma}^m} 
  \right]
    \\
                                % d l / d p_l
  \frac{\partial \likelihood_i(\cdot|\vec{x})}
  {\partial p_{k^1 \dots k^M}}
    & = 
  \likelihood_i(\cdot|\vec{x}, (v_{k^1}^1, \dots, v_{k^M}^M))
\end{align}
$\likelihood_{ij}^m(\cdot|\vec{x}, \vec{v})$ tuletised on nii nagu
sõltumatute vaatluste korral.




\clearpage
\subsubsection{Parametriseerimine}

\selectlanguage{english}
\paragraph{Logistic transition probability in discrete-time}
In discrete time, it is useful to parameterise the
destination-specific exit probabilities logistically as
\begin{equation}
  p^m \equiv \Pr(\text{exit to destination $m$}) = 
  \frac{\me^{\lambda^m}}{1 + \sum_k \me^{\lambda^k}}.
\end{equation}
The probabilities are guaranteed to be positive and their sum to be
less than one while $\lambda$-s may be unbounded.  The components of
the corresponding gradient transformation matrix are
\begin{equation}
  \pderiv{\lambda^k} p^m =
  \indic(k = m) p_m - p_k p_m
\end{equation}
and the inverse transformation
\begin{equation}
  l^m = \log \frac{p^m}{1 - \sum_k p^k}.
\end{equation}



\paragraph{Transition probability in continuous time}
A good choice for parametrising the time-dependent part of the hazard
is
\begin{equation}
  \lambda = \me^{\tilde \lambda}
\end{equation}
The $\lambda$ is now guaranteed to be positive.


\selectlanguage{estonian}
\paragraph{Diskreetne mittevaadeldav heterogeensus}
Diskreetne mittevaadeldav heterogeensus, üks lõppseisund: $v \in
\{v_1, v_2,\dots,v_K$ ja vastavad tõenäousused $p_1, p_2, \dots,
p_K$.  

Kui laiklihuudi maksimeerimisel kasutada vastavaid parameetreid
$\tilde v$ ning $\tilde p$ ($\tilde N$ parameetrit) kuid laiklihuudi
arvutamiseks nad normaalkujule ($N$ parameetrit) teisendada, siis peab
arvestama, et vastavad gradiendi komponendid teisenevad:
\begin{equation}
  \begin{pmatrix}
    \displaystyle\pderiv{\tilde{\vec{v}}}\loglik_i\\[2ex]
    \displaystyle\pderiv{\tilde{\vec{p}}}\loglik_i
  \end{pmatrix}
  =
  \begin{pmatrix}
    \displaystyle\pderiv{\tilde{\vec{v}}}v &
    \displaystyle\pderiv{\tilde{\vec{v}}}p\\[2ex]
    \displaystyle\pderiv{\tilde{\vec{p}}}v &
    \displaystyle\pderiv{\tilde{\vec{p}}}p
  \end{pmatrix}
  \begin{pmatrix}
    \displaystyle\pderiv{v}\loglik_i\\[2ex]
    \displaystyle\pderiv{p}\loglik_i
  \end{pmatrix}
  \equiv
  \mat{C}
  \begin{pmatrix}
    \displaystyle\pderiv{v}\loglik_i\\[2ex]
    \displaystyle\pderiv{p}\loglik_i
  \end{pmatrix}
  \label{eq:kestus_parametriseerimine}
\end{equation}
Maatriksi $\mat{C}$ ridade arv vastab algkuju parameetrite arvule
$\tilde N$ ning veergude arv normaalkuju parameetrite arvule $N$ (s.h.
lineaarselt s\~{o}ltuvad $p_H$ ja $v_H$).  Kovarjatsjoonimaatriksi $p$
ning $v$ sisaldav osa on vastavalt:
\begin{equation}
  \mat\Sigma = \mat C' \tilde{\mat\Sigma} \mat C
\end{equation}


\paragraph{Heterogeensus} 
Tõenäosused on võimalik parametriseerida kui
\begin{equation}
  p_k = \frac{\displaystyle \me^{\tilde p_k}}
  {\displaystyle \sum_{k = 1}^{K} \me^{\tilde p_k}}
  \qquad\mbox{ja}\qquad 
  \sum_{k=1}^K \tilde p_k = 0,
\end{equation}
kus $K$ on jaotuse toetuspunktide arv.

\subparagraph{Keskväärtuse normeerimine} $v$ keskväärtuse võib
normeerida üheks:
\begin{equation}
  v_k = \me^{\tilde v_k}
  \qquad\mbox{ja}\qquad 
  \sum_{k=1}^K p_k v_k = 1.
\end{equation}
Vastav p\"{o}\"{o}rdteisendus t\~{o}en\"{a}osuste tarvis on
\begin{equation}
  \tilde p_k = \log p_k - 
  \frac{\sum_{l=1}^K \log p_l}{K}.
\end{equation}

$\mat{C}$ komponendid on:
\begin{align}
 \pderiv{\tilde{v}_l}v_k
  &=
  \begin{cases}
    v_k & \text{kui\qquad} k = l < K\\
    -v_l \displaystyle\frac{p_l}{p_K} & \text{kui\qquad} k = K\\
    0             & \text{muudel juhtudel}
  \end{cases}\\
                                %
  \pderiv{\tilde{v}_l}p_k
  &=
  0\\
                                %
  \pderiv{\tilde{p}_l}v_k
  &=
  \begin{cases}
    \displaystyle\frac{1}{p_K}
    \left[
      (1 - p_K) + p_K v_K + (1 - p_l)v_l
    \right]
    & \text{kui\qquad} k = K\\
    0 & \text{muudel juhtudel}
  \end{cases}\\
                                %
  \frac{\partial p_k}{\partial \tilde p_l} 
  &= 
  -p_k(p_l - p_K) 
  +
  \indic(k=l) p_K
  -
  \indic{(K=k)}
  p_K
\end{align}



\subparagraph{$v$ komponendi normeerimine}
Defineerime
\begin{equation}
  v_K = 1.
\end{equation}
Näib et nii on intervallandmete juures tulemused mõnevõrra
stabiilsemad, samas on põhihasarti raskem tõlgendada.

$\mat{C}$ komponendid on
\begin{align}
 \pderiv{\tilde{v}_l}v_k
  &=
  \begin{cases}
    v_k & \text{kui\qquad} k = l < K\\
    0             & \text{muudel juhtudel}
  \end{cases}\\
                                %
  \pderiv{\tilde{v}_l}p_k
  &=
  0\\
                                %
  \pderiv{\tilde{p}_l}v_k
  &=
  0\\
                                %
  \frac{\partial p_k}{\partial \tilde p_l} 
  &= 
  -\frac{\displaystyle \me^{\tilde p_k} 
    \left(\me^{\tilde p_l} - \me^{\tilde p_H} \right)}
  {\left( \sum_{i=1}^H \me^{\tilde p_i} \right)^2} +
  \indic(k=l) 
  \frac{\me^{\tilde p_k}}{ \sum_{i=1}^H \me^{\tilde p_i}}
  +
  \indic{(K=k)}
  \frac{\me^{\tilde{p}_K}}
  {\sum_{i=1}^K \me^{\tilde{p}_i}}
\end{align}



\paragraph{$M$ lõppseisundit}

\eqref{eq:kestus_parametriseerimine} asemel võib nüüd kirjutada:
\begin{multline}
  \begin{pmatrix}
    \displaystyle\pderiv{\tilde{\vec{v}}^1}\loglik_i\\[2ex]
    \displaystyle\pderiv{\tilde{\vec{v}}^2}\loglik_i\\[2ex]
    \dots\\[2ex]
    \displaystyle\pderiv{\tilde{\vec{v}}^M}\loglik_i\\[2ex]
    \displaystyle\pderiv{\tilde{\vec{p}}}\loglik_i
  \end{pmatrix}
  =
  \begin{pmatrix}
    \displaystyle\pderiv{\tilde{\vec{v}}^1} v^1 &
    \displaystyle\pderiv{\tilde{\vec{v}}^1} v^2 &
    \dots &
    \displaystyle\pderiv{\tilde{\vec{v}}^1} v^M &
    \displaystyle\pderiv{\tilde{\vec{v}}^1}p\\[2ex]
                                %
    \displaystyle\pderiv{\tilde{\vec{v}}^2} v^1 &
    \displaystyle\pderiv{\tilde{\vec{v}}^2} v^2 &
    \dots &
    \displaystyle\pderiv{\tilde{\vec{v}}^2} v^M &
    \displaystyle\pderiv{\tilde{\vec{v}}^2}p\\[2ex]
                                %
    \cdots &
    \cdots &
    \ddots &
    \cdots &
    \cdots\\[2ex]
                                %
    \displaystyle\pderiv{\tilde{\vec{v}}^M} v^1 &
    \displaystyle\pderiv{\tilde{\vec{v}}^M} v^2 &
    \dots &
    \displaystyle\pderiv{\tilde{\vec{v}}^M} v^M &
    \displaystyle\pderiv{\tilde{\vec{v}}^M}p\\[2ex]
                                %
    \displaystyle\pderiv{\tilde{\vec{p}}} v^1 &
    \displaystyle\pderiv{\tilde{\vec{p}}} v^2 &
    \dots &
    \displaystyle\pderiv{\tilde{\vec{p}}} v^M &
    \displaystyle\pderiv{\tilde{\vec{p}}} p
  \end{pmatrix}
  \begin{pmatrix}
    \displaystyle\pderiv{\vec{v}^1}\loglik_i\\[2ex]
    \displaystyle\pderiv{\vec{v}^2}\loglik_i\\[2ex]
    \dots\\[2ex]
    \displaystyle\pderiv{\vec{v}^M}\loglik_i\\[2ex]
    \displaystyle\pderiv{\vec{p}}\loglik_i
  \end{pmatrix}
  \\
  \equiv
  \mat{C}
  \begin{pmatrix}
    \displaystyle\pderiv{\vec{v}^1}\loglik_i\\[2ex]
    \displaystyle\pderiv{\vec{v}^2}\loglik_i\\[2ex]
    \dots\\[2ex]
    \displaystyle\pderiv{\vec{v}^M}\loglik_i\\[2ex]
    \displaystyle\pderiv{\vec{p}}\loglik_i
  \end{pmatrix},
\end{multline}
kus $\mat{C}$ on $\tilde N \times N$ maatriks.

Olgu $p_{Dk}^m$ suuna $m$ spetsiifiline tõenäosus, s.t. millise
tõenäosusega esineb väärtus $v_k^m$.  Tuletised võib nüüd avaldada kui
\begin{equation}
  \pderiv{\tilde{p}} v^m = 
  \pderiv{\tilde{p}} p
  \pderiv{p} p_D^m
  \pderiv{p_D^m} v^m
\end{equation}



\clearpage
\selectlanguage{english}
\section{Algorithms}
\label{sec:algorithms}

\subsection{Arrays}



\paragraph{Array indexing}

Let $\mat{M}$ be a $N$-dimensional column-major zero-based array, and $m[i_1, i_2, \dots,i_N]$
its element where $i_j \in \{0,\dots,K_j-1\}$ and $K_j$ is the size of
dimension $j$.  Vectorized index for element $m[i_1, i_2, \dots,i_N]$ is
\begin{equation}
  j = i_1 + K_1 i_2  + K_2 i_3 + \dots + K_{N-1} i_N.
\end{equation}
The array indices can be recovered from the vector index $j$ as: 
\begin{align*}
  i_1 &= j \mod K_1\\
  i_2 &= [j \mod (K_1 K_{2})] \idiv K_1\\
  i_3 &= [j \mod (K_1 K_2 K_{3})] \idiv (K_1 K_{2})\\
  i_4 &= [j \mod (K_1 K_2 K_3 K_{4})] \idiv (K_{1} K_{2} K_{3})\\
      & \dots
\end{align*}
where $\idiv$ is the integer division.


\cleardoublepage
\phantomsection
\bibliographystyle{apecon}
\bibliography{cheatsheet}
\addcontentsline{toc}{chapter}{References}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Index}
\printindex

\end{document}
